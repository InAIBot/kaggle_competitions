{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import math\n",
    "import torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from IPython.core.display import display, HTML\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import time\n",
    "from transformers import DataCollatorWithPadding\n",
    "#from datasets import Dataset, load_metric\n",
    "\n",
    "# From this Gist: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('D:/feedback/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic_pred_df = pd.read_csv('D:/feedback/topic_model_feedback.csv')\n",
    "topic_pred_df = topic_pred_df.drop(columns={'prob'})\n",
    "topic_pred_df = topic_pred_df.rename(columns={'id': 'essay_id'})\n",
    "\n",
    "topic_meta_df = pd.read_csv('D:/feedback/topic_model_metadata.csv')\n",
    "topic_meta_df = topic_meta_df.rename(columns={'Topic': 'topic', 'Name': 'topic_name'}).drop(columns=['Count'])\n",
    "topic_meta_df.topic_name = topic_meta_df.topic_name.apply(lambda n: ' '.join(n.split('_')[1:]))\n",
    "\n",
    "topic_pred_df = topic_pred_df.merge(topic_meta_df, on='topic', how='left')\n",
    "\n",
    "train_df = train_df.merge(topic_pred_df, on='essay_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Adequate', 'Effective', 'Ineffective']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "\n",
    "config.seed = 420\n",
    "config.model = 'microsoft/deberta-v3-large'\n",
    "config.output_path = Path('./')\n",
    "config.input_path = Path('D:/feedback')\n",
    "\n",
    "config.n_folds = 5\n",
    "config.lr = 2e-5\n",
    "config.weight_decay = 0.01\n",
    "config.epochs = 4\n",
    "config.batch_size = 4\n",
    "config.gradient_accumulation_steps = 1\n",
    "config.warm_up_ratio = 0.1\n",
    "config.max_len = 512\n",
    "config.hidden_dropout_prob = 0.2\n",
    "config.label_smoothing_factor = 0.\n",
    "config.eval_per_epoch = 2\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = 'D:/feedback/train'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)\n",
    "tokenizer.model_max_length = config.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['discourse_text'] = train_df['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "train_df['essay_text']  = train_df['essay_id'].apply(lambda x: get_essay(x, is_train=True))\n",
    "train_df['essay_text'] = train_df['essay_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "#test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "def add_inputs(df):\n",
    "    df['inputs'] = df.discourse_type.str.lower() + ' ' + df.discourse_text.str.lower() + tokenizer.sep_token + df.essay_text\n",
    "    return df\n",
    "train_df['label'] = train_df['discourse_effectiveness'].map({'Ineffective': 0, 'Adequate': 1, 'Effective': 2})\n",
    "train_df = add_inputs(train_df)\n",
    "#cv = GroupKFold(n_splits=config.n_folds)\n",
    "cv = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=42)\n",
    "train_df['fold'] = -1\n",
    "for fold_num, (train_idxs, test_idxs) in enumerate(cv.split(train_df.index, train_df.discourse_effectiveness, train_df.essay_id)):\n",
    "    train_df.loc[test_idxs, ['fold']] = fold_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv('D:/feedback/trainsplit.csv',index=False)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df=pd.read_csv('D:/feedback/trainsplit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:/feedback/')\n",
    "from datasetv1 import TrainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        # self.pooler = MeanPooling()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2, \n",
    "                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 3))\n",
    "        \n",
    "\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        transformer_out = self.model(**inputs)\n",
    "        \n",
    "        # LSTM/GRU header\n",
    "#         all_hidden_states = torch.stack(transformer_out[1])\n",
    "#         sequence_output = self.pooler(all_hidden_states)\n",
    "        \n",
    "        # simple CLS\n",
    "        sequence_output = transformer_out[0][:, 0, :]\n",
    "\n",
    "        \n",
    "        # Main task\n",
    "        logits1 = self.output(self.dropout1(sequence_output))\n",
    "        logits2 = self.output(self.dropout2(sequence_output))\n",
    "        logits3 = self.output(self.dropout3(sequence_output))\n",
    "        logits4 = self.output(self.dropout4(sequence_output))\n",
    "        logits5 = self.output(self.dropout5(sequence_output))\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        criterion: _Loss,\n",
    "        optimizer: Optimizer,\n",
    "        apex: bool,\n",
    "        adv_param: str=\"weight\",\n",
    "        adv_lr: float=1.0,\n",
    "        adv_eps: float=0.01\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.apex = True\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "    def attack_backward(self, inputs,label) -> Tensor:\n",
    "        with torch.cuda.amp.autocast(enabled=self.apex):\n",
    "            self._save()\n",
    "            self._attack_step() # モデルを近傍の悪い方へ改変\n",
    "            preds = self.model(inputs)\n",
    "            loss = self.criterion(preds, label)\n",
    "            self.optimizer.zero_grad()\n",
    "        return loss\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    # 直前に損失関数に通してパラメータの勾配を取得できるようにしておく必要あり\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(\n",
    "                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, awp):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if CFG.nth_awp_start_epoch <= epoch+1:\n",
    "            loss = awp.attack_backward(inputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp._restore()\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    #preds = np.array([])\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        \n",
    "#         if step > 0:\n",
    "#             preds = np.row_stack((preds, y_preds.sigmoid().to('cpu').numpy()))\n",
    "#         else:\n",
    "#             preds = y_preds.sigmoid().to('cpu').numpy()\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    #print(preds.shape, predictions.shape)\n",
    "#     predictions = np.concatenate(predictions)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=0\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=3\n",
    "    encoder_lr=5e-7\n",
    "    decoder_lr=5e-7\n",
    "    min_lr=1e-8 \n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=4\n",
    "    fc_dropout=0.2\n",
    "    target_size=1\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    early_stop=4\n",
    "    nth_awp_start_epoch=2\n",
    "    awp_lr=1e-4\n",
    "    awp_eps=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(folds, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = TrainDataset(config, train_folds)\n",
    "    valid_dataset = TrainDataset(config, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(config)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    best_score = 100.0\n",
    "    es=0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    awp = AWP(\n",
    "            model, \n",
    "            criterion, \n",
    "            optimizer,\n",
    "            True,\n",
    "            adv_lr=2e-5, \n",
    "            adv_eps=1e-2\n",
    "        )\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        # train\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,awp)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        score = get_score(valid_dataset.target, predictions)\n",
    "        # eval\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  - avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "       # LOGGER.info(f'Epoch {epoch+1} - Score: {avg_val_metric:.4f}')\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "        else:\n",
    "            es+=1\n",
    "            if es == CFG.early_stop:\n",
    "                break\n",
    "        gc.collect()\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    deb_ineffective = []\n",
    "    deb_effective = []\n",
    "    deb_adequate = []\n",
    "\n",
    "    \n",
    "    deb_ineffective.append(predictions[:, 0])\n",
    "    deb_adequate.append(predictions[:, 1])\n",
    "    deb_effective.append(predictions[:, 2])\n",
    "    # list -> dataframe\n",
    "    deb_ineffective = pd.DataFrame(deb_ineffective).T\n",
    "    deb_adequate = pd.DataFrame(deb_adequate).T\n",
    "    deb_effective = pd.DataFrame(deb_effective).T\n",
    "    \n",
    "    valid_folds['oof_ineffective']=deb_ineffective\n",
    "    valid_folds['oof_adequate']=deb_adequate\n",
    "    valid_folds['oof_effective']=deb_effective\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'D:/feedback/output/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7353] Elapsed 0m 1s (remain 136m 59s) Loss: 1.3826(1.3826) Grad: inf  LR: 0.00000050  \n",
      "Epoch: [1][100/7353] Elapsed 0m 41s (remain 50m 3s) Loss: 1.1263(1.0505) Grad: 621882.6250  LR: 0.00000050  \n",
      "Epoch: [1][200/7353] Elapsed 1m 22s (remain 49m 6s) Loss: 1.1527(1.0404) Grad: 589467.8125  LR: 0.00000050  \n",
      "Epoch: [1][300/7353] Elapsed 2m 3s (remain 48m 18s) Loss: 0.9888(1.0437) Grad: 605109.1250  LR: 0.00000050  \n",
      "Epoch: [1][400/7353] Elapsed 2m 44s (remain 47m 31s) Loss: 0.5361(1.0314) Grad: 675627.0625  LR: 0.00000050  \n",
      "Epoch: [1][500/7353] Elapsed 3m 25s (remain 46m 49s) Loss: 0.6528(1.0164) Grad: 453738.8125  LR: 0.00000050  \n",
      "Epoch: [1][600/7353] Elapsed 4m 6s (remain 46m 6s) Loss: 1.4624(1.0157) Grad: 777399.6250  LR: 0.00000050  \n",
      "Epoch: [1][700/7353] Elapsed 4m 47s (remain 45m 24s) Loss: 0.7860(1.0084) Grad: 654568.7500  LR: 0.00000050  \n",
      "Epoch: [1][800/7353] Elapsed 5m 27s (remain 44m 42s) Loss: 0.6813(1.0030) Grad: 802901.0625  LR: 0.00000050  \n",
      "Epoch: [1][900/7353] Elapsed 6m 8s (remain 44m 0s) Loss: 0.3971(0.9976) Grad: 558107.1250  LR: 0.00000050  \n",
      "Epoch: [1][1000/7353] Elapsed 6m 49s (remain 43m 19s) Loss: 1.3511(0.9964) Grad: 884297.5000  LR: 0.00000050  \n",
      "Epoch: [1][1100/7353] Elapsed 7m 30s (remain 42m 38s) Loss: 1.3938(0.9922) Grad: 824580.6250  LR: 0.00000050  \n",
      "Epoch: [1][1200/7353] Elapsed 8m 11s (remain 41m 57s) Loss: 0.5632(0.9878) Grad: 781843.1250  LR: 0.00000050  \n",
      "Epoch: [1][1300/7353] Elapsed 8m 52s (remain 41m 15s) Loss: 1.0356(0.9807) Grad: 562494.0000  LR: 0.00000050  \n",
      "Epoch: [1][1400/7353] Elapsed 9m 32s (remain 40m 34s) Loss: 0.8680(0.9755) Grad: 841484.5000  LR: 0.00000050  \n",
      "Epoch: [1][1500/7353] Elapsed 10m 13s (remain 39m 52s) Loss: 0.6873(0.9674) Grad: 397484.0625  LR: 0.00000049  \n",
      "Epoch: [1][1600/7353] Elapsed 10m 54s (remain 39m 11s) Loss: 0.9291(0.9633) Grad: 827302.1250  LR: 0.00000049  \n",
      "Epoch: [1][1700/7353] Elapsed 11m 35s (remain 38m 30s) Loss: 1.0675(0.9594) Grad: 619272.5000  LR: 0.00000049  \n",
      "Epoch: [1][1800/7353] Elapsed 12m 16s (remain 37m 49s) Loss: 1.5242(0.9580) Grad: 1467654.2500  LR: 0.00000049  \n",
      "Epoch: [1][1900/7353] Elapsed 12m 57s (remain 37m 8s) Loss: 0.8638(0.9537) Grad: 593561.4375  LR: 0.00000049  \n",
      "Epoch: [1][2000/7353] Elapsed 13m 37s (remain 36m 27s) Loss: 1.4073(0.9528) Grad: 1165168.7500  LR: 0.00000049  \n",
      "Epoch: [1][2100/7353] Elapsed 14m 18s (remain 35m 46s) Loss: 0.4905(0.9464) Grad: 393847.0625  LR: 0.00000049  \n",
      "Epoch: [1][2200/7353] Elapsed 14m 59s (remain 35m 5s) Loss: 1.0378(0.9437) Grad: 1152284.7500  LR: 0.00000049  \n",
      "Epoch: [1][2300/7353] Elapsed 15m 40s (remain 34m 25s) Loss: 0.6631(0.9414) Grad: 1034210.4375  LR: 0.00000049  \n",
      "Epoch: [1][2400/7353] Elapsed 16m 21s (remain 33m 44s) Loss: 0.5240(0.9386) Grad: 462757.3750  LR: 0.00000049  \n",
      "Epoch: [1][2500/7353] Elapsed 17m 2s (remain 33m 4s) Loss: 0.7250(0.9374) Grad: 1287556.1250  LR: 0.00000048  \n",
      "Epoch: [1][2600/7353] Elapsed 17m 43s (remain 32m 23s) Loss: 0.8703(0.9352) Grad: 476165.9688  LR: 0.00000048  \n",
      "Epoch: [1][2700/7353] Elapsed 18m 25s (remain 31m 43s) Loss: 0.3367(0.9334) Grad: 459196.7500  LR: 0.00000048  \n",
      "Epoch: [1][2800/7353] Elapsed 19m 6s (remain 31m 2s) Loss: 0.4508(0.9319) Grad: 614812.9375  LR: 0.00000048  \n",
      "Epoch: [1][2900/7353] Elapsed 19m 47s (remain 30m 22s) Loss: 0.8868(0.9299) Grad: 519640.9688  LR: 0.00000048  \n",
      "Epoch: [1][3000/7353] Elapsed 20m 28s (remain 29m 41s) Loss: 1.3621(0.9308) Grad: 1095326.6250  LR: 0.00000048  \n",
      "Epoch: [1][3100/7353] Elapsed 21m 9s (remain 29m 0s) Loss: 0.6953(0.9279) Grad: 623410.7500  LR: 0.00000048  \n",
      "Epoch: [1][3200/7353] Elapsed 21m 50s (remain 28m 20s) Loss: 0.3808(0.9274) Grad: 489375.3125  LR: 0.00000047  \n",
      "Epoch: [1][3300/7353] Elapsed 22m 31s (remain 27m 39s) Loss: 2.1346(0.9284) Grad: 1823909.0000  LR: 0.00000047  \n",
      "Epoch: [1][3400/7353] Elapsed 23m 13s (remain 26m 58s) Loss: 0.8589(0.9272) Grad: 468378.1562  LR: 0.00000047  \n",
      "Epoch: [1][3500/7353] Elapsed 23m 54s (remain 26m 18s) Loss: 0.7578(0.9249) Grad: 875588.6250  LR: 0.00000047  \n",
      "Epoch: [1][3600/7353] Elapsed 24m 35s (remain 25m 37s) Loss: 0.9700(0.9240) Grad: 481785.8750  LR: 0.00000047  \n",
      "Epoch: [1][3700/7353] Elapsed 25m 16s (remain 24m 56s) Loss: 0.6162(0.9215) Grad: 547330.3750  LR: 0.00000047  \n",
      "Epoch: [1][3800/7353] Elapsed 25m 57s (remain 24m 15s) Loss: 1.0453(0.9202) Grad: 450564.5312  LR: 0.00000046  \n",
      "Epoch: [1][3900/7353] Elapsed 26m 38s (remain 23m 34s) Loss: 0.9763(0.9203) Grad: 574115.3750  LR: 0.00000046  \n",
      "Epoch: [1][4000/7353] Elapsed 27m 19s (remain 22m 53s) Loss: 0.3525(0.9187) Grad: 468308.8750  LR: 0.00000046  \n",
      "Epoch: [1][4100/7353] Elapsed 28m 0s (remain 22m 12s) Loss: 0.6815(0.9155) Grad: 407213.0000  LR: 0.00000046  \n",
      "Epoch: [1][4200/7353] Elapsed 28m 41s (remain 21m 31s) Loss: 0.4626(0.9145) Grad: 341341.1250  LR: 0.00000046  \n",
      "Epoch: [1][4300/7353] Elapsed 29m 22s (remain 20m 50s) Loss: 0.3484(0.9136) Grad: 434201.5000  LR: 0.00000045  \n",
      "Epoch: [1][4400/7353] Elapsed 30m 4s (remain 20m 10s) Loss: 0.5691(0.9117) Grad: 327183.1875  LR: 0.00000045  \n",
      "Epoch: [1][4500/7353] Elapsed 30m 45s (remain 19m 29s) Loss: 0.3771(0.9102) Grad: 315979.7812  LR: 0.00000045  \n",
      "Epoch: [1][4600/7353] Elapsed 31m 26s (remain 18m 48s) Loss: 0.2662(0.9084) Grad: 312145.5312  LR: 0.00000045  \n",
      "Epoch: [1][4700/7353] Elapsed 32m 7s (remain 18m 7s) Loss: 1.0664(0.9069) Grad: 791554.3750  LR: 0.00000045  \n",
      "Epoch: [1][4800/7353] Elapsed 32m 48s (remain 17m 26s) Loss: 0.7125(0.9067) Grad: 398543.6250  LR: 0.00000044  \n",
      "Epoch: [1][4900/7353] Elapsed 33m 29s (remain 16m 45s) Loss: 0.7139(0.9053) Grad: 397936.5938  LR: 0.00000044  \n",
      "Epoch: [1][5000/7353] Elapsed 34m 10s (remain 16m 4s) Loss: 0.3641(0.9047) Grad: 425884.9375  LR: 0.00000044  \n",
      "Epoch: [1][5100/7353] Elapsed 34m 51s (remain 15m 23s) Loss: 0.7702(0.9047) Grad: 435792.7812  LR: 0.00000044  \n",
      "Epoch: [1][5200/7353] Elapsed 35m 32s (remain 14m 42s) Loss: 0.9846(0.9036) Grad: 957341.0000  LR: 0.00000043  \n",
      "Epoch: [1][5300/7353] Elapsed 36m 14s (remain 14m 1s) Loss: 1.3925(0.9012) Grad: 710409.8125  LR: 0.00000043  \n",
      "Epoch: [1][5400/7353] Elapsed 36m 55s (remain 13m 20s) Loss: 0.9887(0.9005) Grad: 821303.7500  LR: 0.00000043  \n",
      "Epoch: [1][5500/7353] Elapsed 37m 36s (remain 12m 39s) Loss: 0.5792(0.8985) Grad: 463468.6250  LR: 0.00000043  \n",
      "Epoch: [1][5600/7353] Elapsed 38m 17s (remain 11m 58s) Loss: 0.7374(0.8985) Grad: 461184.1562  LR: 0.00000042  \n",
      "Epoch: [1][5700/7353] Elapsed 38m 58s (remain 11m 17s) Loss: 0.9532(0.8982) Grad: 1136954.0000  LR: 0.00000042  \n",
      "Epoch: [1][5800/7353] Elapsed 39m 39s (remain 10m 36s) Loss: 0.7232(0.8980) Grad: 386821.5312  LR: 0.00000042  \n",
      "Epoch: [1][5900/7353] Elapsed 40m 20s (remain 9m 55s) Loss: 1.4818(0.8968) Grad: 1391637.2500  LR: 0.00000042  \n",
      "Epoch: [1][6000/7353] Elapsed 41m 1s (remain 9m 14s) Loss: 0.3047(0.8957) Grad: 372942.3438  LR: 0.00000041  \n",
      "Epoch: [1][6100/7353] Elapsed 41m 42s (remain 8m 33s) Loss: 1.1218(0.8948) Grad: 824754.9375  LR: 0.00000041  \n",
      "Epoch: [1][6200/7353] Elapsed 42m 24s (remain 7m 52s) Loss: 1.9973(0.8940) Grad: 1623309.2500  LR: 0.00000041  \n",
      "Epoch: [1][6300/7353] Elapsed 43m 5s (remain 7m 11s) Loss: 0.9031(0.8930) Grad: 969614.1875  LR: 0.00000041  \n",
      "Epoch: [1][6400/7353] Elapsed 43m 46s (remain 6m 30s) Loss: 0.6559(0.8933) Grad: 486974.9062  LR: 0.00000040  \n",
      "Epoch: [1][6500/7353] Elapsed 44m 27s (remain 5m 49s) Loss: 0.2416(0.8924) Grad: 330730.0938  LR: 0.00000040  \n",
      "Epoch: [1][6600/7353] Elapsed 45m 8s (remain 5m 8s) Loss: 0.7234(0.8918) Grad: 333466.0312  LR: 0.00000040  \n",
      "Epoch: [1][6700/7353] Elapsed 45m 49s (remain 4m 27s) Loss: 0.6508(0.8914) Grad: 394443.3750  LR: 0.00000039  \n",
      "Epoch: [1][6800/7353] Elapsed 46m 30s (remain 3m 46s) Loss: 0.3455(0.8905) Grad: 459100.8750  LR: 0.00000039  \n",
      "Epoch: [1][6900/7353] Elapsed 47m 12s (remain 3m 5s) Loss: 0.5220(0.8900) Grad: 373686.1875  LR: 0.00000039  \n",
      "Epoch: [1][7000/7353] Elapsed 47m 53s (remain 2m 24s) Loss: 1.4999(0.8893) Grad: 1409274.5000  LR: 0.00000039  \n",
      "Epoch: [1][7100/7353] Elapsed 48m 34s (remain 1m 43s) Loss: 0.7381(0.8891) Grad: 409672.3125  LR: 0.00000038  \n",
      "Epoch: [1][7200/7353] Elapsed 49m 15s (remain 1m 2s) Loss: 0.4304(0.8887) Grad: 533140.7500  LR: 0.00000038  \n",
      "Epoch: [1][7300/7353] Elapsed 49m 56s (remain 0m 21s) Loss: 1.1115(0.8880) Grad: 819710.8750  LR: 0.00000038  \n",
      "Epoch: [1][7352/7353] Elapsed 50m 18s (remain 0m 0s) Loss: 0.5699(0.8871) Grad: 986880.3750  LR: 0.00000038  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 59s) Loss: 0.3329(0.3329) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 58s) Loss: 2.3335(0.7260) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 55s) Loss: 0.3379(0.7761) \n",
      "EVAL: [300/7353] Elapsed 0m 17s (remain 6m 51s) Loss: 0.4320(0.8164) \n",
      "EVAL: [400/7353] Elapsed 0m 23s (remain 6m 46s) Loss: 0.1918(0.8196) \n",
      "EVAL: [500/7353] Elapsed 0m 29s (remain 6m 39s) Loss: 0.1371(0.8288) \n",
      "EVAL: [600/7353] Elapsed 0m 35s (remain 6m 34s) Loss: 0.2398(0.8471) \n",
      "EVAL: [700/7353] Elapsed 0m 40s (remain 6m 27s) Loss: 0.1851(0.8177) \n",
      "EVAL: [800/7353] Elapsed 0m 46s (remain 6m 21s) Loss: 0.1856(0.8168) \n",
      "EVAL: [900/7353] Elapsed 0m 52s (remain 6m 15s) Loss: 1.3144(0.8265) \n",
      "EVAL: [1000/7353] Elapsed 0m 58s (remain 6m 10s) Loss: 0.2040(0.8277) \n",
      "EVAL: [1100/7353] Elapsed 1m 4s (remain 6m 4s) Loss: 0.0635(0.8147) \n",
      "EVAL: [1200/7353] Elapsed 1m 10s (remain 5m 59s) Loss: 2.0091(0.8110) \n",
      "EVAL: [1300/7353] Elapsed 1m 15s (remain 5m 53s) Loss: 1.5163(0.8038) \n",
      "EVAL: [1400/7353] Elapsed 1m 21s (remain 5m 47s) Loss: 0.2233(0.8041) \n",
      "EVAL: [1500/7353] Elapsed 1m 27s (remain 5m 42s) Loss: 0.1811(0.8039) \n",
      "EVAL: [1600/7353] Elapsed 1m 33s (remain 5m 36s) Loss: 0.1763(0.8081) \n",
      "EVAL: [1700/7353] Elapsed 1m 39s (remain 5m 30s) Loss: 1.1307(0.8005) \n",
      "EVAL: [1800/7353] Elapsed 1m 45s (remain 5m 24s) Loss: 0.2221(0.7934) \n",
      "EVAL: [1900/7353] Elapsed 1m 51s (remain 5m 18s) Loss: 3.0000(0.8025) \n",
      "EVAL: [2000/7353] Elapsed 1m 56s (remain 5m 12s) Loss: 0.1988(0.8127) \n",
      "EVAL: [2100/7353] Elapsed 2m 2s (remain 5m 6s) Loss: 0.5605(0.8112) \n",
      "EVAL: [2200/7353] Elapsed 2m 8s (remain 5m 0s) Loss: 0.2317(0.8174) \n",
      "EVAL: [2300/7353] Elapsed 2m 14s (remain 4m 54s) Loss: 0.2928(0.8174) \n",
      "EVAL: [2400/7353] Elapsed 2m 19s (remain 4m 48s) Loss: 0.1008(0.8173) \n",
      "EVAL: [2500/7353] Elapsed 2m 25s (remain 4m 42s) Loss: 0.2261(0.8156) \n",
      "EVAL: [2600/7353] Elapsed 2m 31s (remain 4m 36s) Loss: 0.1557(0.8102) \n",
      "EVAL: [2700/7353] Elapsed 2m 37s (remain 4m 30s) Loss: 0.3582(0.8074) \n",
      "EVAL: [2800/7353] Elapsed 2m 43s (remain 4m 25s) Loss: 0.6579(0.8005) \n",
      "EVAL: [2900/7353] Elapsed 2m 49s (remain 4m 19s) Loss: 3.9998(0.8061) \n",
      "EVAL: [3000/7353] Elapsed 2m 54s (remain 4m 13s) Loss: 0.8323(0.8067) \n",
      "EVAL: [3100/7353] Elapsed 3m 0s (remain 4m 7s) Loss: 1.1241(0.8037) \n",
      "EVAL: [3200/7353] Elapsed 3m 6s (remain 4m 2s) Loss: 0.4100(0.8014) \n",
      "EVAL: [3300/7353] Elapsed 3m 12s (remain 3m 56s) Loss: 1.7326(0.8025) \n",
      "EVAL: [3400/7353] Elapsed 3m 18s (remain 3m 50s) Loss: 0.1564(0.8041) \n",
      "EVAL: [3500/7353] Elapsed 3m 23s (remain 3m 44s) Loss: 0.4243(0.8059) \n",
      "EVAL: [3600/7353] Elapsed 3m 29s (remain 3m 38s) Loss: 0.2363(0.8033) \n",
      "EVAL: [3700/7353] Elapsed 3m 35s (remain 3m 32s) Loss: 0.2224(0.8051) \n",
      "EVAL: [3800/7353] Elapsed 3m 41s (remain 3m 26s) Loss: 1.7476(0.8047) \n",
      "EVAL: [3900/7353] Elapsed 3m 46s (remain 3m 20s) Loss: 0.2477(0.8018) \n",
      "EVAL: [4000/7353] Elapsed 3m 52s (remain 3m 14s) Loss: 0.2458(0.8038) \n",
      "EVAL: [4100/7353] Elapsed 3m 58s (remain 3m 9s) Loss: 1.0923(0.8033) \n",
      "EVAL: [4200/7353] Elapsed 4m 4s (remain 3m 3s) Loss: 0.2071(0.8089) \n",
      "EVAL: [4300/7353] Elapsed 4m 9s (remain 2m 57s) Loss: 2.7183(0.8126) \n",
      "EVAL: [4400/7353] Elapsed 4m 15s (remain 2m 51s) Loss: 0.7529(0.8183) \n",
      "EVAL: [4500/7353] Elapsed 4m 21s (remain 2m 45s) Loss: 3.0901(0.8185) \n",
      "EVAL: [4600/7353] Elapsed 4m 27s (remain 2m 39s) Loss: 0.6633(0.8257) \n",
      "EVAL: [4700/7353] Elapsed 4m 33s (remain 2m 34s) Loss: 1.0413(0.8280) \n",
      "EVAL: [4800/7353] Elapsed 4m 38s (remain 2m 28s) Loss: 1.6023(0.8258) \n",
      "EVAL: [4900/7353] Elapsed 4m 44s (remain 2m 22s) Loss: 1.0837(0.8263) \n",
      "EVAL: [5000/7353] Elapsed 4m 50s (remain 2m 16s) Loss: 0.5263(0.8281) \n",
      "EVAL: [5100/7353] Elapsed 4m 56s (remain 2m 10s) Loss: 1.4160(0.8303) \n",
      "EVAL: [5200/7353] Elapsed 5m 2s (remain 2m 5s) Loss: 0.1757(0.8316) \n",
      "EVAL: [5300/7353] Elapsed 5m 8s (remain 1m 59s) Loss: 0.3070(0.8298) \n",
      "EVAL: [5400/7353] Elapsed 5m 13s (remain 1m 53s) Loss: 0.8876(0.8321) \n",
      "EVAL: [5500/7353] Elapsed 5m 19s (remain 1m 47s) Loss: 0.2339(0.8331) \n",
      "EVAL: [5600/7353] Elapsed 5m 25s (remain 1m 41s) Loss: 2.1815(0.8331) \n",
      "EVAL: [5700/7353] Elapsed 5m 31s (remain 1m 35s) Loss: 1.4783(0.8385) \n",
      "EVAL: [5800/7353] Elapsed 5m 36s (remain 1m 30s) Loss: 1.5628(0.8435) \n",
      "EVAL: [5900/7353] Elapsed 5m 42s (remain 1m 24s) Loss: 0.2359(0.8413) \n",
      "EVAL: [6000/7353] Elapsed 5m 48s (remain 1m 18s) Loss: 0.1366(0.8428) \n",
      "EVAL: [6100/7353] Elapsed 5m 54s (remain 1m 12s) Loss: 0.2408(0.8424) \n",
      "EVAL: [6200/7353] Elapsed 5m 59s (remain 1m 6s) Loss: 0.2291(0.8416) \n",
      "EVAL: [6300/7353] Elapsed 6m 6s (remain 1m 1s) Loss: 0.3120(0.8416) \n",
      "EVAL: [6400/7353] Elapsed 6m 11s (remain 0m 55s) Loss: 0.3665(0.8437) \n",
      "EVAL: [6500/7353] Elapsed 6m 17s (remain 0m 49s) Loss: 0.1451(0.8427) \n",
      "EVAL: [6600/7353] Elapsed 6m 23s (remain 0m 43s) Loss: 1.3959(0.8422) \n",
      "EVAL: [6700/7353] Elapsed 6m 29s (remain 0m 37s) Loss: 0.1048(0.8412) \n",
      "EVAL: [6800/7353] Elapsed 6m 35s (remain 0m 32s) Loss: 0.1927(0.8414) \n",
      "EVAL: [6900/7353] Elapsed 6m 41s (remain 0m 26s) Loss: 1.4498(0.8412) \n",
      "EVAL: [7000/7353] Elapsed 6m 47s (remain 0m 20s) Loss: 0.0735(0.8414) \n",
      "EVAL: [7100/7353] Elapsed 6m 53s (remain 0m 14s) Loss: 0.2349(0.8393) \n",
      "EVAL: [7200/7353] Elapsed 6m 58s (remain 0m 8s) Loss: 0.0349(0.8414) \n",
      "EVAL: [7300/7353] Elapsed 7m 4s (remain 0m 3s) Loss: 3.9519(0.8427) \n",
      "EVAL: [7352/7353] Elapsed 7m 7s (remain 0m 0s) Loss: 0.7051(0.8414) \n",
      "Epoch 1 - Save Best Score: 0.8414 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/7353] Elapsed 0m 0s (remain 101m 14s) Loss: 1.9287(1.9287) Grad: inf  LR: 0.00000037  \n",
      "Epoch: [2][100/7353] Elapsed 1m 23s (remain 100m 20s) Loss: 0.9570(0.9599) Grad: 389343.3750  LR: 0.00000037  \n",
      "Epoch: [2][200/7353] Elapsed 2m 46s (remain 98m 58s) Loss: 0.4670(0.9244) Grad: 207897.5156  LR: 0.00000037  \n",
      "Epoch: [2][300/7353] Elapsed 4m 9s (remain 97m 25s) Loss: 0.7526(0.9085) Grad: 390935.6875  LR: 0.00000037  \n",
      "Epoch: [2][400/7353] Elapsed 5m 32s (remain 95m 59s) Loss: 0.8688(0.8953) Grad: 212119.6875  LR: 0.00000036  \n",
      "Epoch: [2][500/7353] Elapsed 6m 54s (remain 94m 33s) Loss: 0.8435(0.8851) Grad: 635593.2500  LR: 0.00000036  \n",
      "Epoch: [2][600/7353] Elapsed 8m 17s (remain 93m 10s) Loss: 1.4818(0.8816) Grad: 723017.2500  LR: 0.00000036  \n",
      "Epoch: [2][700/7353] Elapsed 9m 40s (remain 91m 45s) Loss: 0.7508(0.8802) Grad: 296538.8438  LR: 0.00000035  \n",
      "Epoch: [2][800/7353] Elapsed 11m 2s (remain 90m 21s) Loss: 1.4698(0.8793) Grad: 487359.2812  LR: 0.00000035  \n",
      "Epoch: [2][900/7353] Elapsed 12m 25s (remain 88m 59s) Loss: 0.7636(0.8676) Grad: 268087.5000  LR: 0.00000035  \n",
      "Epoch: [2][1000/7353] Elapsed 13m 48s (remain 87m 36s) Loss: 1.1439(0.8696) Grad: 323184.1562  LR: 0.00000034  \n",
      "Epoch: [2][1100/7353] Elapsed 15m 11s (remain 86m 13s) Loss: 1.0140(0.8676) Grad: 403573.0938  LR: 0.00000034  \n",
      "Epoch: [2][1200/7353] Elapsed 16m 33s (remain 84m 50s) Loss: 0.6980(0.8621) Grad: 322220.3750  LR: 0.00000034  \n",
      "Epoch: [2][1300/7353] Elapsed 17m 56s (remain 83m 26s) Loss: 0.7361(0.8614) Grad: 191723.1250  LR: 0.00000033  \n",
      "Epoch: [2][1400/7353] Elapsed 19m 18s (remain 82m 3s) Loss: 1.0319(0.8589) Grad: 392973.2188  LR: 0.00000033  \n",
      "Epoch: [2][1500/7353] Elapsed 20m 41s (remain 80m 41s) Loss: 0.4226(0.8543) Grad: 322298.0000  LR: 0.00000033  \n",
      "Epoch: [2][1600/7353] Elapsed 22m 4s (remain 79m 18s) Loss: 0.9286(0.8526) Grad: 367741.3750  LR: 0.00000032  \n",
      "Epoch: [2][1700/7353] Elapsed 23m 27s (remain 77m 55s) Loss: 0.7456(0.8504) Grad: 217572.5156  LR: 0.00000032  \n",
      "Epoch: [2][1800/7353] Elapsed 24m 49s (remain 76m 32s) Loss: 0.8770(0.8492) Grad: 240137.0781  LR: 0.00000032  \n",
      "Epoch: [2][1900/7353] Elapsed 26m 12s (remain 75m 9s) Loss: 0.9216(0.8511) Grad: 431720.8750  LR: 0.00000031  \n",
      "Epoch: [2][2000/7353] Elapsed 27m 34s (remain 73m 45s) Loss: 0.7645(0.8521) Grad: 267062.5938  LR: 0.00000031  \n",
      "Epoch: [2][2100/7353] Elapsed 28m 57s (remain 72m 22s) Loss: 0.4130(0.8502) Grad: 164721.8281  LR: 0.00000031  \n",
      "Epoch: [2][2200/7353] Elapsed 30m 19s (remain 71m 0s) Loss: 0.6827(0.8492) Grad: 240319.8594  LR: 0.00000030  \n",
      "Epoch: [2][2300/7353] Elapsed 31m 42s (remain 69m 37s) Loss: 0.5992(0.8477) Grad: 250907.9531  LR: 0.00000030  \n",
      "Epoch: [2][2400/7353] Elapsed 33m 5s (remain 68m 14s) Loss: 1.1808(0.8465) Grad: 264986.6250  LR: 0.00000030  \n",
      "Epoch: [2][2500/7353] Elapsed 34m 27s (remain 66m 51s) Loss: 0.6396(0.8464) Grad: 352083.0625  LR: 0.00000029  \n",
      "Epoch: [2][2600/7353] Elapsed 35m 50s (remain 65m 29s) Loss: 0.9640(0.8450) Grad: 369097.9688  LR: 0.00000029  \n",
      "Epoch: [2][2700/7353] Elapsed 37m 13s (remain 64m 6s) Loss: 1.2499(0.8419) Grad: 632650.8125  LR: 0.00000028  \n",
      "Epoch: [2][2800/7353] Elapsed 38m 35s (remain 62m 43s) Loss: 0.6343(0.8416) Grad: 176017.2969  LR: 0.00000028  \n",
      "Epoch: [2][2900/7353] Elapsed 39m 58s (remain 61m 21s) Loss: 1.0566(0.8417) Grad: 258195.6719  LR: 0.00000028  \n",
      "Epoch: [2][3000/7353] Elapsed 41m 21s (remain 59m 58s) Loss: 0.8702(0.8424) Grad: 214012.3438  LR: 0.00000027  \n",
      "Epoch: [2][3100/7353] Elapsed 42m 44s (remain 58m 35s) Loss: 1.4971(0.8408) Grad: 658225.0625  LR: 0.00000027  \n",
      "Epoch: [2][3200/7353] Elapsed 44m 6s (remain 57m 13s) Loss: 1.0283(0.8402) Grad: 230223.7969  LR: 0.00000027  \n",
      "Epoch: [2][3300/7353] Elapsed 45m 29s (remain 55m 50s) Loss: 0.6056(0.8393) Grad: 229308.7969  LR: 0.00000026  \n",
      "Epoch: [2][3400/7353] Elapsed 46m 52s (remain 54m 27s) Loss: 0.6851(0.8390) Grad: 215719.4375  LR: 0.00000026  \n",
      "Epoch: [2][3500/7353] Elapsed 48m 14s (remain 53m 5s) Loss: 0.8179(0.8401) Grad: 395990.7500  LR: 0.00000026  \n",
      "Epoch: [2][3600/7353] Elapsed 49m 37s (remain 51m 42s) Loss: 0.9249(0.8389) Grad: 437876.5938  LR: 0.00000025  \n",
      "Epoch: [2][3700/7353] Elapsed 51m 0s (remain 50m 19s) Loss: 0.8047(0.8392) Grad: 400878.6250  LR: 0.00000025  \n",
      "Epoch: [2][3800/7353] Elapsed 52m 23s (remain 48m 57s) Loss: 1.4913(0.8394) Grad: 754911.2500  LR: 0.00000025  \n",
      "Epoch: [2][3900/7353] Elapsed 53m 45s (remain 47m 34s) Loss: 0.9116(0.8394) Grad: 261358.6719  LR: 0.00000024  \n",
      "Epoch: [2][4000/7353] Elapsed 55m 8s (remain 46m 11s) Loss: 0.6884(0.8400) Grad: 188885.4062  LR: 0.00000024  \n",
      "Epoch: [2][4100/7353] Elapsed 56m 31s (remain 44m 49s) Loss: 0.6755(0.8403) Grad: 199848.9375  LR: 0.00000023  \n",
      "Epoch: [2][4200/7353] Elapsed 57m 53s (remain 43m 26s) Loss: 0.6267(0.8391) Grad: 233670.1094  LR: 0.00000023  \n",
      "Epoch: [2][4300/7353] Elapsed 59m 16s (remain 42m 3s) Loss: 1.1494(0.8381) Grad: 381562.7812  LR: 0.00000023  \n",
      "Epoch: [2][4400/7353] Elapsed 60m 39s (remain 40m 40s) Loss: 1.1252(0.8373) Grad: 414793.8750  LR: 0.00000022  \n",
      "Epoch: [2][4500/7353] Elapsed 62m 1s (remain 39m 18s) Loss: 0.7671(0.8361) Grad: 477805.6875  LR: 0.00000022  \n",
      "Epoch: [2][4600/7353] Elapsed 63m 24s (remain 37m 55s) Loss: 0.5890(0.8370) Grad: 195518.7500  LR: 0.00000022  \n",
      "Epoch: [2][4700/7353] Elapsed 64m 47s (remain 36m 32s) Loss: 0.6342(0.8371) Grad: 162970.9375  LR: 0.00000021  \n",
      "Epoch: [2][4800/7353] Elapsed 66m 9s (remain 35m 10s) Loss: 0.8707(0.8369) Grad: 525915.1875  LR: 0.00000021  \n",
      "Epoch: [2][4900/7353] Elapsed 67m 32s (remain 33m 47s) Loss: 0.8715(0.8361) Grad: 305233.0312  LR: 0.00000021  \n",
      "Epoch: [2][5000/7353] Elapsed 68m 55s (remain 32m 24s) Loss: 1.0396(0.8355) Grad: 403191.5938  LR: 0.00000020  \n",
      "Epoch: [2][5100/7353] Elapsed 70m 17s (remain 31m 2s) Loss: 0.3808(0.8351) Grad: 225996.4531  LR: 0.00000020  \n",
      "Epoch: [2][5200/7353] Elapsed 71m 40s (remain 29m 39s) Loss: 0.6211(0.8338) Grad: 213735.2031  LR: 0.00000020  \n",
      "Epoch: [2][5300/7353] Elapsed 73m 3s (remain 28m 16s) Loss: 1.0637(0.8330) Grad: 228926.6875  LR: 0.00000019  \n",
      "Epoch: [2][5400/7353] Elapsed 74m 25s (remain 26m 54s) Loss: 0.4100(0.8329) Grad: 190836.9219  LR: 0.00000019  \n",
      "Epoch: [2][5500/7353] Elapsed 75m 48s (remain 25m 31s) Loss: 0.5608(0.8327) Grad: 153289.1719  LR: 0.00000019  \n",
      "Epoch: [2][5600/7353] Elapsed 77m 11s (remain 24m 8s) Loss: 0.4200(0.8332) Grad: 319954.4375  LR: 0.00000018  \n",
      "Epoch: [2][5700/7353] Elapsed 78m 33s (remain 22m 45s) Loss: 0.9135(0.8332) Grad: 325647.9062  LR: 0.00000018  \n",
      "Epoch: [2][5800/7353] Elapsed 79m 56s (remain 21m 23s) Loss: 0.6531(0.8331) Grad: 191618.5156  LR: 0.00000018  \n",
      "Epoch: [2][5900/7353] Elapsed 81m 19s (remain 20m 0s) Loss: 0.8046(0.8329) Grad: 245828.2344  LR: 0.00000017  \n",
      "Epoch: [2][6000/7353] Elapsed 82m 42s (remain 18m 37s) Loss: 0.7168(0.8328) Grad: 188595.2656  LR: 0.00000017  \n",
      "Epoch: [2][6100/7353] Elapsed 84m 4s (remain 17m 15s) Loss: 0.8983(0.8329) Grad: 381239.5312  LR: 0.00000017  \n",
      "Epoch: [2][6200/7353] Elapsed 85m 27s (remain 15m 52s) Loss: 0.4735(0.8326) Grad: 203119.1250  LR: 0.00000016  \n",
      "Epoch: [2][6300/7353] Elapsed 86m 49s (remain 14m 29s) Loss: 0.7399(0.8334) Grad: 424176.1250  LR: 0.00000016  \n",
      "Epoch: [2][6400/7353] Elapsed 88m 12s (remain 13m 7s) Loss: 1.0809(0.8329) Grad: 228485.2031  LR: 0.00000016  \n",
      "Epoch: [2][6500/7353] Elapsed 89m 35s (remain 11m 44s) Loss: 0.9131(0.8323) Grad: 223201.2500  LR: 0.00000015  \n",
      "Epoch: [2][6600/7353] Elapsed 90m 57s (remain 10m 21s) Loss: 0.8185(0.8323) Grad: 219108.2812  LR: 0.00000015  \n",
      "Epoch: [2][6700/7353] Elapsed 92m 20s (remain 8m 59s) Loss: 0.5718(0.8312) Grad: 179686.1250  LR: 0.00000015  \n",
      "Epoch: [2][6800/7353] Elapsed 93m 43s (remain 7m 36s) Loss: 1.6255(0.8304) Grad: 565607.4375  LR: 0.00000014  \n",
      "Epoch: [2][6900/7353] Elapsed 95m 5s (remain 6m 13s) Loss: 0.8226(0.8300) Grad: 306013.9688  LR: 0.00000014  \n",
      "Epoch: [2][7000/7353] Elapsed 96m 28s (remain 4m 51s) Loss: 1.3066(0.8297) Grad: 298918.5000  LR: 0.00000014  \n",
      "Epoch: [2][7100/7353] Elapsed 97m 51s (remain 3m 28s) Loss: 1.4182(0.8302) Grad: 525553.6875  LR: 0.00000013  \n",
      "Epoch: [2][7200/7353] Elapsed 99m 14s (remain 2m 5s) Loss: 0.7175(0.8295) Grad: 218725.9844  LR: 0.00000013  \n",
      "Epoch: [2][7300/7353] Elapsed 100m 36s (remain 0m 42s) Loss: 0.7016(0.8290) Grad: 330956.0625  LR: 0.00000013  \n",
      "Epoch: [2][7352/7353] Elapsed 101m 19s (remain 0m 0s) Loss: 0.4921(0.8288) Grad: 395086.6250  LR: 0.00000013  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 44s) Loss: 0.3638(0.3638) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 44s) Loss: 1.8731(0.7240) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 40s) Loss: 0.4426(0.7676) \n",
      "EVAL: [300/7353] Elapsed 0m 16s (remain 6m 34s) Loss: 0.4332(0.7900) \n",
      "EVAL: [400/7353] Elapsed 0m 22s (remain 6m 31s) Loss: 0.3644(0.7902) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 25s) Loss: 0.3431(0.7951) \n",
      "EVAL: [600/7353] Elapsed 0m 33s (remain 6m 19s) Loss: 0.4288(0.8029) \n",
      "EVAL: [700/7353] Elapsed 0m 39s (remain 6m 13s) Loss: 0.3193(0.7853) \n",
      "EVAL: [800/7353] Elapsed 0m 44s (remain 6m 7s) Loss: 0.3263(0.7855) \n",
      "EVAL: [900/7353] Elapsed 0m 50s (remain 6m 2s) Loss: 1.2106(0.7932) \n",
      "EVAL: [1000/7353] Elapsed 0m 56s (remain 5m 56s) Loss: 0.3773(0.7978) \n",
      "EVAL: [1100/7353] Elapsed 1m 1s (remain 5m 50s) Loss: 0.2621(0.7881) \n",
      "EVAL: [1200/7353] Elapsed 1m 7s (remain 5m 45s) Loss: 1.7149(0.7853) \n",
      "EVAL: [1300/7353] Elapsed 1m 13s (remain 5m 39s) Loss: 1.0643(0.7798) \n",
      "EVAL: [1400/7353] Elapsed 1m 18s (remain 5m 34s) Loss: 0.3610(0.7770) \n",
      "EVAL: [1500/7353] Elapsed 1m 24s (remain 5m 28s) Loss: 0.3799(0.7767) \n",
      "EVAL: [1600/7353] Elapsed 1m 29s (remain 5m 22s) Loss: 0.3818(0.7809) \n",
      "EVAL: [1700/7353] Elapsed 1m 35s (remain 5m 17s) Loss: 0.8740(0.7749) \n",
      "EVAL: [1800/7353] Elapsed 1m 41s (remain 5m 11s) Loss: 0.4277(0.7694) \n",
      "EVAL: [1900/7353] Elapsed 1m 46s (remain 5m 6s) Loss: 2.5240(0.7794) \n",
      "EVAL: [2000/7353] Elapsed 1m 52s (remain 5m 0s) Loss: 0.4015(0.7854) \n",
      "EVAL: [2100/7353] Elapsed 1m 58s (remain 4m 55s) Loss: 0.6170(0.7826) \n",
      "EVAL: [2200/7353] Elapsed 2m 3s (remain 4m 49s) Loss: 0.3490(0.7873) \n",
      "EVAL: [2300/7353] Elapsed 2m 9s (remain 4m 43s) Loss: 0.4595(0.7857) \n",
      "EVAL: [2400/7353] Elapsed 2m 14s (remain 4m 38s) Loss: 0.2867(0.7859) \n",
      "EVAL: [2500/7353] Elapsed 2m 20s (remain 4m 32s) Loss: 0.5008(0.7848) \n",
      "EVAL: [2600/7353] Elapsed 2m 26s (remain 4m 27s) Loss: 0.3138(0.7806) \n",
      "EVAL: [2700/7353] Elapsed 2m 31s (remain 4m 21s) Loss: 0.5745(0.7788) \n",
      "EVAL: [2800/7353] Elapsed 2m 37s (remain 4m 16s) Loss: 0.7155(0.7743) \n",
      "EVAL: [2900/7353] Elapsed 2m 43s (remain 4m 10s) Loss: 3.1429(0.7782) \n",
      "EVAL: [3000/7353] Elapsed 2m 48s (remain 4m 4s) Loss: 0.8165(0.7782) \n",
      "EVAL: [3100/7353] Elapsed 2m 54s (remain 3m 59s) Loss: 0.9660(0.7761) \n",
      "EVAL: [3200/7353] Elapsed 2m 59s (remain 3m 53s) Loss: 0.5662(0.7752) \n",
      "EVAL: [3300/7353] Elapsed 3m 5s (remain 3m 47s) Loss: 1.3580(0.7755) \n",
      "EVAL: [3400/7353] Elapsed 3m 11s (remain 3m 42s) Loss: 0.3745(0.7766) \n",
      "EVAL: [3500/7353] Elapsed 3m 16s (remain 3m 36s) Loss: 0.5520(0.7773) \n",
      "EVAL: [3600/7353] Elapsed 3m 22s (remain 3m 30s) Loss: 0.3514(0.7749) \n",
      "EVAL: [3700/7353] Elapsed 3m 28s (remain 3m 25s) Loss: 0.3923(0.7765) \n",
      "EVAL: [3800/7353] Elapsed 3m 33s (remain 3m 19s) Loss: 1.3108(0.7763) \n",
      "EVAL: [3900/7353] Elapsed 3m 39s (remain 3m 13s) Loss: 0.3570(0.7745) \n",
      "EVAL: [4000/7353] Elapsed 3m 44s (remain 3m 8s) Loss: 0.3446(0.7750) \n",
      "EVAL: [4100/7353] Elapsed 3m 50s (remain 3m 2s) Loss: 0.9248(0.7749) \n",
      "EVAL: [4200/7353] Elapsed 3m 55s (remain 2m 56s) Loss: 0.5149(0.7782) \n",
      "EVAL: [4300/7353] Elapsed 4m 1s (remain 2m 51s) Loss: 2.0245(0.7808) \n",
      "EVAL: [4400/7353] Elapsed 4m 7s (remain 2m 45s) Loss: 0.7012(0.7847) \n",
      "EVAL: [4500/7353] Elapsed 4m 12s (remain 2m 40s) Loss: 1.5767(0.7850) \n",
      "EVAL: [4600/7353] Elapsed 4m 18s (remain 2m 34s) Loss: 0.8725(0.7901) \n",
      "EVAL: [4700/7353] Elapsed 4m 23s (remain 2m 28s) Loss: 1.2018(0.7920) \n",
      "EVAL: [4800/7353] Elapsed 4m 29s (remain 2m 23s) Loss: 1.1590(0.7911) \n",
      "EVAL: [4900/7353] Elapsed 4m 34s (remain 2m 17s) Loss: 0.9805(0.7917) \n",
      "EVAL: [5000/7353] Elapsed 4m 40s (remain 2m 11s) Loss: 0.7215(0.7932) \n",
      "EVAL: [5100/7353] Elapsed 4m 46s (remain 2m 6s) Loss: 1.2163(0.7945) \n",
      "EVAL: [5200/7353] Elapsed 4m 51s (remain 2m 0s) Loss: 0.3200(0.7949) \n",
      "EVAL: [5300/7353] Elapsed 4m 57s (remain 1m 55s) Loss: 0.4650(0.7941) \n",
      "EVAL: [5400/7353] Elapsed 5m 2s (remain 1m 49s) Loss: 0.8130(0.7958) \n",
      "EVAL: [5500/7353] Elapsed 5m 8s (remain 1m 43s) Loss: 0.3918(0.7964) \n",
      "EVAL: [5600/7353] Elapsed 5m 13s (remain 1m 38s) Loss: 1.6388(0.7964) \n",
      "EVAL: [5700/7353] Elapsed 5m 19s (remain 1m 32s) Loss: 1.0100(0.7997) \n",
      "EVAL: [5800/7353] Elapsed 5m 24s (remain 1m 26s) Loss: 1.3409(0.8032) \n",
      "EVAL: [5900/7353] Elapsed 5m 30s (remain 1m 21s) Loss: 0.4764(0.8017) \n",
      "EVAL: [6000/7353] Elapsed 5m 36s (remain 1m 15s) Loss: 0.3080(0.8026) \n",
      "EVAL: [6100/7353] Elapsed 5m 41s (remain 1m 10s) Loss: 0.4205(0.8026) \n",
      "EVAL: [6200/7353] Elapsed 5m 47s (remain 1m 4s) Loss: 0.4222(0.8024) \n",
      "EVAL: [6300/7353] Elapsed 5m 52s (remain 0m 58s) Loss: 0.4926(0.8024) \n",
      "EVAL: [6400/7353] Elapsed 5m 58s (remain 0m 53s) Loss: 0.4946(0.8041) \n",
      "EVAL: [6500/7353] Elapsed 6m 4s (remain 0m 47s) Loss: 0.3263(0.8035) \n",
      "EVAL: [6600/7353] Elapsed 6m 9s (remain 0m 42s) Loss: 1.0228(0.8030) \n",
      "EVAL: [6700/7353] Elapsed 6m 15s (remain 0m 36s) Loss: 0.3297(0.8018) \n",
      "EVAL: [6800/7353] Elapsed 6m 21s (remain 0m 30s) Loss: 0.3978(0.8015) \n",
      "EVAL: [6900/7353] Elapsed 6m 26s (remain 0m 25s) Loss: 1.2789(0.8017) \n",
      "EVAL: [7000/7353] Elapsed 6m 32s (remain 0m 19s) Loss: 0.2626(0.8008) \n",
      "EVAL: [7100/7353] Elapsed 6m 38s (remain 0m 14s) Loss: 0.4280(0.7993) \n",
      "EVAL: [7200/7353] Elapsed 6m 44s (remain 0m 8s) Loss: 0.2364(0.8001) \n",
      "EVAL: [7300/7353] Elapsed 6m 49s (remain 0m 2s) Loss: 3.2189(0.8010) \n",
      "EVAL: [7352/7353] Elapsed 6m 52s (remain 0m 0s) Loss: 0.7431(0.8001) \n",
      "Epoch 2 - Save Best Score: 0.8001 Model\n",
      "Epoch: [3][0/7353] Elapsed 0m 0s (remain 102m 9s) Loss: 0.6321(0.6321) Grad: 804781.5000  LR: 0.00000012  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/7353] Elapsed 1m 23s (remain 99m 49s) Loss: 1.2926(0.7598) Grad: 500159.1562  LR: 0.00000012  \n",
      "Epoch: [3][200/7353] Elapsed 2m 46s (remain 98m 32s) Loss: 0.8946(0.7805) Grad: 264112.9062  LR: 0.00000012  \n",
      "Epoch: [3][300/7353] Elapsed 4m 9s (remain 97m 14s) Loss: 0.8224(0.7907) Grad: 251634.5781  LR: 0.00000012  \n",
      "Epoch: [3][400/7353] Elapsed 5m 31s (remain 95m 49s) Loss: 1.0052(0.7923) Grad: 333527.3438  LR: 0.00000011  \n",
      "Epoch: [3][500/7353] Elapsed 6m 54s (remain 94m 25s) Loss: 1.1322(0.8054) Grad: 366649.1250  LR: 0.00000011  \n",
      "Epoch: [3][600/7353] Elapsed 8m 16s (remain 93m 1s) Loss: 1.1505(0.8137) Grad: 483448.9375  LR: 0.00000011  \n",
      "Epoch: [3][700/7353] Elapsed 9m 39s (remain 91m 39s) Loss: 0.6620(0.8128) Grad: 238343.6875  LR: 0.00000010  \n",
      "Epoch: [3][800/7353] Elapsed 11m 2s (remain 90m 15s) Loss: 0.8829(0.8107) Grad: 221946.6250  LR: 0.00000010  \n",
      "Epoch: [3][900/7353] Elapsed 12m 24s (remain 88m 53s) Loss: 0.7355(0.8100) Grad: 178962.3906  LR: 0.00000010  \n",
      "Epoch: [3][1000/7353] Elapsed 13m 47s (remain 87m 30s) Loss: 0.7876(0.8131) Grad: 314030.0938  LR: 0.00000010  \n",
      "Epoch: [3][1100/7353] Elapsed 15m 10s (remain 86m 7s) Loss: 0.9714(0.8119) Grad: 342904.7500  LR: 0.00000009  \n",
      "Epoch: [3][1200/7353] Elapsed 16m 32s (remain 84m 44s) Loss: 0.6806(0.8100) Grad: 203738.1406  LR: 0.00000009  \n",
      "Epoch: [3][1300/7353] Elapsed 17m 55s (remain 83m 22s) Loss: 0.5372(0.8090) Grad: 362100.9688  LR: 0.00000009  \n",
      "Epoch: [3][1400/7353] Elapsed 19m 18s (remain 81m 59s) Loss: 0.6533(0.8050) Grad: 237745.2969  LR: 0.00000008  \n",
      "Epoch: [3][1500/7353] Elapsed 20m 40s (remain 80m 38s) Loss: 0.7421(0.8040) Grad: 340675.3750  LR: 0.00000008  \n",
      "Epoch: [3][1600/7353] Elapsed 22m 3s (remain 79m 15s) Loss: 1.2130(0.8043) Grad: 736061.1250  LR: 0.00000008  \n",
      "Epoch: [3][1700/7353] Elapsed 23m 26s (remain 77m 52s) Loss: 0.7353(0.8033) Grad: 216735.7500  LR: 0.00000008  \n",
      "Epoch: [3][1800/7353] Elapsed 24m 48s (remain 76m 30s) Loss: 1.1327(0.8030) Grad: 391551.5312  LR: 0.00000007  \n",
      "Epoch: [3][1900/7353] Elapsed 26m 11s (remain 75m 7s) Loss: 0.9974(0.8007) Grad: 405193.4375  LR: 0.00000007  \n",
      "Epoch: [3][2000/7353] Elapsed 27m 34s (remain 73m 44s) Loss: 0.6825(0.8010) Grad: 178532.5000  LR: 0.00000007  \n",
      "Epoch: [3][2100/7353] Elapsed 28m 56s (remain 72m 21s) Loss: 0.6354(0.8014) Grad: 408413.8438  LR: 0.00000007  \n",
      "Epoch: [3][2200/7353] Elapsed 30m 19s (remain 70m 58s) Loss: 1.1099(0.8018) Grad: 647514.8750  LR: 0.00000006  \n",
      "Epoch: [3][2300/7353] Elapsed 31m 41s (remain 69m 35s) Loss: 0.7230(0.8039) Grad: 189789.8125  LR: 0.00000006  \n",
      "Epoch: [3][2400/7353] Elapsed 33m 4s (remain 68m 13s) Loss: 0.9525(0.8022) Grad: 390433.1875  LR: 0.00000006  \n",
      "Epoch: [3][2500/7353] Elapsed 34m 27s (remain 66m 50s) Loss: 0.8280(0.8025) Grad: 255479.7500  LR: 0.00000006  \n",
      "Epoch: [3][2600/7353] Elapsed 35m 50s (remain 65m 28s) Loss: 0.7996(0.8015) Grad: 271863.3750  LR: 0.00000006  \n",
      "Epoch: [3][2700/7353] Elapsed 37m 12s (remain 64m 5s) Loss: 1.1871(0.8017) Grad: 502232.8750  LR: 0.00000005  \n",
      "Epoch: [3][2800/7353] Elapsed 38m 35s (remain 62m 42s) Loss: 0.7350(0.8013) Grad: 231182.6406  LR: 0.00000005  \n",
      "Epoch: [3][2900/7353] Elapsed 39m 58s (remain 61m 20s) Loss: 0.6837(0.8018) Grad: 362682.7812  LR: 0.00000005  \n",
      "Epoch: [3][3000/7353] Elapsed 41m 20s (remain 59m 57s) Loss: 0.4045(0.8020) Grad: 288253.5938  LR: 0.00000005  \n",
      "Epoch: [3][3100/7353] Elapsed 42m 43s (remain 58m 34s) Loss: 0.7612(0.8041) Grad: 221770.2656  LR: 0.00000004  \n",
      "Epoch: [3][3200/7353] Elapsed 44m 6s (remain 57m 12s) Loss: 1.2523(0.8042) Grad: 546017.6875  LR: 0.00000004  \n",
      "Epoch: [3][3300/7353] Elapsed 45m 28s (remain 55m 49s) Loss: 0.6125(0.8038) Grad: 174628.3438  LR: 0.00000004  \n",
      "Epoch: [3][3400/7353] Elapsed 46m 51s (remain 54m 26s) Loss: 1.3950(0.8039) Grad: 600781.7500  LR: 0.00000004  \n",
      "Epoch: [3][3500/7353] Elapsed 48m 13s (remain 53m 4s) Loss: 0.6142(0.8036) Grad: 156623.3125  LR: 0.00000004  \n",
      "Epoch: [3][3600/7353] Elapsed 49m 36s (remain 51m 41s) Loss: 0.6499(0.8027) Grad: 268571.1250  LR: 0.00000003  \n",
      "Epoch: [3][3700/7353] Elapsed 50m 59s (remain 50m 18s) Loss: 0.8849(0.8030) Grad: 255777.4375  LR: 0.00000003  \n",
      "Epoch: [3][3800/7353] Elapsed 52m 21s (remain 48m 55s) Loss: 0.6509(0.8032) Grad: 225002.4531  LR: 0.00000003  \n",
      "Epoch: [3][3900/7353] Elapsed 53m 44s (remain 47m 33s) Loss: 0.5812(0.8031) Grad: 255051.0469  LR: 0.00000003  \n",
      "Epoch: [3][4000/7353] Elapsed 55m 7s (remain 46m 10s) Loss: 0.9507(0.8027) Grad: 398167.5000  LR: 0.00000003  \n",
      "Epoch: [3][4100/7353] Elapsed 56m 29s (remain 44m 48s) Loss: 0.6252(0.8030) Grad: 618882.2500  LR: 0.00000003  \n",
      "Epoch: [3][4200/7353] Elapsed 57m 52s (remain 43m 25s) Loss: 0.8035(0.8032) Grad: 301596.5625  LR: 0.00000002  \n",
      "Epoch: [3][4300/7353] Elapsed 59m 15s (remain 42m 2s) Loss: 0.8240(0.8024) Grad: 256443.9375  LR: 0.00000002  \n",
      "Epoch: [3][4400/7353] Elapsed 60m 37s (remain 40m 40s) Loss: 0.5955(0.8028) Grad: 219495.8594  LR: 0.00000002  \n",
      "Epoch: [3][4500/7353] Elapsed 62m 0s (remain 39m 17s) Loss: 0.6200(0.8026) Grad: 193215.8906  LR: 0.00000002  \n",
      "Epoch: [3][4600/7353] Elapsed 63m 23s (remain 37m 54s) Loss: 1.5130(0.8042) Grad: 479253.5312  LR: 0.00000002  \n",
      "Epoch: [3][4700/7353] Elapsed 64m 45s (remain 36m 32s) Loss: 0.8451(0.8044) Grad: 273076.0312  LR: 0.00000002  \n",
      "Epoch: [3][4800/7353] Elapsed 66m 8s (remain 35m 9s) Loss: 1.1273(0.8045) Grad: 515664.4062  LR: 0.00000002  \n",
      "Epoch: [3][4900/7353] Elapsed 67m 31s (remain 33m 46s) Loss: 0.5760(0.8037) Grad: 252861.0312  LR: 0.00000002  \n",
      "Epoch: [3][5000/7353] Elapsed 68m 53s (remain 32m 24s) Loss: 0.4243(0.8025) Grad: 310137.3125  LR: 0.00000001  \n",
      "Epoch: [3][5100/7353] Elapsed 70m 16s (remain 31m 1s) Loss: 0.8995(0.8027) Grad: 350225.1250  LR: 0.00000001  \n",
      "Epoch: [3][5200/7353] Elapsed 71m 39s (remain 29m 38s) Loss: 0.6959(0.8023) Grad: 468795.0938  LR: 0.00000001  \n",
      "Epoch: [3][5300/7353] Elapsed 73m 1s (remain 28m 16s) Loss: 1.3056(0.8025) Grad: 293456.9375  LR: 0.00000001  \n",
      "Epoch: [3][5400/7353] Elapsed 74m 24s (remain 26m 53s) Loss: 0.7443(0.8023) Grad: 271895.6875  LR: 0.00000001  \n",
      "Epoch: [3][5500/7353] Elapsed 75m 47s (remain 25m 30s) Loss: 0.8687(0.8019) Grad: 341114.1875  LR: 0.00000001  \n",
      "Epoch: [3][5600/7353] Elapsed 77m 9s (remain 24m 8s) Loss: 0.4152(0.8017) Grad: 190318.6562  LR: 0.00000001  \n",
      "Epoch: [3][5700/7353] Elapsed 78m 32s (remain 22m 45s) Loss: 1.2357(0.8021) Grad: 291004.4062  LR: 0.00000001  \n",
      "Epoch: [3][5800/7353] Elapsed 79m 55s (remain 21m 22s) Loss: 0.7416(0.8030) Grad: 303994.1250  LR: 0.00000001  \n",
      "Epoch: [3][5900/7353] Elapsed 81m 18s (remain 20m 0s) Loss: 0.7397(0.8035) Grad: 329946.8438  LR: 0.00000001  \n",
      "Epoch: [3][6000/7353] Elapsed 82m 40s (remain 18m 37s) Loss: 1.1037(0.8034) Grad: 390163.8750  LR: 0.00000000  \n",
      "Epoch: [3][6100/7353] Elapsed 84m 3s (remain 17m 15s) Loss: 1.1312(0.8034) Grad: 490270.7812  LR: 0.00000000  \n",
      "Epoch: [3][6200/7353] Elapsed 85m 26s (remain 15m 52s) Loss: 1.1956(0.8030) Grad: 405005.6562  LR: 0.00000000  \n",
      "Epoch: [3][6300/7353] Elapsed 86m 48s (remain 14m 29s) Loss: 1.0116(0.8027) Grad: 458373.0000  LR: 0.00000000  \n",
      "Epoch: [3][6400/7353] Elapsed 88m 11s (remain 13m 6s) Loss: 0.7998(0.8031) Grad: 235139.1250  LR: 0.00000000  \n",
      "Epoch: [3][6500/7353] Elapsed 89m 34s (remain 11m 44s) Loss: 0.5698(0.8034) Grad: 387520.2188  LR: 0.00000000  \n",
      "Epoch: [3][6600/7353] Elapsed 90m 56s (remain 10m 21s) Loss: 0.6513(0.8035) Grad: 430180.6250  LR: 0.00000000  \n",
      "Epoch: [3][6700/7353] Elapsed 92m 19s (remain 8m 58s) Loss: 0.6581(0.8029) Grad: 257585.1719  LR: 0.00000000  \n",
      "Epoch: [3][6800/7353] Elapsed 93m 42s (remain 7m 36s) Loss: 0.8253(0.8032) Grad: 447119.9062  LR: 0.00000000  \n",
      "Epoch: [3][6900/7353] Elapsed 95m 5s (remain 6m 13s) Loss: 0.6010(0.8034) Grad: 156736.6094  LR: 0.00000000  \n",
      "Epoch: [3][7000/7353] Elapsed 96m 27s (remain 4m 51s) Loss: 0.6075(0.8035) Grad: 236158.0000  LR: 0.00000000  \n",
      "Epoch: [3][7100/7353] Elapsed 97m 50s (remain 3m 28s) Loss: 0.9658(0.8036) Grad: 297961.8750  LR: 0.00000000  \n",
      "Epoch: [3][7200/7353] Elapsed 99m 13s (remain 2m 5s) Loss: 0.7354(0.8034) Grad: 248951.8594  LR: 0.00000000  \n",
      "Epoch: [3][7300/7353] Elapsed 100m 36s (remain 0m 42s) Loss: 0.7499(0.8039) Grad: 216136.7500  LR: 0.00000000  \n",
      "Epoch: [3][7352/7353] Elapsed 101m 19s (remain 0m 0s) Loss: 0.6904(0.8037) Grad: 214641.3906  LR: 0.00000000  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 48s) Loss: 0.3748(0.3748) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 51s) Loss: 1.7467(0.6967) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 41s) Loss: 0.5201(0.7555) \n",
      "EVAL: [300/7353] Elapsed 0m 16s (remain 6m 35s) Loss: 0.4520(0.7745) \n",
      "EVAL: [400/7353] Elapsed 0m 22s (remain 6m 30s) Loss: 0.4067(0.7768) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 24s) Loss: 0.3873(0.7793) \n",
      "EVAL: [600/7353] Elapsed 0m 33s (remain 6m 19s) Loss: 0.4961(0.7865) \n",
      "EVAL: [700/7353] Elapsed 0m 39s (remain 6m 14s) Loss: 0.3185(0.7697) \n",
      "EVAL: [800/7353] Elapsed 0m 45s (remain 6m 8s) Loss: 0.3278(0.7700) \n",
      "EVAL: [900/7353] Elapsed 0m 50s (remain 6m 2s) Loss: 1.1601(0.7779) \n",
      "EVAL: [1000/7353] Elapsed 0m 56s (remain 5m 56s) Loss: 0.3659(0.7811) \n",
      "EVAL: [1100/7353] Elapsed 1m 1s (remain 5m 51s) Loss: 0.2905(0.7720) \n",
      "EVAL: [1200/7353] Elapsed 1m 7s (remain 5m 45s) Loss: 1.6127(0.7701) \n",
      "EVAL: [1300/7353] Elapsed 1m 13s (remain 5m 40s) Loss: 0.9710(0.7644) \n",
      "EVAL: [1400/7353] Elapsed 1m 18s (remain 5m 35s) Loss: 0.3687(0.7608) \n",
      "EVAL: [1500/7353] Elapsed 1m 24s (remain 5m 29s) Loss: 0.3534(0.7605) \n",
      "EVAL: [1600/7353] Elapsed 1m 30s (remain 5m 24s) Loss: 0.4302(0.7640) \n",
      "EVAL: [1700/7353] Elapsed 1m 35s (remain 5m 18s) Loss: 1.0550(0.7579) \n",
      "EVAL: [1800/7353] Elapsed 1m 41s (remain 5m 13s) Loss: 0.3856(0.7529) \n",
      "EVAL: [1900/7353] Elapsed 1m 47s (remain 5m 8s) Loss: 2.3886(0.7615) \n",
      "EVAL: [2000/7353] Elapsed 1m 53s (remain 5m 2s) Loss: 0.4453(0.7678) \n",
      "EVAL: [2100/7353] Elapsed 1m 58s (remain 4m 57s) Loss: 0.6724(0.7658) \n",
      "EVAL: [2200/7353] Elapsed 2m 4s (remain 4m 51s) Loss: 0.3561(0.7696) \n",
      "EVAL: [2300/7353] Elapsed 2m 10s (remain 4m 46s) Loss: 0.5536(0.7680) \n",
      "EVAL: [2400/7353] Elapsed 2m 16s (remain 4m 40s) Loss: 0.3215(0.7684) \n",
      "EVAL: [2500/7353] Elapsed 2m 21s (remain 4m 35s) Loss: 0.4233(0.7675) \n",
      "EVAL: [2600/7353] Elapsed 2m 27s (remain 4m 29s) Loss: 0.3086(0.7632) \n",
      "EVAL: [2700/7353] Elapsed 2m 33s (remain 4m 23s) Loss: 0.5200(0.7613) \n",
      "EVAL: [2800/7353] Elapsed 2m 38s (remain 4m 18s) Loss: 0.6309(0.7568) \n",
      "EVAL: [2900/7353] Elapsed 2m 44s (remain 4m 12s) Loss: 3.0306(0.7603) \n",
      "EVAL: [3000/7353] Elapsed 2m 50s (remain 4m 7s) Loss: 0.7606(0.7606) \n",
      "EVAL: [3100/7353] Elapsed 2m 56s (remain 4m 1s) Loss: 0.8680(0.7586) \n",
      "EVAL: [3200/7353] Elapsed 3m 1s (remain 3m 55s) Loss: 0.6257(0.7576) \n",
      "EVAL: [3300/7353] Elapsed 3m 7s (remain 3m 50s) Loss: 1.3027(0.7582) \n",
      "EVAL: [3400/7353] Elapsed 3m 13s (remain 3m 44s) Loss: 0.3434(0.7596) \n",
      "EVAL: [3500/7353] Elapsed 3m 18s (remain 3m 38s) Loss: 0.6300(0.7605) \n",
      "EVAL: [3600/7353] Elapsed 3m 24s (remain 3m 33s) Loss: 0.3487(0.7587) \n",
      "EVAL: [3700/7353] Elapsed 3m 30s (remain 3m 27s) Loss: 0.3839(0.7602) \n",
      "EVAL: [3800/7353] Elapsed 3m 35s (remain 3m 21s) Loss: 1.4611(0.7605) \n",
      "EVAL: [3900/7353] Elapsed 3m 41s (remain 3m 15s) Loss: 0.3622(0.7590) \n",
      "EVAL: [4000/7353] Elapsed 3m 47s (remain 3m 10s) Loss: 0.3518(0.7599) \n",
      "EVAL: [4100/7353] Elapsed 3m 52s (remain 3m 4s) Loss: 1.0546(0.7597) \n",
      "EVAL: [4200/7353] Elapsed 3m 58s (remain 2m 58s) Loss: 0.4211(0.7636) \n",
      "EVAL: [4300/7353] Elapsed 4m 4s (remain 2m 53s) Loss: 1.9484(0.7662) \n",
      "EVAL: [4400/7353] Elapsed 4m 9s (remain 2m 47s) Loss: 0.8200(0.7704) \n",
      "EVAL: [4500/7353] Elapsed 4m 15s (remain 2m 41s) Loss: 1.4773(0.7708) \n",
      "EVAL: [4600/7353] Elapsed 4m 21s (remain 2m 36s) Loss: 0.9273(0.7759) \n",
      "EVAL: [4700/7353] Elapsed 4m 26s (remain 2m 30s) Loss: 1.1479(0.7775) \n",
      "EVAL: [4800/7353] Elapsed 4m 32s (remain 2m 24s) Loss: 1.0703(0.7764) \n",
      "EVAL: [4900/7353] Elapsed 4m 38s (remain 2m 19s) Loss: 0.8805(0.7767) \n",
      "EVAL: [5000/7353] Elapsed 4m 43s (remain 2m 13s) Loss: 0.6232(0.7784) \n",
      "EVAL: [5100/7353] Elapsed 4m 49s (remain 2m 7s) Loss: 1.1784(0.7801) \n",
      "EVAL: [5200/7353] Elapsed 4m 55s (remain 2m 2s) Loss: 0.3216(0.7806) \n",
      "EVAL: [5300/7353] Elapsed 5m 0s (remain 1m 56s) Loss: 0.4424(0.7798) \n",
      "EVAL: [5400/7353] Elapsed 5m 6s (remain 1m 50s) Loss: 0.7680(0.7815) \n",
      "EVAL: [5500/7353] Elapsed 5m 12s (remain 1m 45s) Loss: 0.3848(0.7822) \n",
      "EVAL: [5600/7353] Elapsed 5m 18s (remain 1m 39s) Loss: 1.8036(0.7823) \n",
      "EVAL: [5700/7353] Elapsed 5m 23s (remain 1m 33s) Loss: 0.9548(0.7857) \n",
      "EVAL: [5800/7353] Elapsed 5m 29s (remain 1m 28s) Loss: 1.2646(0.7892) \n",
      "EVAL: [5900/7353] Elapsed 5m 35s (remain 1m 22s) Loss: 0.4289(0.7875) \n",
      "EVAL: [6000/7353] Elapsed 5m 40s (remain 1m 16s) Loss: 0.3079(0.7886) \n",
      "EVAL: [6100/7353] Elapsed 5m 46s (remain 1m 11s) Loss: 0.4895(0.7884) \n",
      "EVAL: [6200/7353] Elapsed 5m 52s (remain 1m 5s) Loss: 0.4163(0.7882) \n",
      "EVAL: [6300/7353] Elapsed 5m 57s (remain 0m 59s) Loss: 0.4752(0.7884) \n",
      "EVAL: [6400/7353] Elapsed 6m 3s (remain 0m 54s) Loss: 0.4810(0.7902) \n",
      "EVAL: [6500/7353] Elapsed 6m 9s (remain 0m 48s) Loss: 0.3099(0.7894) \n",
      "EVAL: [6600/7353] Elapsed 6m 15s (remain 0m 42s) Loss: 0.9126(0.7890) \n",
      "EVAL: [6700/7353] Elapsed 6m 20s (remain 0m 37s) Loss: 0.3708(0.7878) \n",
      "EVAL: [6800/7353] Elapsed 6m 26s (remain 0m 31s) Loss: 0.4616(0.7880) \n",
      "EVAL: [6900/7353] Elapsed 6m 32s (remain 0m 25s) Loss: 1.4399(0.7883) \n",
      "EVAL: [7000/7353] Elapsed 6m 38s (remain 0m 20s) Loss: 0.2933(0.7874) \n",
      "EVAL: [7100/7353] Elapsed 6m 44s (remain 0m 14s) Loss: 0.3902(0.7862) \n",
      "EVAL: [7200/7353] Elapsed 6m 50s (remain 0m 8s) Loss: 0.2621(0.7870) \n",
      "EVAL: [7300/7353] Elapsed 6m 55s (remain 0m 2s) Loss: 3.0194(0.7879) \n",
      "EVAL: [7352/7353] Elapsed 6m 58s (remain 0m 0s) Loss: 0.6421(0.7868) \n",
      "Epoch 3 - Save Best Score: 0.7868 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7353] Elapsed 0m 0s (remain 44m 5s) Loss: 0.9629(0.9629) Grad: inf  LR: 0.00000050  \n",
      "Epoch: [1][100/7353] Elapsed 0m 41s (remain 49m 5s) Loss: 0.6503(1.0273) Grad: 454671.7500  LR: 0.00000050  \n",
      "Epoch: [1][200/7353] Elapsed 1m 21s (remain 48m 29s) Loss: 1.1749(0.9971) Grad: 615997.3125  LR: 0.00000050  \n",
      "Epoch: [1][300/7353] Elapsed 2m 2s (remain 47m 53s) Loss: 0.7081(1.0037) Grad: 472586.9688  LR: 0.00000050  \n",
      "Epoch: [1][400/7353] Elapsed 2m 43s (remain 47m 13s) Loss: 1.0322(0.9951) Grad: 695097.2500  LR: 0.00000050  \n",
      "Epoch: [1][500/7353] Elapsed 3m 24s (remain 46m 34s) Loss: 0.7720(0.9882) Grad: 581345.1875  LR: 0.00000050  \n",
      "Epoch: [1][600/7353] Elapsed 4m 4s (remain 45m 52s) Loss: 0.3571(0.9755) Grad: 521273.6562  LR: 0.00000050  \n",
      "Epoch: [1][700/7353] Elapsed 4m 45s (remain 45m 12s) Loss: 1.4233(0.9754) Grad: 808323.3750  LR: 0.00000050  \n",
      "Epoch: [1][800/7353] Elapsed 5m 26s (remain 44m 32s) Loss: 0.8393(0.9756) Grad: 498836.6250  LR: 0.00000050  \n",
      "Epoch: [1][900/7353] Elapsed 6m 7s (remain 43m 50s) Loss: 1.5167(0.9715) Grad: 559487.8750  LR: 0.00000050  \n",
      "Epoch: [1][1000/7353] Elapsed 6m 48s (remain 43m 10s) Loss: 1.4775(0.9670) Grad: 704165.2500  LR: 0.00000050  \n",
      "Epoch: [1][1100/7353] Elapsed 7m 28s (remain 42m 28s) Loss: 1.6073(0.9622) Grad: 1093661.2500  LR: 0.00000050  \n",
      "Epoch: [1][1200/7353] Elapsed 8m 9s (remain 41m 48s) Loss: 1.4463(0.9629) Grad: 941936.5000  LR: 0.00000050  \n",
      "Epoch: [1][1300/7353] Elapsed 8m 50s (remain 41m 7s) Loss: 0.9552(0.9572) Grad: 715571.2500  LR: 0.00000050  \n",
      "Epoch: [1][1400/7353] Elapsed 9m 31s (remain 40m 27s) Loss: 1.1713(0.9516) Grad: 682330.9375  LR: 0.00000050  \n",
      "Epoch: [1][1500/7353] Elapsed 10m 12s (remain 39m 46s) Loss: 1.3784(0.9499) Grad: 563168.3750  LR: 0.00000049  \n",
      "Epoch: [1][1600/7353] Elapsed 10m 52s (remain 39m 5s) Loss: 1.2867(0.9442) Grad: 1150835.1250  LR: 0.00000049  \n",
      "Epoch: [1][1700/7353] Elapsed 11m 33s (remain 38m 24s) Loss: 0.9057(0.9417) Grad: 558271.2500  LR: 0.00000049  \n",
      "Epoch: [1][1800/7353] Elapsed 12m 14s (remain 37m 44s) Loss: 1.0772(0.9387) Grad: 770391.9375  LR: 0.00000049  \n",
      "Epoch: [1][1900/7353] Elapsed 12m 55s (remain 37m 3s) Loss: 0.6642(0.9361) Grad: 1099136.8750  LR: 0.00000049  \n",
      "Epoch: [1][2000/7353] Elapsed 13m 36s (remain 36m 22s) Loss: 0.3207(0.9345) Grad: 422244.0312  LR: 0.00000049  \n",
      "Epoch: [1][2100/7353] Elapsed 14m 16s (remain 35m 42s) Loss: 1.5408(0.9309) Grad: 653561.1875  LR: 0.00000049  \n",
      "Epoch: [1][2200/7353] Elapsed 14m 57s (remain 35m 1s) Loss: 0.7833(0.9295) Grad: 417597.0938  LR: 0.00000049  \n",
      "Epoch: [1][2300/7353] Elapsed 15m 38s (remain 34m 20s) Loss: 1.4821(0.9276) Grad: 1377476.3750  LR: 0.00000049  \n",
      "Epoch: [1][2400/7353] Elapsed 16m 19s (remain 33m 39s) Loss: 1.3386(0.9249) Grad: 1218236.8750  LR: 0.00000049  \n",
      "Epoch: [1][2500/7353] Elapsed 17m 0s (remain 32m 58s) Loss: 0.6962(0.9238) Grad: 580966.1250  LR: 0.00000048  \n",
      "Epoch: [1][2600/7353] Elapsed 17m 40s (remain 32m 18s) Loss: 1.5243(0.9219) Grad: 901185.7500  LR: 0.00000048  \n",
      "Epoch: [1][2700/7353] Elapsed 18m 21s (remain 31m 37s) Loss: 0.5023(0.9185) Grad: 998310.1250  LR: 0.00000048  \n",
      "Epoch: [1][2800/7353] Elapsed 19m 2s (remain 30m 56s) Loss: 1.4174(0.9187) Grad: 492459.5625  LR: 0.00000048  \n",
      "Epoch: [1][2900/7353] Elapsed 19m 43s (remain 30m 15s) Loss: 1.3763(0.9190) Grad: 1362124.7500  LR: 0.00000048  \n",
      "Epoch: [1][3000/7353] Elapsed 20m 23s (remain 29m 34s) Loss: 1.1185(0.9161) Grad: 1090021.1250  LR: 0.00000048  \n",
      "Epoch: [1][3100/7353] Elapsed 21m 4s (remain 28m 54s) Loss: 1.0498(0.9154) Grad: 553977.0000  LR: 0.00000048  \n",
      "Epoch: [1][3200/7353] Elapsed 21m 45s (remain 28m 13s) Loss: 0.9102(0.9152) Grad: 581734.0000  LR: 0.00000047  \n",
      "Epoch: [1][3300/7353] Elapsed 22m 26s (remain 27m 32s) Loss: 1.0683(0.9137) Grad: 618558.1875  LR: 0.00000047  \n",
      "Epoch: [1][3400/7353] Elapsed 23m 7s (remain 26m 51s) Loss: 0.4379(0.9121) Grad: 542294.0000  LR: 0.00000047  \n",
      "Epoch: [1][3500/7353] Elapsed 23m 47s (remain 26m 11s) Loss: 0.6254(0.9096) Grad: 725200.3125  LR: 0.00000047  \n",
      "Epoch: [1][3600/7353] Elapsed 24m 28s (remain 25m 30s) Loss: 0.9654(0.9084) Grad: 755343.9375  LR: 0.00000047  \n",
      "Epoch: [1][3700/7353] Elapsed 25m 9s (remain 24m 49s) Loss: 0.7036(0.9086) Grad: 684467.8125  LR: 0.00000047  \n",
      "Epoch: [1][3800/7353] Elapsed 25m 50s (remain 24m 8s) Loss: 1.2367(0.9084) Grad: 443979.1250  LR: 0.00000046  \n",
      "Epoch: [1][3900/7353] Elapsed 26m 31s (remain 23m 27s) Loss: 0.7599(0.9078) Grad: 456390.8125  LR: 0.00000046  \n",
      "Epoch: [1][4000/7353] Elapsed 27m 11s (remain 22m 47s) Loss: 0.9667(0.9064) Grad: 755704.6250  LR: 0.00000046  \n",
      "Epoch: [1][4100/7353] Elapsed 27m 52s (remain 22m 6s) Loss: 0.7249(0.9052) Grad: 594161.5000  LR: 0.00000046  \n",
      "Epoch: [1][4200/7353] Elapsed 28m 33s (remain 21m 25s) Loss: 0.7782(0.9040) Grad: 424756.1875  LR: 0.00000046  \n",
      "Epoch: [1][4300/7353] Elapsed 29m 13s (remain 20m 44s) Loss: 1.1804(0.9027) Grad: 687010.8125  LR: 0.00000045  \n",
      "Epoch: [1][4400/7353] Elapsed 29m 54s (remain 20m 3s) Loss: 1.1887(0.9021) Grad: 523226.8125  LR: 0.00000045  \n",
      "Epoch: [1][4500/7353] Elapsed 30m 35s (remain 19m 23s) Loss: 0.4777(0.9015) Grad: 439004.7812  LR: 0.00000045  \n",
      "Epoch: [1][4600/7353] Elapsed 31m 16s (remain 18m 42s) Loss: 1.2427(0.9012) Grad: 822215.0625  LR: 0.00000045  \n",
      "Epoch: [1][4700/7353] Elapsed 31m 57s (remain 18m 1s) Loss: 0.5341(0.9004) Grad: 838516.8750  LR: 0.00000045  \n",
      "Epoch: [1][4800/7353] Elapsed 32m 37s (remain 17m 20s) Loss: 1.3885(0.8988) Grad: 1078100.6250  LR: 0.00000044  \n",
      "Epoch: [1][4900/7353] Elapsed 33m 18s (remain 16m 39s) Loss: 0.4747(0.8984) Grad: 520645.9375  LR: 0.00000044  \n",
      "Epoch: [1][5000/7353] Elapsed 33m 59s (remain 15m 59s) Loss: 1.3604(0.8987) Grad: 529766.1250  LR: 0.00000044  \n",
      "Epoch: [1][5100/7353] Elapsed 34m 40s (remain 15m 18s) Loss: 1.3815(0.8988) Grad: 609214.6250  LR: 0.00000044  \n",
      "Epoch: [1][5200/7353] Elapsed 35m 21s (remain 14m 37s) Loss: 1.4117(0.8973) Grad: 630401.6250  LR: 0.00000043  \n",
      "Epoch: [1][5300/7353] Elapsed 36m 1s (remain 13m 56s) Loss: 0.7837(0.8976) Grad: 708415.0625  LR: 0.00000043  \n",
      "Epoch: [1][5400/7353] Elapsed 36m 42s (remain 13m 16s) Loss: 0.5639(0.8970) Grad: 393118.8438  LR: 0.00000043  \n",
      "Epoch: [1][5500/7353] Elapsed 37m 23s (remain 12m 35s) Loss: 1.4402(0.8965) Grad: 533306.7500  LR: 0.00000043  \n",
      "Epoch: [1][5600/7353] Elapsed 38m 4s (remain 11m 54s) Loss: 0.9280(0.8950) Grad: 928143.5000  LR: 0.00000042  \n",
      "Epoch: [1][5700/7353] Elapsed 38m 45s (remain 11m 13s) Loss: 0.6279(0.8941) Grad: 657095.4375  LR: 0.00000042  \n",
      "Epoch: [1][5800/7353] Elapsed 39m 26s (remain 10m 33s) Loss: 0.9933(0.8938) Grad: 884925.7500  LR: 0.00000042  \n",
      "Epoch: [1][5900/7353] Elapsed 40m 6s (remain 9m 52s) Loss: 0.8405(0.8939) Grad: 1111563.0000  LR: 0.00000042  \n",
      "Epoch: [1][6000/7353] Elapsed 40m 47s (remain 9m 11s) Loss: 0.7226(0.8934) Grad: 325589.3125  LR: 0.00000041  \n",
      "Epoch: [1][6100/7353] Elapsed 41m 28s (remain 8m 30s) Loss: 0.7963(0.8921) Grad: 406622.8438  LR: 0.00000041  \n",
      "Epoch: [1][6200/7353] Elapsed 42m 9s (remain 7m 49s) Loss: 2.4524(0.8900) Grad: 2033831.3750  LR: 0.00000041  \n",
      "Epoch: [1][6300/7353] Elapsed 42m 50s (remain 7m 9s) Loss: 1.2274(0.8898) Grad: 530293.8750  LR: 0.00000041  \n",
      "Epoch: [1][6400/7353] Elapsed 43m 30s (remain 6m 28s) Loss: 0.5179(0.8892) Grad: 704973.8125  LR: 0.00000040  \n",
      "Epoch: [1][6500/7353] Elapsed 44m 11s (remain 5m 47s) Loss: 1.0049(0.8886) Grad: 1003557.5625  LR: 0.00000040  \n",
      "Epoch: [1][6600/7353] Elapsed 44m 52s (remain 5m 6s) Loss: 0.3314(0.8871) Grad: 378387.4062  LR: 0.00000040  \n",
      "Epoch: [1][6700/7353] Elapsed 45m 33s (remain 4m 25s) Loss: 0.8955(0.8862) Grad: 929188.8125  LR: 0.00000039  \n",
      "Epoch: [1][6800/7353] Elapsed 46m 13s (remain 3m 45s) Loss: 2.0940(0.8864) Grad: 1312578.8750  LR: 0.00000039  \n",
      "Epoch: [1][6900/7353] Elapsed 46m 54s (remain 3m 4s) Loss: 0.9858(0.8865) Grad: 1428966.5000  LR: 0.00000039  \n",
      "Epoch: [1][7000/7353] Elapsed 47m 35s (remain 2m 23s) Loss: 0.8433(0.8863) Grad: 660252.1875  LR: 0.00000039  \n",
      "Epoch: [1][7100/7353] Elapsed 48m 16s (remain 1m 42s) Loss: 0.5551(0.8857) Grad: 398882.8438  LR: 0.00000038  \n",
      "Epoch: [1][7200/7353] Elapsed 48m 57s (remain 1m 1s) Loss: 1.0594(0.8840) Grad: 788398.9375  LR: 0.00000038  \n",
      "Epoch: [1][7300/7353] Elapsed 49m 37s (remain 0m 21s) Loss: 0.4200(0.8836) Grad: 311352.7188  LR: 0.00000038  \n",
      "Epoch: [1][7352/7353] Elapsed 49m 59s (remain 0m 0s) Loss: 0.9956(0.8827) Grad: 932940.1250  LR: 0.00000038  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 59s) Loss: 0.4701(0.4701) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 56s) Loss: 0.1501(0.9523) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 51s) Loss: 0.5766(0.8870) \n",
      "EVAL: [300/7353] Elapsed 0m 17s (remain 6m 45s) Loss: 0.0520(0.8805) \n",
      "EVAL: [400/7353] Elapsed 0m 23s (remain 6m 39s) Loss: 0.2588(0.8628) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 33s) Loss: 0.0452(0.8303) \n",
      "EVAL: [600/7353] Elapsed 0m 34s (remain 6m 28s) Loss: 0.0237(0.8296) \n",
      "EVAL: [700/7353] Elapsed 0m 40s (remain 6m 22s) Loss: 0.3625(0.8138) \n",
      "EVAL: [800/7353] Elapsed 0m 46s (remain 6m 17s) Loss: 0.4959(0.8120) \n",
      "EVAL: [900/7353] Elapsed 0m 51s (remain 6m 11s) Loss: 1.8409(0.8284) \n",
      "EVAL: [1000/7353] Elapsed 0m 57s (remain 6m 5s) Loss: 2.9417(0.8296) \n",
      "EVAL: [1100/7353] Elapsed 1m 3s (remain 5m 59s) Loss: 1.5981(0.8365) \n",
      "EVAL: [1200/7353] Elapsed 1m 9s (remain 5m 54s) Loss: 2.9880(0.8490) \n",
      "EVAL: [1300/7353] Elapsed 1m 15s (remain 5m 48s) Loss: 3.0705(0.8623) \n",
      "EVAL: [1400/7353] Elapsed 1m 20s (remain 5m 43s) Loss: 0.5889(0.8684) \n",
      "EVAL: [1500/7353] Elapsed 1m 26s (remain 5m 37s) Loss: 1.5270(0.8717) \n",
      "EVAL: [1600/7353] Elapsed 1m 32s (remain 5m 31s) Loss: 0.6284(0.8679) \n",
      "EVAL: [1700/7353] Elapsed 1m 38s (remain 5m 25s) Loss: 0.4445(0.8656) \n",
      "EVAL: [1800/7353] Elapsed 1m 43s (remain 5m 20s) Loss: 1.1023(0.8697) \n",
      "EVAL: [1900/7353] Elapsed 1m 49s (remain 5m 14s) Loss: 0.0795(0.8731) \n",
      "EVAL: [2000/7353] Elapsed 1m 55s (remain 5m 8s) Loss: 0.0163(0.8674) \n",
      "EVAL: [2100/7353] Elapsed 2m 0s (remain 5m 2s) Loss: 1.2728(0.8746) \n",
      "EVAL: [2200/7353] Elapsed 2m 6s (remain 4m 56s) Loss: 0.2677(0.8794) \n",
      "EVAL: [2300/7353] Elapsed 2m 12s (remain 4m 51s) Loss: 0.6865(0.8802) \n",
      "EVAL: [2400/7353] Elapsed 2m 18s (remain 4m 45s) Loss: 0.6080(0.8818) \n",
      "EVAL: [2500/7353] Elapsed 2m 24s (remain 4m 39s) Loss: 0.4120(0.8766) \n",
      "EVAL: [2600/7353] Elapsed 2m 29s (remain 4m 33s) Loss: 1.9988(0.8726) \n",
      "EVAL: [2700/7353] Elapsed 2m 35s (remain 4m 27s) Loss: 0.3122(0.8702) \n",
      "EVAL: [2800/7353] Elapsed 2m 41s (remain 4m 22s) Loss: 0.5940(0.8645) \n",
      "EVAL: [2900/7353] Elapsed 2m 47s (remain 4m 16s) Loss: 1.9085(0.8755) \n",
      "EVAL: [3000/7353] Elapsed 2m 52s (remain 4m 10s) Loss: 0.3383(0.8757) \n",
      "EVAL: [3100/7353] Elapsed 2m 58s (remain 4m 4s) Loss: 0.3870(0.8714) \n",
      "EVAL: [3200/7353] Elapsed 3m 4s (remain 3m 58s) Loss: 1.9726(0.8678) \n",
      "EVAL: [3300/7353] Elapsed 3m 9s (remain 3m 53s) Loss: 0.3621(0.8713) \n",
      "EVAL: [3400/7353] Elapsed 3m 15s (remain 3m 47s) Loss: 1.0776(0.8718) \n",
      "EVAL: [3500/7353] Elapsed 3m 21s (remain 3m 41s) Loss: 0.3069(0.8724) \n",
      "EVAL: [3600/7353] Elapsed 3m 26s (remain 3m 35s) Loss: 0.0283(0.8690) \n",
      "EVAL: [3700/7353] Elapsed 3m 32s (remain 3m 29s) Loss: 0.0644(0.8698) \n",
      "EVAL: [3800/7353] Elapsed 3m 38s (remain 3m 24s) Loss: 1.4540(0.8703) \n",
      "EVAL: [3900/7353] Elapsed 3m 44s (remain 3m 18s) Loss: 0.4086(0.8688) \n",
      "EVAL: [4000/7353] Elapsed 3m 50s (remain 3m 12s) Loss: 0.2317(0.8764) \n",
      "EVAL: [4100/7353] Elapsed 3m 55s (remain 3m 6s) Loss: 0.0569(0.8732) \n",
      "EVAL: [4200/7353] Elapsed 4m 1s (remain 3m 1s) Loss: 1.5698(0.8738) \n",
      "EVAL: [4300/7353] Elapsed 4m 7s (remain 2m 55s) Loss: 1.4419(0.8756) \n",
      "EVAL: [4400/7353] Elapsed 4m 12s (remain 2m 49s) Loss: 1.6622(0.8712) \n",
      "EVAL: [4500/7353] Elapsed 4m 18s (remain 2m 43s) Loss: 0.0759(0.8712) \n",
      "EVAL: [4600/7353] Elapsed 4m 24s (remain 2m 38s) Loss: 1.3235(0.8729) \n",
      "EVAL: [4700/7353] Elapsed 4m 30s (remain 2m 32s) Loss: 1.9447(0.8745) \n",
      "EVAL: [4800/7353] Elapsed 4m 35s (remain 2m 26s) Loss: 1.0136(0.8754) \n",
      "EVAL: [4900/7353] Elapsed 4m 41s (remain 2m 20s) Loss: 0.3300(0.8754) \n",
      "EVAL: [5000/7353] Elapsed 4m 47s (remain 2m 15s) Loss: 1.0817(0.8777) \n",
      "EVAL: [5100/7353] Elapsed 4m 53s (remain 2m 9s) Loss: 0.3239(0.8795) \n",
      "EVAL: [5200/7353] Elapsed 4m 58s (remain 2m 3s) Loss: 0.3501(0.8827) \n",
      "EVAL: [5300/7353] Elapsed 5m 4s (remain 1m 57s) Loss: 0.3938(0.8825) \n",
      "EVAL: [5400/7353] Elapsed 5m 10s (remain 1m 52s) Loss: 1.9356(0.8791) \n",
      "EVAL: [5500/7353] Elapsed 5m 16s (remain 1m 46s) Loss: 0.3432(0.8802) \n",
      "EVAL: [5600/7353] Elapsed 5m 21s (remain 1m 40s) Loss: 0.1781(0.8799) \n",
      "EVAL: [5700/7353] Elapsed 5m 27s (remain 1m 34s) Loss: 0.1482(0.8818) \n",
      "EVAL: [5800/7353] Elapsed 5m 33s (remain 1m 29s) Loss: 0.2873(0.8801) \n",
      "EVAL: [5900/7353] Elapsed 5m 39s (remain 1m 23s) Loss: 1.1331(0.8795) \n",
      "EVAL: [6000/7353] Elapsed 5m 44s (remain 1m 17s) Loss: 0.7656(0.8771) \n",
      "EVAL: [6100/7353] Elapsed 5m 50s (remain 1m 11s) Loss: 2.5901(0.8775) \n",
      "EVAL: [6200/7353] Elapsed 5m 56s (remain 1m 6s) Loss: 1.4972(0.8765) \n",
      "EVAL: [6300/7353] Elapsed 6m 1s (remain 1m 0s) Loss: 2.1868(0.8761) \n",
      "EVAL: [6400/7353] Elapsed 6m 7s (remain 0m 54s) Loss: 1.3483(0.8760) \n",
      "EVAL: [6500/7353] Elapsed 6m 13s (remain 0m 48s) Loss: 0.1335(0.8772) \n",
      "EVAL: [6600/7353] Elapsed 6m 19s (remain 0m 43s) Loss: 0.0945(0.8763) \n",
      "EVAL: [6700/7353] Elapsed 6m 25s (remain 0m 37s) Loss: 0.0798(0.8777) \n",
      "EVAL: [6800/7353] Elapsed 6m 30s (remain 0m 31s) Loss: 1.3898(0.8790) \n",
      "EVAL: [6900/7353] Elapsed 6m 36s (remain 0m 25s) Loss: 1.3774(0.8756) \n",
      "EVAL: [7000/7353] Elapsed 6m 42s (remain 0m 20s) Loss: 0.0507(0.8788) \n",
      "EVAL: [7100/7353] Elapsed 6m 48s (remain 0m 14s) Loss: 0.0585(0.8778) \n",
      "EVAL: [7200/7353] Elapsed 6m 54s (remain 0m 8s) Loss: 0.1366(0.8778) \n",
      "EVAL: [7300/7353] Elapsed 6m 59s (remain 0m 2s) Loss: 0.0546(0.8779) \n",
      "EVAL: [7352/7353] Elapsed 7m 2s (remain 0m 0s) Loss: 4.3651(0.8785) \n",
      "Epoch 1 - Save Best Score: 0.8785 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/7353] Elapsed 0m 0s (remain 101m 58s) Loss: 0.6777(0.6777) Grad: inf  LR: 0.00000037  \n",
      "Epoch: [2][100/7353] Elapsed 1m 23s (remain 99m 58s) Loss: 1.1419(0.8288) Grad: 340843.7812  LR: 0.00000037  \n",
      "Epoch: [2][200/7353] Elapsed 2m 46s (remain 98m 41s) Loss: 1.4247(0.8507) Grad: 950418.0000  LR: 0.00000037  \n",
      "Epoch: [2][300/7353] Elapsed 4m 9s (remain 97m 16s) Loss: 0.6057(0.8611) Grad: 348337.0938  LR: 0.00000037  \n",
      "Epoch: [2][400/7353] Elapsed 5m 31s (remain 95m 51s) Loss: 0.3461(0.8554) Grad: 300258.7812  LR: 0.00000036  \n",
      "Epoch: [2][500/7353] Elapsed 6m 54s (remain 94m 29s) Loss: 1.2875(0.8522) Grad: 630457.3750  LR: 0.00000036  \n",
      "Epoch: [2][600/7353] Elapsed 8m 17s (remain 93m 6s) Loss: 0.9023(0.8545) Grad: 283211.0000  LR: 0.00000036  \n",
      "Epoch: [2][700/7353] Elapsed 9m 40s (remain 91m 51s) Loss: 1.0517(0.8549) Grad: 334680.5625  LR: 0.00000035  \n",
      "Epoch: [2][800/7353] Elapsed 11m 3s (remain 90m 29s) Loss: 0.8859(0.8545) Grad: 315763.5625  LR: 0.00000035  \n",
      "Epoch: [2][900/7353] Elapsed 12m 26s (remain 89m 4s) Loss: 0.5874(0.8520) Grad: 224883.4844  LR: 0.00000035  \n",
      "Epoch: [2][1000/7353] Elapsed 13m 49s (remain 87m 41s) Loss: 0.9894(0.8534) Grad: 381426.9062  LR: 0.00000034  \n",
      "Epoch: [2][1100/7353] Elapsed 15m 11s (remain 86m 17s) Loss: 1.1822(0.8506) Grad: 269433.8750  LR: 0.00000034  \n",
      "Epoch: [2][1200/7353] Elapsed 16m 34s (remain 84m 54s) Loss: 1.2609(0.8553) Grad: 291724.2812  LR: 0.00000034  \n",
      "Epoch: [2][1300/7353] Elapsed 17m 57s (remain 83m 31s) Loss: 0.6818(0.8499) Grad: 244450.1406  LR: 0.00000033  \n",
      "Epoch: [2][1400/7353] Elapsed 19m 20s (remain 82m 8s) Loss: 0.5312(0.8487) Grad: 225115.7344  LR: 0.00000033  \n",
      "Epoch: [2][1500/7353] Elapsed 20m 42s (remain 80m 45s) Loss: 0.9045(0.8466) Grad: 416909.8438  LR: 0.00000033  \n",
      "Epoch: [2][1600/7353] Elapsed 22m 5s (remain 79m 22s) Loss: 0.3392(0.8446) Grad: 271925.5625  LR: 0.00000032  \n",
      "Epoch: [2][1700/7353] Elapsed 23m 28s (remain 77m 58s) Loss: 0.8870(0.8435) Grad: 337010.5000  LR: 0.00000032  \n",
      "Epoch: [2][1800/7353] Elapsed 24m 50s (remain 76m 35s) Loss: 0.4919(0.8434) Grad: 226230.1250  LR: 0.00000032  \n",
      "Epoch: [2][1900/7353] Elapsed 26m 13s (remain 75m 12s) Loss: 0.7515(0.8442) Grad: 380630.5312  LR: 0.00000031  \n",
      "Epoch: [2][2000/7353] Elapsed 27m 36s (remain 73m 49s) Loss: 0.7380(0.8445) Grad: 260194.9844  LR: 0.00000031  \n",
      "Epoch: [2][2100/7353] Elapsed 28m 58s (remain 72m 26s) Loss: 0.8017(0.8458) Grad: 295544.1562  LR: 0.00000031  \n",
      "Epoch: [2][2200/7353] Elapsed 30m 21s (remain 71m 3s) Loss: 0.4092(0.8460) Grad: 227869.2031  LR: 0.00000030  \n",
      "Epoch: [2][2300/7353] Elapsed 31m 44s (remain 69m 40s) Loss: 1.1663(0.8457) Grad: 448254.0938  LR: 0.00000030  \n",
      "Epoch: [2][2400/7353] Elapsed 33m 6s (remain 68m 17s) Loss: 0.7513(0.8473) Grad: 240377.6406  LR: 0.00000030  \n",
      "Epoch: [2][2500/7353] Elapsed 34m 29s (remain 66m 54s) Loss: 0.5840(0.8456) Grad: 196675.9219  LR: 0.00000029  \n",
      "Epoch: [2][2600/7353] Elapsed 35m 52s (remain 65m 31s) Loss: 1.0186(0.8440) Grad: 378643.2188  LR: 0.00000029  \n",
      "Epoch: [2][2700/7353] Elapsed 37m 14s (remain 64m 8s) Loss: 0.9667(0.8413) Grad: 428800.5000  LR: 0.00000028  \n",
      "Epoch: [2][2800/7353] Elapsed 38m 37s (remain 62m 46s) Loss: 0.9093(0.8430) Grad: 286949.5938  LR: 0.00000028  \n",
      "Epoch: [2][2900/7353] Elapsed 40m 0s (remain 61m 23s) Loss: 0.9738(0.8412) Grad: 229150.5469  LR: 0.00000028  \n",
      "Epoch: [2][3000/7353] Elapsed 41m 22s (remain 60m 0s) Loss: 0.5489(0.8405) Grad: 273519.9062  LR: 0.00000027  \n",
      "Epoch: [2][3100/7353] Elapsed 42m 45s (remain 58m 37s) Loss: 1.0058(0.8406) Grad: 260287.0938  LR: 0.00000027  \n",
      "Epoch: [2][3200/7353] Elapsed 44m 8s (remain 57m 15s) Loss: 1.3144(0.8392) Grad: 311880.1250  LR: 0.00000027  \n",
      "Epoch: [2][3300/7353] Elapsed 45m 30s (remain 55m 52s) Loss: 0.9144(0.8399) Grad: 216245.3125  LR: 0.00000026  \n",
      "Epoch: [2][3400/7353] Elapsed 46m 53s (remain 54m 29s) Loss: 0.6168(0.8388) Grad: 414160.2188  LR: 0.00000026  \n",
      "Epoch: [2][3500/7353] Elapsed 48m 16s (remain 53m 6s) Loss: 0.5290(0.8383) Grad: 311692.9062  LR: 0.00000026  \n",
      "Epoch: [2][3600/7353] Elapsed 49m 38s (remain 51m 43s) Loss: 0.6411(0.8381) Grad: 339566.0625  LR: 0.00000025  \n",
      "Epoch: [2][3700/7353] Elapsed 51m 1s (remain 50m 20s) Loss: 0.8754(0.8371) Grad: 388560.2812  LR: 0.00000025  \n",
      "Epoch: [2][3800/7353] Elapsed 52m 24s (remain 48m 58s) Loss: 0.7057(0.8372) Grad: 171012.5312  LR: 0.00000025  \n",
      "Epoch: [2][3900/7353] Elapsed 53m 46s (remain 47m 35s) Loss: 0.8068(0.8369) Grad: 197714.0938  LR: 0.00000024  \n",
      "Epoch: [2][4000/7353] Elapsed 55m 9s (remain 46m 12s) Loss: 0.9549(0.8375) Grad: 324716.2812  LR: 0.00000024  \n",
      "Epoch: [2][4100/7353] Elapsed 56m 32s (remain 44m 50s) Loss: 0.8596(0.8373) Grad: 227836.4688  LR: 0.00000023  \n",
      "Epoch: [2][4200/7353] Elapsed 57m 55s (remain 43m 27s) Loss: 0.8943(0.8376) Grad: 434941.1250  LR: 0.00000023  \n",
      "Epoch: [2][4300/7353] Elapsed 59m 17s (remain 42m 4s) Loss: 1.1359(0.8375) Grad: 402742.1250  LR: 0.00000023  \n",
      "Epoch: [2][4400/7353] Elapsed 60m 40s (remain 40m 41s) Loss: 1.0256(0.8372) Grad: 263652.0312  LR: 0.00000022  \n",
      "Epoch: [2][4500/7353] Elapsed 62m 3s (remain 39m 19s) Loss: 0.4222(0.8362) Grad: 166165.5469  LR: 0.00000022  \n",
      "Epoch: [2][4600/7353] Elapsed 63m 25s (remain 37m 56s) Loss: 1.0098(0.8363) Grad: 349667.2812  LR: 0.00000022  \n",
      "Epoch: [2][4700/7353] Elapsed 64m 48s (remain 36m 33s) Loss: 0.8088(0.8364) Grad: 322875.7188  LR: 0.00000021  \n",
      "Epoch: [2][4800/7353] Elapsed 66m 11s (remain 35m 10s) Loss: 1.1343(0.8373) Grad: 428650.9375  LR: 0.00000021  \n",
      "Epoch: [2][4900/7353] Elapsed 67m 34s (remain 33m 48s) Loss: 0.4583(0.8366) Grad: 193558.8281  LR: 0.00000021  \n",
      "Epoch: [2][5000/7353] Elapsed 68m 56s (remain 32m 25s) Loss: 0.9344(0.8368) Grad: 203081.2812  LR: 0.00000020  \n",
      "Epoch: [2][5100/7353] Elapsed 70m 19s (remain 31m 2s) Loss: 0.4756(0.8369) Grad: 233288.4219  LR: 0.00000020  \n",
      "Epoch: [2][5200/7353] Elapsed 71m 42s (remain 29m 40s) Loss: 0.7928(0.8361) Grad: 245214.0156  LR: 0.00000020  \n",
      "Epoch: [2][5300/7353] Elapsed 73m 4s (remain 28m 17s) Loss: 1.3025(0.8358) Grad: 594890.8125  LR: 0.00000019  \n",
      "Epoch: [2][5400/7353] Elapsed 74m 27s (remain 26m 54s) Loss: 0.9676(0.8363) Grad: 343464.2812  LR: 0.00000019  \n",
      "Epoch: [2][5500/7353] Elapsed 75m 50s (remain 25m 32s) Loss: 0.7533(0.8355) Grad: 190260.1875  LR: 0.00000019  \n",
      "Epoch: [2][5600/7353] Elapsed 77m 13s (remain 24m 9s) Loss: 0.5426(0.8353) Grad: 144035.6094  LR: 0.00000018  \n",
      "Epoch: [2][5700/7353] Elapsed 78m 36s (remain 22m 46s) Loss: 1.1046(0.8359) Grad: 385584.9062  LR: 0.00000018  \n",
      "Epoch: [2][5800/7353] Elapsed 79m 58s (remain 21m 23s) Loss: 0.8235(0.8351) Grad: 588497.2500  LR: 0.00000018  \n",
      "Epoch: [2][5900/7353] Elapsed 81m 21s (remain 20m 1s) Loss: 1.0485(0.8338) Grad: 343039.3438  LR: 0.00000017  \n",
      "Epoch: [2][6000/7353] Elapsed 82m 44s (remain 18m 38s) Loss: 0.6953(0.8336) Grad: 285154.1250  LR: 0.00000017  \n",
      "Epoch: [2][6100/7353] Elapsed 84m 6s (remain 17m 15s) Loss: 0.8683(0.8332) Grad: 263761.1250  LR: 0.00000017  \n",
      "Epoch: [2][6200/7353] Elapsed 85m 29s (remain 15m 52s) Loss: 0.9017(0.8329) Grad: 323742.0938  LR: 0.00000016  \n",
      "Epoch: [2][6300/7353] Elapsed 86m 52s (remain 14m 30s) Loss: 0.5575(0.8327) Grad: 198126.9375  LR: 0.00000016  \n",
      "Epoch: [2][6400/7353] Elapsed 88m 14s (remain 13m 7s) Loss: 0.7842(0.8318) Grad: 417862.4375  LR: 0.00000016  \n",
      "Epoch: [2][6500/7353] Elapsed 89m 37s (remain 11m 44s) Loss: 0.4218(0.8314) Grad: 217618.5469  LR: 0.00000015  \n",
      "Epoch: [2][6600/7353] Elapsed 91m 0s (remain 10m 22s) Loss: 0.7764(0.8312) Grad: 228066.2188  LR: 0.00000015  \n",
      "Epoch: [2][6700/7353] Elapsed 92m 22s (remain 8m 59s) Loss: 1.3973(0.8308) Grad: 297979.5938  LR: 0.00000015  \n",
      "Epoch: [2][6800/7353] Elapsed 93m 45s (remain 7m 36s) Loss: 0.5923(0.8300) Grad: 406609.0312  LR: 0.00000014  \n",
      "Epoch: [2][6900/7353] Elapsed 95m 8s (remain 6m 13s) Loss: 0.8422(0.8301) Grad: 216433.0156  LR: 0.00000014  \n",
      "Epoch: [2][7000/7353] Elapsed 96m 30s (remain 4m 51s) Loss: 0.5298(0.8300) Grad: 234328.6250  LR: 0.00000014  \n",
      "Epoch: [2][7100/7353] Elapsed 97m 53s (remain 3m 28s) Loss: 0.9434(0.8298) Grad: 211971.0938  LR: 0.00000013  \n",
      "Epoch: [2][7200/7353] Elapsed 99m 16s (remain 2m 5s) Loss: 1.4170(0.8297) Grad: 762131.2500  LR: 0.00000013  \n",
      "Epoch: [2][7300/7353] Elapsed 100m 38s (remain 0m 43s) Loss: 0.7203(0.8296) Grad: 270757.4062  LR: 0.00000013  \n",
      "Epoch: [2][7352/7353] Elapsed 101m 21s (remain 0m 0s) Loss: 0.8266(0.8295) Grad: 332101.5312  LR: 0.00000013  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 37s) Loss: 0.4200(0.4200) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 53s) Loss: 0.6981(0.8077) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 48s) Loss: 0.4910(0.7806) \n",
      "EVAL: [300/7353] Elapsed 0m 17s (remain 6m 41s) Loss: 0.3696(0.7819) \n",
      "EVAL: [400/7353] Elapsed 0m 22s (remain 6m 35s) Loss: 0.3763(0.7609) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 29s) Loss: 0.3758(0.7495) \n",
      "EVAL: [600/7353] Elapsed 0m 34s (remain 6m 23s) Loss: 0.2828(0.7504) \n",
      "EVAL: [700/7353] Elapsed 0m 39s (remain 6m 17s) Loss: 0.4077(0.7410) \n",
      "EVAL: [800/7353] Elapsed 0m 45s (remain 6m 12s) Loss: 0.4366(0.7432) \n",
      "EVAL: [900/7353] Elapsed 0m 51s (remain 6m 7s) Loss: 1.4041(0.7487) \n",
      "EVAL: [1000/7353] Elapsed 0m 57s (remain 6m 1s) Loss: 1.2120(0.7483) \n",
      "EVAL: [1100/7353] Elapsed 1m 2s (remain 5m 55s) Loss: 1.6799(0.7490) \n",
      "EVAL: [1200/7353] Elapsed 1m 8s (remain 5m 50s) Loss: 1.1716(0.7514) \n",
      "EVAL: [1300/7353] Elapsed 1m 14s (remain 5m 44s) Loss: 1.2361(0.7549) \n",
      "EVAL: [1400/7353] Elapsed 1m 19s (remain 5m 38s) Loss: 0.4421(0.7538) \n",
      "EVAL: [1500/7353] Elapsed 1m 25s (remain 5m 32s) Loss: 0.8622(0.7559) \n",
      "EVAL: [1600/7353] Elapsed 1m 31s (remain 5m 27s) Loss: 0.5001(0.7546) \n",
      "EVAL: [1700/7353] Elapsed 1m 36s (remain 5m 21s) Loss: 0.4177(0.7543) \n",
      "EVAL: [1800/7353] Elapsed 1m 42s (remain 5m 15s) Loss: 1.1558(0.7571) \n",
      "EVAL: [1900/7353] Elapsed 1m 48s (remain 5m 10s) Loss: 0.4022(0.7602) \n",
      "EVAL: [2000/7353] Elapsed 1m 53s (remain 5m 4s) Loss: 0.2586(0.7542) \n",
      "EVAL: [2100/7353] Elapsed 1m 59s (remain 4m 58s) Loss: 1.1879(0.7599) \n",
      "EVAL: [2200/7353] Elapsed 2m 5s (remain 4m 52s) Loss: 0.3990(0.7614) \n",
      "EVAL: [2300/7353] Elapsed 2m 10s (remain 4m 46s) Loss: 1.1274(0.7628) \n",
      "EVAL: [2400/7353] Elapsed 2m 16s (remain 4m 41s) Loss: 0.4566(0.7655) \n",
      "EVAL: [2500/7353] Elapsed 2m 22s (remain 4m 35s) Loss: 0.4293(0.7647) \n",
      "EVAL: [2600/7353] Elapsed 2m 27s (remain 4m 30s) Loss: 1.5581(0.7614) \n",
      "EVAL: [2700/7353] Elapsed 2m 33s (remain 4m 24s) Loss: 0.3878(0.7593) \n",
      "EVAL: [2800/7353] Elapsed 2m 39s (remain 4m 18s) Loss: 0.4421(0.7555) \n",
      "EVAL: [2900/7353] Elapsed 2m 44s (remain 4m 13s) Loss: 0.8890(0.7604) \n",
      "EVAL: [3000/7353] Elapsed 2m 50s (remain 4m 7s) Loss: 0.4138(0.7624) \n",
      "EVAL: [3100/7353] Elapsed 2m 56s (remain 4m 1s) Loss: 0.4149(0.7616) \n",
      "EVAL: [3200/7353] Elapsed 3m 1s (remain 3m 55s) Loss: 1.4891(0.7608) \n",
      "EVAL: [3300/7353] Elapsed 3m 7s (remain 3m 50s) Loss: 0.4164(0.7634) \n",
      "EVAL: [3400/7353] Elapsed 3m 13s (remain 3m 44s) Loss: 1.1618(0.7648) \n",
      "EVAL: [3500/7353] Elapsed 3m 18s (remain 3m 38s) Loss: 0.3939(0.7663) \n",
      "EVAL: [3600/7353] Elapsed 3m 24s (remain 3m 33s) Loss: 0.3424(0.7657) \n",
      "EVAL: [3700/7353] Elapsed 3m 30s (remain 3m 27s) Loss: 0.4244(0.7669) \n",
      "EVAL: [3800/7353] Elapsed 3m 35s (remain 3m 21s) Loss: 1.3738(0.7691) \n",
      "EVAL: [3900/7353] Elapsed 3m 41s (remain 3m 16s) Loss: 0.4114(0.7694) \n",
      "EVAL: [4000/7353] Elapsed 3m 47s (remain 3m 10s) Loss: 0.3620(0.7763) \n",
      "EVAL: [4100/7353] Elapsed 3m 52s (remain 3m 4s) Loss: 0.4145(0.7750) \n",
      "EVAL: [4200/7353] Elapsed 3m 58s (remain 2m 59s) Loss: 1.7937(0.7772) \n",
      "EVAL: [4300/7353] Elapsed 4m 4s (remain 2m 53s) Loss: 1.2674(0.7779) \n",
      "EVAL: [4400/7353] Elapsed 4m 9s (remain 2m 47s) Loss: 0.8343(0.7759) \n",
      "EVAL: [4500/7353] Elapsed 4m 15s (remain 2m 41s) Loss: 0.4224(0.7764) \n",
      "EVAL: [4600/7353] Elapsed 4m 21s (remain 2m 36s) Loss: 1.2200(0.7776) \n",
      "EVAL: [4700/7353] Elapsed 4m 26s (remain 2m 30s) Loss: 0.8353(0.7771) \n",
      "EVAL: [4800/7353] Elapsed 4m 32s (remain 2m 24s) Loss: 1.3149(0.7773) \n",
      "EVAL: [4900/7353] Elapsed 4m 38s (remain 2m 19s) Loss: 0.3897(0.7784) \n",
      "EVAL: [5000/7353] Elapsed 4m 43s (remain 2m 13s) Loss: 0.6031(0.7799) \n",
      "EVAL: [5100/7353] Elapsed 4m 49s (remain 2m 7s) Loss: 0.4015(0.7823) \n",
      "EVAL: [5200/7353] Elapsed 4m 55s (remain 2m 2s) Loss: 0.7952(0.7831) \n",
      "EVAL: [5300/7353] Elapsed 5m 1s (remain 1m 56s) Loss: 0.4175(0.7834) \n",
      "EVAL: [5400/7353] Elapsed 5m 6s (remain 1m 50s) Loss: 1.6628(0.7817) \n",
      "EVAL: [5500/7353] Elapsed 5m 12s (remain 1m 45s) Loss: 0.3995(0.7834) \n",
      "EVAL: [5600/7353] Elapsed 5m 18s (remain 1m 39s) Loss: 0.6135(0.7836) \n",
      "EVAL: [5700/7353] Elapsed 5m 23s (remain 1m 33s) Loss: 0.6007(0.7853) \n",
      "EVAL: [5800/7353] Elapsed 5m 29s (remain 1m 28s) Loss: 0.3886(0.7836) \n",
      "EVAL: [5900/7353] Elapsed 5m 35s (remain 1m 22s) Loss: 1.1518(0.7835) \n",
      "EVAL: [6000/7353] Elapsed 5m 40s (remain 1m 16s) Loss: 0.5323(0.7820) \n",
      "EVAL: [6100/7353] Elapsed 5m 46s (remain 1m 11s) Loss: 1.7803(0.7826) \n",
      "EVAL: [6200/7353] Elapsed 5m 52s (remain 1m 5s) Loss: 1.7195(0.7826) \n",
      "EVAL: [6300/7353] Elapsed 5m 57s (remain 0m 59s) Loss: 2.3739(0.7830) \n",
      "EVAL: [6400/7353] Elapsed 6m 3s (remain 0m 54s) Loss: 0.6685(0.7828) \n",
      "EVAL: [6500/7353] Elapsed 6m 9s (remain 0m 48s) Loss: 0.5466(0.7837) \n",
      "EVAL: [6600/7353] Elapsed 6m 14s (remain 0m 42s) Loss: 0.5644(0.7834) \n",
      "EVAL: [6700/7353] Elapsed 6m 20s (remain 0m 37s) Loss: 0.4901(0.7836) \n",
      "EVAL: [6800/7353] Elapsed 6m 26s (remain 0m 31s) Loss: 0.7427(0.7851) \n",
      "EVAL: [6900/7353] Elapsed 6m 32s (remain 0m 25s) Loss: 0.6703(0.7841) \n",
      "EVAL: [7000/7353] Elapsed 6m 38s (remain 0m 20s) Loss: 0.3711(0.7837) \n",
      "EVAL: [7100/7353] Elapsed 6m 43s (remain 0m 14s) Loss: 0.4840(0.7830) \n",
      "EVAL: [7200/7353] Elapsed 6m 49s (remain 0m 8s) Loss: 0.6697(0.7820) \n",
      "EVAL: [7300/7353] Elapsed 6m 55s (remain 0m 2s) Loss: 0.3798(0.7814) \n",
      "EVAL: [7352/7353] Elapsed 6m 58s (remain 0m 0s) Loss: 2.3772(0.7811) \n",
      "Epoch 2 - Save Best Score: 0.7811 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/7353] Elapsed 0m 0s (remain 87m 52s) Loss: 0.9719(0.9719) Grad: inf  LR: 0.00000012  \n",
      "Epoch: [3][100/7353] Elapsed 1m 23s (remain 99m 37s) Loss: 0.5045(0.7818) Grad: 186376.5781  LR: 0.00000012  \n",
      "Epoch: [3][200/7353] Elapsed 2m 45s (remain 98m 22s) Loss: 1.0174(0.8170) Grad: 307359.5938  LR: 0.00000012  \n",
      "Epoch: [3][300/7353] Elapsed 4m 8s (remain 97m 5s) Loss: 0.5466(0.8022) Grad: 380228.3750  LR: 0.00000012  \n",
      "Epoch: [3][400/7353] Elapsed 5m 31s (remain 95m 47s) Loss: 0.8998(0.8067) Grad: 299002.9375  LR: 0.00000011  \n",
      "Epoch: [3][500/7353] Elapsed 6m 54s (remain 94m 24s) Loss: 1.0682(0.8034) Grad: 382363.9062  LR: 0.00000011  \n",
      "Epoch: [3][600/7353] Elapsed 8m 16s (remain 93m 2s) Loss: 0.8063(0.8037) Grad: 258571.5312  LR: 0.00000011  \n",
      "Epoch: [3][700/7353] Elapsed 9m 39s (remain 91m 41s) Loss: 0.4827(0.7955) Grad: 203530.7344  LR: 0.00000010  \n",
      "Epoch: [3][800/7353] Elapsed 11m 2s (remain 90m 18s) Loss: 0.7687(0.7953) Grad: 375046.7500  LR: 0.00000010  \n",
      "Epoch: [3][900/7353] Elapsed 12m 25s (remain 88m 56s) Loss: 1.0833(0.7930) Grad: 350466.0000  LR: 0.00000010  \n",
      "Epoch: [3][1000/7353] Elapsed 13m 47s (remain 87m 33s) Loss: 0.9074(0.7967) Grad: 481118.6250  LR: 0.00000010  \n",
      "Epoch: [3][1100/7353] Elapsed 15m 10s (remain 86m 11s) Loss: 1.0139(0.7973) Grad: 215934.7344  LR: 0.00000009  \n",
      "Epoch: [3][1200/7353] Elapsed 16m 33s (remain 84m 48s) Loss: 1.2754(0.7987) Grad: 603585.3750  LR: 0.00000009  \n",
      "Epoch: [3][1300/7353] Elapsed 17m 56s (remain 83m 25s) Loss: 0.5069(0.8004) Grad: 378461.0625  LR: 0.00000009  \n",
      "Epoch: [3][1400/7353] Elapsed 19m 18s (remain 82m 2s) Loss: 0.5068(0.8025) Grad: 237603.2812  LR: 0.00000008  \n",
      "Epoch: [3][1500/7353] Elapsed 20m 41s (remain 80m 40s) Loss: 0.9563(0.8071) Grad: 277941.2812  LR: 0.00000008  \n",
      "Epoch: [3][1600/7353] Elapsed 22m 4s (remain 79m 17s) Loss: 0.9999(0.8083) Grad: 318677.1250  LR: 0.00000008  \n",
      "Epoch: [3][1700/7353] Elapsed 23m 26s (remain 77m 54s) Loss: 0.7877(0.8097) Grad: 186960.7031  LR: 0.00000008  \n",
      "Epoch: [3][1800/7353] Elapsed 24m 49s (remain 76m 32s) Loss: 1.0438(0.8114) Grad: 253807.0312  LR: 0.00000007  \n",
      "Epoch: [3][1900/7353] Elapsed 26m 12s (remain 75m 10s) Loss: 0.8182(0.8102) Grad: 309458.2188  LR: 0.00000007  \n",
      "Epoch: [3][2000/7353] Elapsed 27m 35s (remain 73m 47s) Loss: 0.6818(0.8104) Grad: 202249.3906  LR: 0.00000007  \n",
      "Epoch: [3][2100/7353] Elapsed 28m 58s (remain 72m 24s) Loss: 0.3565(0.8086) Grad: 152014.4688  LR: 0.00000007  \n",
      "Epoch: [3][2200/7353] Elapsed 30m 20s (remain 71m 2s) Loss: 0.6860(0.8090) Grad: 244926.3750  LR: 0.00000006  \n",
      "Epoch: [3][2300/7353] Elapsed 31m 43s (remain 69m 39s) Loss: 0.5576(0.8074) Grad: 257940.7188  LR: 0.00000006  \n",
      "Epoch: [3][2400/7353] Elapsed 33m 6s (remain 68m 16s) Loss: 0.7717(0.8071) Grad: 203378.1875  LR: 0.00000006  \n",
      "Epoch: [3][2500/7353] Elapsed 34m 29s (remain 66m 54s) Loss: 1.0643(0.8074) Grad: 231863.3594  LR: 0.00000006  \n",
      "Epoch: [3][2600/7353] Elapsed 35m 51s (remain 65m 31s) Loss: 0.8925(0.8077) Grad: 216644.2188  LR: 0.00000006  \n",
      "Epoch: [3][2700/7353] Elapsed 37m 14s (remain 64m 8s) Loss: 0.7528(0.8071) Grad: 320559.8750  LR: 0.00000005  \n",
      "Epoch: [3][2800/7353] Elapsed 38m 36s (remain 62m 45s) Loss: 1.0946(0.8069) Grad: 428594.4688  LR: 0.00000005  \n",
      "Epoch: [3][2900/7353] Elapsed 39m 59s (remain 61m 22s) Loss: 1.1985(0.8076) Grad: 439696.9688  LR: 0.00000005  \n",
      "Epoch: [3][3000/7353] Elapsed 41m 22s (remain 59m 59s) Loss: 0.5535(0.8062) Grad: 234108.3125  LR: 0.00000005  \n",
      "Epoch: [3][3100/7353] Elapsed 42m 45s (remain 58m 37s) Loss: 1.2193(0.8068) Grad: 338461.0000  LR: 0.00000004  \n",
      "Epoch: [3][3200/7353] Elapsed 44m 7s (remain 57m 14s) Loss: 0.6603(0.8070) Grad: 330199.0000  LR: 0.00000004  \n",
      "Epoch: [3][3300/7353] Elapsed 45m 30s (remain 55m 51s) Loss: 0.8058(0.8085) Grad: 308443.1562  LR: 0.00000004  \n",
      "Epoch: [3][3400/7353] Elapsed 46m 53s (remain 54m 28s) Loss: 1.1016(0.8082) Grad: 549074.1875  LR: 0.00000004  \n",
      "Epoch: [3][3500/7353] Elapsed 48m 16s (remain 53m 6s) Loss: 0.4695(0.8068) Grad: 196783.2500  LR: 0.00000004  \n",
      "Epoch: [3][3600/7353] Elapsed 49m 38s (remain 51m 43s) Loss: 0.8799(0.8056) Grad: 347592.0000  LR: 0.00000003  \n",
      "Epoch: [3][3700/7353] Elapsed 51m 1s (remain 50m 20s) Loss: 0.4240(0.8050) Grad: 304463.8125  LR: 0.00000003  \n",
      "Epoch: [3][3800/7353] Elapsed 52m 24s (remain 48m 58s) Loss: 1.1492(0.8058) Grad: 394951.2188  LR: 0.00000003  \n",
      "Epoch: [3][3900/7353] Elapsed 53m 46s (remain 47m 35s) Loss: 0.8690(0.8056) Grad: 368786.0938  LR: 0.00000003  \n",
      "Epoch: [3][4000/7353] Elapsed 55m 9s (remain 46m 12s) Loss: 0.5707(0.8054) Grad: 398197.1250  LR: 0.00000003  \n",
      "Epoch: [3][4100/7353] Elapsed 56m 31s (remain 44m 49s) Loss: 0.6745(0.8050) Grad: 228124.3281  LR: 0.00000003  \n",
      "Epoch: [3][4200/7353] Elapsed 57m 54s (remain 43m 27s) Loss: 0.8285(0.8052) Grad: 399449.7188  LR: 0.00000002  \n",
      "Epoch: [3][4300/7353] Elapsed 59m 17s (remain 42m 4s) Loss: 0.5060(0.8051) Grad: 213110.5625  LR: 0.00000002  \n",
      "Epoch: [3][4400/7353] Elapsed 60m 40s (remain 40m 41s) Loss: 0.9841(0.8049) Grad: 289303.2500  LR: 0.00000002  \n",
      "Epoch: [3][4500/7353] Elapsed 62m 2s (remain 39m 18s) Loss: 0.3527(0.8046) Grad: 256872.5000  LR: 0.00000002  \n",
      "Epoch: [3][4600/7353] Elapsed 63m 25s (remain 37m 56s) Loss: 1.4755(0.8050) Grad: 322801.4062  LR: 0.00000002  \n",
      "Epoch: [3][4700/7353] Elapsed 64m 48s (remain 36m 33s) Loss: 0.6802(0.8047) Grad: 191575.2500  LR: 0.00000002  \n",
      "Epoch: [3][4800/7353] Elapsed 66m 10s (remain 35m 10s) Loss: 1.0782(0.8047) Grad: 368633.6250  LR: 0.00000002  \n",
      "Epoch: [3][4900/7353] Elapsed 67m 33s (remain 33m 48s) Loss: 0.7511(0.8049) Grad: 291615.9688  LR: 0.00000002  \n",
      "Epoch: [3][5000/7353] Elapsed 68m 56s (remain 32m 25s) Loss: 0.7275(0.8057) Grad: 259423.9844  LR: 0.00000001  \n",
      "Epoch: [3][5100/7353] Elapsed 70m 19s (remain 31m 2s) Loss: 0.4551(0.8053) Grad: 212803.4688  LR: 0.00000001  \n",
      "Epoch: [3][5200/7353] Elapsed 71m 41s (remain 29m 39s) Loss: 0.6151(0.8048) Grad: 389928.4375  LR: 0.00000001  \n",
      "Epoch: [3][5300/7353] Elapsed 73m 4s (remain 28m 17s) Loss: 1.7047(0.8041) Grad: 430857.8750  LR: 0.00000001  \n",
      "Epoch: [3][5400/7353] Elapsed 74m 26s (remain 26m 54s) Loss: 0.3900(0.8037) Grad: 215761.1406  LR: 0.00000001  \n",
      "Epoch: [3][5500/7353] Elapsed 75m 49s (remain 25m 31s) Loss: 0.9578(0.8036) Grad: 319234.9375  LR: 0.00000001  \n",
      "Epoch: [3][5600/7353] Elapsed 77m 12s (remain 24m 9s) Loss: 1.0721(0.8039) Grad: 440950.0938  LR: 0.00000001  \n",
      "Epoch: [3][5700/7353] Elapsed 78m 35s (remain 22m 46s) Loss: 0.9785(0.8037) Grad: 211311.5000  LR: 0.00000001  \n",
      "Epoch: [3][5800/7353] Elapsed 79m 58s (remain 21m 23s) Loss: 1.0787(0.8042) Grad: 343005.9062  LR: 0.00000001  \n",
      "Epoch: [3][5900/7353] Elapsed 81m 20s (remain 20m 0s) Loss: 0.6753(0.8048) Grad: 199942.0312  LR: 0.00000001  \n",
      "Epoch: [3][6000/7353] Elapsed 82m 43s (remain 18m 38s) Loss: 0.3231(0.8051) Grad: 161677.5625  LR: 0.00000000  \n",
      "Epoch: [3][6100/7353] Elapsed 84m 6s (remain 17m 15s) Loss: 0.5298(0.8051) Grad: 400875.6875  LR: 0.00000000  \n",
      "Epoch: [3][6200/7353] Elapsed 85m 28s (remain 15m 52s) Loss: 0.5157(0.8050) Grad: 197546.1094  LR: 0.00000000  \n",
      "Epoch: [3][6300/7353] Elapsed 86m 51s (remain 14m 30s) Loss: 0.8062(0.8058) Grad: 248402.5781  LR: 0.00000000  \n",
      "Epoch: [3][6400/7353] Elapsed 88m 14s (remain 13m 7s) Loss: 0.5271(0.8049) Grad: 138858.8750  LR: 0.00000000  \n",
      "Epoch: [3][6500/7353] Elapsed 89m 36s (remain 11m 44s) Loss: 0.5750(0.8047) Grad: 415849.1250  LR: 0.00000000  \n",
      "Epoch: [3][6600/7353] Elapsed 90m 59s (remain 10m 21s) Loss: 0.3093(0.8049) Grad: 210232.5156  LR: 0.00000000  \n",
      "Epoch: [3][6700/7353] Elapsed 92m 22s (remain 8m 59s) Loss: 1.0675(0.8059) Grad: 412605.5625  LR: 0.00000000  \n",
      "Epoch: [3][6800/7353] Elapsed 93m 44s (remain 7m 36s) Loss: 0.4084(0.8060) Grad: 221933.1719  LR: 0.00000000  \n",
      "Epoch: [3][6900/7353] Elapsed 95m 7s (remain 6m 13s) Loss: 0.8506(0.8052) Grad: 200522.7344  LR: 0.00000000  \n",
      "Epoch: [3][7000/7353] Elapsed 96m 30s (remain 4m 51s) Loss: 0.7130(0.8054) Grad: 291647.5938  LR: 0.00000000  \n",
      "Epoch: [3][7100/7353] Elapsed 97m 52s (remain 3m 28s) Loss: 0.7501(0.8053) Grad: 181786.2344  LR: 0.00000000  \n",
      "Epoch: [3][7200/7353] Elapsed 99m 15s (remain 2m 5s) Loss: 0.4238(0.8051) Grad: 202150.1094  LR: 0.00000000  \n",
      "Epoch: [3][7300/7353] Elapsed 100m 38s (remain 0m 43s) Loss: 0.9355(0.8050) Grad: 310332.1250  LR: 0.00000000  \n",
      "Epoch: [3][7352/7353] Elapsed 101m 21s (remain 0m 0s) Loss: 1.1283(0.8049) Grad: 397627.2812  LR: 0.00000000  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 55s) Loss: 0.3923(0.3923) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 45s) Loss: 0.6470(0.8114) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 44s) Loss: 0.4849(0.7835) \n",
      "EVAL: [300/7353] Elapsed 0m 16s (remain 6m 37s) Loss: 0.3617(0.7841) \n",
      "EVAL: [400/7353] Elapsed 0m 22s (remain 6m 34s) Loss: 0.3506(0.7608) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 29s) Loss: 0.3606(0.7482) \n",
      "EVAL: [600/7353] Elapsed 0m 34s (remain 6m 23s) Loss: 0.2768(0.7491) \n",
      "EVAL: [700/7353] Elapsed 0m 39s (remain 6m 18s) Loss: 0.3873(0.7388) \n",
      "EVAL: [800/7353] Elapsed 0m 45s (remain 6m 13s) Loss: 0.4083(0.7408) \n",
      "EVAL: [900/7353] Elapsed 0m 51s (remain 6m 8s) Loss: 1.4818(0.7468) \n",
      "EVAL: [1000/7353] Elapsed 0m 57s (remain 6m 2s) Loss: 1.2340(0.7461) \n",
      "EVAL: [1100/7353] Elapsed 1m 2s (remain 5m 56s) Loss: 1.6166(0.7471) \n",
      "EVAL: [1200/7353] Elapsed 1m 8s (remain 5m 51s) Loss: 1.1756(0.7495) \n",
      "EVAL: [1300/7353] Elapsed 1m 14s (remain 5m 45s) Loss: 1.2577(0.7535) \n",
      "EVAL: [1400/7353] Elapsed 1m 19s (remain 5m 39s) Loss: 0.4108(0.7521) \n",
      "EVAL: [1500/7353] Elapsed 1m 25s (remain 5m 34s) Loss: 0.8808(0.7545) \n",
      "EVAL: [1600/7353] Elapsed 1m 31s (remain 5m 28s) Loss: 0.4844(0.7536) \n",
      "EVAL: [1700/7353] Elapsed 1m 37s (remain 5m 22s) Loss: 0.3907(0.7533) \n",
      "EVAL: [1800/7353] Elapsed 1m 42s (remain 5m 17s) Loss: 1.2186(0.7565) \n",
      "EVAL: [1900/7353] Elapsed 1m 48s (remain 5m 11s) Loss: 0.3808(0.7597) \n",
      "EVAL: [2000/7353] Elapsed 1m 54s (remain 5m 5s) Loss: 0.2526(0.7531) \n",
      "EVAL: [2100/7353] Elapsed 2m 0s (remain 5m 0s) Loss: 1.2571(0.7591) \n",
      "EVAL: [2200/7353] Elapsed 2m 5s (remain 4m 54s) Loss: 0.3807(0.7611) \n",
      "EVAL: [2300/7353] Elapsed 2m 11s (remain 4m 48s) Loss: 1.0501(0.7625) \n",
      "EVAL: [2400/7353] Elapsed 2m 17s (remain 4m 43s) Loss: 0.4409(0.7653) \n",
      "EVAL: [2500/7353] Elapsed 2m 22s (remain 4m 37s) Loss: 0.4009(0.7640) \n",
      "EVAL: [2600/7353] Elapsed 2m 28s (remain 4m 31s) Loss: 1.6548(0.7606) \n",
      "EVAL: [2700/7353] Elapsed 2m 34s (remain 4m 25s) Loss: 0.3591(0.7587) \n",
      "EVAL: [2800/7353] Elapsed 2m 40s (remain 4m 20s) Loss: 0.4133(0.7548) \n",
      "EVAL: [2900/7353] Elapsed 2m 45s (remain 4m 14s) Loss: 0.8979(0.7601) \n",
      "EVAL: [3000/7353] Elapsed 2m 51s (remain 4m 8s) Loss: 0.3933(0.7618) \n",
      "EVAL: [3100/7353] Elapsed 2m 57s (remain 4m 2s) Loss: 0.3865(0.7609) \n",
      "EVAL: [3200/7353] Elapsed 3m 2s (remain 3m 57s) Loss: 1.5753(0.7601) \n",
      "EVAL: [3300/7353] Elapsed 3m 8s (remain 3m 51s) Loss: 0.3875(0.7630) \n",
      "EVAL: [3400/7353] Elapsed 3m 14s (remain 3m 45s) Loss: 1.2207(0.7647) \n",
      "EVAL: [3500/7353] Elapsed 3m 19s (remain 3m 39s) Loss: 0.3668(0.7662) \n",
      "EVAL: [3600/7353] Elapsed 3m 25s (remain 3m 34s) Loss: 0.3307(0.7655) \n",
      "EVAL: [3700/7353] Elapsed 3m 31s (remain 3m 28s) Loss: 0.4020(0.7669) \n",
      "EVAL: [3800/7353] Elapsed 3m 36s (remain 3m 22s) Loss: 1.4202(0.7688) \n",
      "EVAL: [3900/7353] Elapsed 3m 42s (remain 3m 16s) Loss: 0.3841(0.7689) \n",
      "EVAL: [4000/7353] Elapsed 3m 48s (remain 3m 11s) Loss: 0.3344(0.7757) \n",
      "EVAL: [4100/7353] Elapsed 3m 53s (remain 3m 5s) Loss: 0.3961(0.7743) \n",
      "EVAL: [4200/7353] Elapsed 3m 59s (remain 2m 59s) Loss: 1.6487(0.7763) \n",
      "EVAL: [4300/7353] Elapsed 4m 5s (remain 2m 54s) Loss: 1.3323(0.7770) \n",
      "EVAL: [4400/7353] Elapsed 4m 10s (remain 2m 48s) Loss: 0.8514(0.7749) \n",
      "EVAL: [4500/7353] Elapsed 4m 16s (remain 2m 42s) Loss: 0.4073(0.7754) \n",
      "EVAL: [4600/7353] Elapsed 4m 22s (remain 2m 36s) Loss: 1.2874(0.7766) \n",
      "EVAL: [4700/7353] Elapsed 4m 27s (remain 2m 31s) Loss: 0.8720(0.7762) \n",
      "EVAL: [4800/7353] Elapsed 4m 33s (remain 2m 25s) Loss: 1.2241(0.7764) \n",
      "EVAL: [4900/7353] Elapsed 4m 39s (remain 2m 19s) Loss: 0.3723(0.7773) \n",
      "EVAL: [5000/7353] Elapsed 4m 44s (remain 2m 14s) Loss: 0.6177(0.7790) \n",
      "EVAL: [5100/7353] Elapsed 4m 50s (remain 2m 8s) Loss: 0.3765(0.7812) \n",
      "EVAL: [5200/7353] Elapsed 4m 56s (remain 2m 2s) Loss: 0.7612(0.7820) \n",
      "EVAL: [5300/7353] Elapsed 5m 2s (remain 1m 56s) Loss: 0.3954(0.7821) \n",
      "EVAL: [5400/7353] Elapsed 5m 7s (remain 1m 51s) Loss: 1.7484(0.7803) \n",
      "EVAL: [5500/7353] Elapsed 5m 13s (remain 1m 45s) Loss: 0.3703(0.7821) \n",
      "EVAL: [5600/7353] Elapsed 5m 19s (remain 1m 39s) Loss: 0.5768(0.7822) \n",
      "EVAL: [5700/7353] Elapsed 5m 24s (remain 1m 34s) Loss: 0.5849(0.7841) \n",
      "EVAL: [5800/7353] Elapsed 5m 30s (remain 1m 28s) Loss: 0.3679(0.7824) \n",
      "EVAL: [5900/7353] Elapsed 5m 36s (remain 1m 22s) Loss: 1.2209(0.7821) \n",
      "EVAL: [6000/7353] Elapsed 5m 41s (remain 1m 17s) Loss: 0.5380(0.7806) \n",
      "EVAL: [6100/7353] Elapsed 5m 47s (remain 1m 11s) Loss: 1.8816(0.7812) \n",
      "EVAL: [6200/7353] Elapsed 5m 53s (remain 1m 5s) Loss: 1.5748(0.7812) \n",
      "EVAL: [6300/7353] Elapsed 5m 58s (remain 0m 59s) Loss: 2.2460(0.7815) \n",
      "EVAL: [6400/7353] Elapsed 6m 4s (remain 0m 54s) Loss: 0.6871(0.7814) \n",
      "EVAL: [6500/7353] Elapsed 6m 10s (remain 0m 48s) Loss: 0.5095(0.7824) \n",
      "EVAL: [6600/7353] Elapsed 6m 16s (remain 0m 42s) Loss: 0.5364(0.7820) \n",
      "EVAL: [6700/7353] Elapsed 6m 21s (remain 0m 37s) Loss: 0.4644(0.7822) \n",
      "EVAL: [6800/7353] Elapsed 6m 27s (remain 0m 31s) Loss: 0.7311(0.7835) \n",
      "EVAL: [6900/7353] Elapsed 6m 33s (remain 0m 25s) Loss: 0.6979(0.7822) \n",
      "EVAL: [7000/7353] Elapsed 6m 39s (remain 0m 20s) Loss: 0.3562(0.7818) \n",
      "EVAL: [7100/7353] Elapsed 6m 45s (remain 0m 14s) Loss: 0.4646(0.7810) \n",
      "EVAL: [7200/7353] Elapsed 6m 50s (remain 0m 8s) Loss: 0.6368(0.7799) \n",
      "EVAL: [7300/7353] Elapsed 6m 56s (remain 0m 2s) Loss: 0.3651(0.7792) \n",
      "EVAL: [7352/7353] Elapsed 6m 59s (remain 0m 0s) Loss: 2.5091(0.7789) \n",
      "Epoch 3 - Save Best Score: 0.7789 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7353] Elapsed 0m 0s (remain 42m 7s) Loss: 0.9698(0.9698) Grad: inf  LR: 0.00000050  \n",
      "Epoch: [1][100/7353] Elapsed 0m 41s (remain 49m 3s) Loss: 1.5286(0.9957) Grad: 997102.6875  LR: 0.00000050  \n",
      "Epoch: [1][200/7353] Elapsed 1m 21s (remain 48m 26s) Loss: 1.7183(1.0101) Grad: 980012.8125  LR: 0.00000050  \n",
      "Epoch: [1][300/7353] Elapsed 2m 2s (remain 47m 47s) Loss: 0.6753(1.0160) Grad: 649928.6875  LR: 0.00000050  \n",
      "Epoch: [1][400/7353] Elapsed 2m 43s (remain 47m 7s) Loss: 0.9193(1.0114) Grad: 1004403.9375  LR: 0.00000050  \n",
      "Epoch: [1][500/7353] Elapsed 3m 23s (remain 46m 28s) Loss: 0.8080(1.0113) Grad: 475682.0938  LR: 0.00000050  \n",
      "Epoch: [1][600/7353] Elapsed 4m 4s (remain 45m 49s) Loss: 1.5771(1.0100) Grad: 883028.9375  LR: 0.00000050  \n",
      "Epoch: [1][700/7353] Elapsed 4m 45s (remain 45m 8s) Loss: 0.5206(1.0027) Grad: 652313.6250  LR: 0.00000050  \n",
      "Epoch: [1][800/7353] Elapsed 5m 26s (remain 44m 28s) Loss: 0.5433(0.9950) Grad: 377651.5000  LR: 0.00000050  \n",
      "Epoch: [1][900/7353] Elapsed 6m 7s (remain 43m 48s) Loss: 1.1680(0.9874) Grad: 549327.1875  LR: 0.00000050  \n",
      "Epoch: [1][1000/7353] Elapsed 6m 47s (remain 43m 7s) Loss: 0.4974(0.9822) Grad: 406793.3750  LR: 0.00000050  \n",
      "Epoch: [1][1100/7353] Elapsed 7m 28s (remain 42m 26s) Loss: 1.0497(0.9754) Grad: 941609.4375  LR: 0.00000050  \n",
      "Epoch: [1][1200/7353] Elapsed 8m 9s (remain 41m 45s) Loss: 0.3560(0.9674) Grad: 361125.0312  LR: 0.00000050  \n",
      "Epoch: [1][1300/7353] Elapsed 8m 49s (remain 41m 4s) Loss: 1.5830(0.9622) Grad: 1261978.3750  LR: 0.00000050  \n",
      "Epoch: [1][1400/7353] Elapsed 9m 30s (remain 40m 24s) Loss: 0.9459(0.9585) Grad: 510498.2812  LR: 0.00000050  \n",
      "Epoch: [1][1500/7353] Elapsed 10m 11s (remain 39m 43s) Loss: 0.7245(0.9543) Grad: 1043754.1250  LR: 0.00000049  \n",
      "Epoch: [1][1600/7353] Elapsed 10m 52s (remain 39m 2s) Loss: 1.0115(0.9514) Grad: 661873.0625  LR: 0.00000049  \n",
      "Epoch: [1][1700/7353] Elapsed 11m 32s (remain 38m 22s) Loss: 0.7318(0.9466) Grad: 788443.5000  LR: 0.00000049  \n",
      "Epoch: [1][1800/7353] Elapsed 12m 13s (remain 37m 41s) Loss: 0.9324(0.9432) Grad: 768482.1250  LR: 0.00000049  \n",
      "Epoch: [1][1900/7353] Elapsed 12m 54s (remain 37m 0s) Loss: 0.9977(0.9435) Grad: 750060.1875  LR: 0.00000049  \n",
      "Epoch: [1][2000/7353] Elapsed 13m 35s (remain 36m 20s) Loss: 0.7039(0.9413) Grad: 656432.5625  LR: 0.00000049  \n",
      "Epoch: [1][2100/7353] Elapsed 14m 15s (remain 35m 39s) Loss: 1.2027(0.9402) Grad: 961086.2500  LR: 0.00000049  \n",
      "Epoch: [1][2200/7353] Elapsed 14m 56s (remain 34m 58s) Loss: 0.9252(0.9367) Grad: 521924.7500  LR: 0.00000049  \n",
      "Epoch: [1][2300/7353] Elapsed 15m 37s (remain 34m 17s) Loss: 0.8537(0.9335) Grad: 597122.1875  LR: 0.00000049  \n",
      "Epoch: [1][2400/7353] Elapsed 16m 17s (remain 33m 37s) Loss: 0.4022(0.9320) Grad: 387983.8438  LR: 0.00000049  \n",
      "Epoch: [1][2500/7353] Elapsed 16m 58s (remain 32m 56s) Loss: 1.1226(0.9332) Grad: 505818.9375  LR: 0.00000048  \n",
      "Epoch: [1][2600/7353] Elapsed 17m 39s (remain 32m 15s) Loss: 0.6148(0.9317) Grad: 486638.6562  LR: 0.00000048  \n",
      "Epoch: [1][2700/7353] Elapsed 18m 20s (remain 31m 34s) Loss: 1.0437(0.9296) Grad: 490470.0625  LR: 0.00000048  \n",
      "Epoch: [1][2800/7353] Elapsed 19m 0s (remain 30m 54s) Loss: 0.3366(0.9266) Grad: 422647.1875  LR: 0.00000048  \n",
      "Epoch: [1][2900/7353] Elapsed 19m 41s (remain 30m 13s) Loss: 0.7644(0.9239) Grad: 567512.1875  LR: 0.00000048  \n",
      "Epoch: [1][3000/7353] Elapsed 20m 22s (remain 29m 32s) Loss: 1.0451(0.9209) Grad: 732529.6250  LR: 0.00000048  \n",
      "Epoch: [1][3100/7353] Elapsed 21m 3s (remain 28m 51s) Loss: 1.0271(0.9213) Grad: 735748.1875  LR: 0.00000048  \n",
      "Epoch: [1][3200/7353] Elapsed 21m 43s (remain 28m 11s) Loss: 0.9717(0.9213) Grad: 929607.1250  LR: 0.00000047  \n",
      "Epoch: [1][3300/7353] Elapsed 22m 24s (remain 27m 30s) Loss: 0.9386(0.9187) Grad: 629186.7500  LR: 0.00000047  \n",
      "Epoch: [1][3400/7353] Elapsed 23m 5s (remain 26m 49s) Loss: 1.3088(0.9174) Grad: 616694.3125  LR: 0.00000047  \n",
      "Epoch: [1][3500/7353] Elapsed 23m 45s (remain 26m 8s) Loss: 0.5524(0.9164) Grad: 763700.7500  LR: 0.00000047  \n",
      "Epoch: [1][3600/7353] Elapsed 24m 26s (remain 25m 28s) Loss: 1.0153(0.9169) Grad: 693356.1250  LR: 0.00000047  \n",
      "Epoch: [1][3700/7353] Elapsed 25m 7s (remain 24m 47s) Loss: 0.8821(0.9161) Grad: 833333.5000  LR: 0.00000047  \n",
      "Epoch: [1][3800/7353] Elapsed 25m 48s (remain 24m 6s) Loss: 0.8392(0.9158) Grad: 498562.0312  LR: 0.00000046  \n",
      "Epoch: [1][3900/7353] Elapsed 26m 28s (remain 23m 26s) Loss: 0.6038(0.9144) Grad: 809501.6875  LR: 0.00000046  \n",
      "Epoch: [1][4000/7353] Elapsed 27m 9s (remain 22m 45s) Loss: 0.7522(0.9144) Grad: 627940.5000  LR: 0.00000046  \n",
      "Epoch: [1][4100/7353] Elapsed 27m 50s (remain 22m 4s) Loss: 1.0014(0.9118) Grad: 919156.5625  LR: 0.00000046  \n",
      "Epoch: [1][4200/7353] Elapsed 28m 31s (remain 21m 24s) Loss: 0.9569(0.9120) Grad: 892753.8125  LR: 0.00000046  \n",
      "Epoch: [1][4300/7353] Elapsed 29m 12s (remain 20m 43s) Loss: 1.4668(0.9100) Grad: 790941.3750  LR: 0.00000045  \n",
      "Epoch: [1][4400/7353] Elapsed 29m 52s (remain 20m 2s) Loss: 0.5218(0.9087) Grad: 655547.2500  LR: 0.00000045  \n",
      "Epoch: [1][4500/7353] Elapsed 30m 33s (remain 19m 21s) Loss: 0.4753(0.9074) Grad: 556071.3750  LR: 0.00000045  \n",
      "Epoch: [1][4600/7353] Elapsed 31m 14s (remain 18m 41s) Loss: 0.5410(0.9068) Grad: 535478.1250  LR: 0.00000045  \n",
      "Epoch: [1][4700/7353] Elapsed 31m 55s (remain 18m 0s) Loss: 1.6101(0.9073) Grad: 520345.8750  LR: 0.00000045  \n",
      "Epoch: [1][4800/7353] Elapsed 32m 35s (remain 17m 19s) Loss: 1.0745(0.9054) Grad: 926016.6875  LR: 0.00000044  \n",
      "Epoch: [1][4900/7353] Elapsed 33m 16s (remain 16m 38s) Loss: 0.3466(0.9048) Grad: 403127.8750  LR: 0.00000044  \n",
      "Epoch: [1][5000/7353] Elapsed 33m 57s (remain 15m 58s) Loss: 1.3922(0.9025) Grad: 736014.3750  LR: 0.00000044  \n",
      "Epoch: [1][5100/7353] Elapsed 34m 38s (remain 15m 17s) Loss: 0.6138(0.9024) Grad: 436849.6562  LR: 0.00000044  \n",
      "Epoch: [1][5200/7353] Elapsed 35m 18s (remain 14m 36s) Loss: 0.9799(0.9019) Grad: 393545.1250  LR: 0.00000043  \n",
      "Epoch: [1][5300/7353] Elapsed 35m 59s (remain 13m 56s) Loss: 0.5791(0.9002) Grad: 614243.4375  LR: 0.00000043  \n",
      "Epoch: [1][5400/7353] Elapsed 36m 40s (remain 13m 15s) Loss: 0.6370(0.8998) Grad: 834087.3750  LR: 0.00000043  \n",
      "Epoch: [1][5500/7353] Elapsed 37m 21s (remain 12m 34s) Loss: 1.5004(0.8992) Grad: 1271143.1250  LR: 0.00000043  \n",
      "Epoch: [1][5600/7353] Elapsed 38m 2s (remain 11m 53s) Loss: 0.6255(0.8982) Grad: 424402.3438  LR: 0.00000042  \n",
      "Epoch: [1][5700/7353] Elapsed 38m 42s (remain 11m 13s) Loss: 1.0746(0.8975) Grad: 940886.5000  LR: 0.00000042  \n",
      "Epoch: [1][5800/7353] Elapsed 39m 23s (remain 10m 32s) Loss: 0.5400(0.8973) Grad: 855432.8125  LR: 0.00000042  \n",
      "Epoch: [1][5900/7353] Elapsed 40m 4s (remain 9m 51s) Loss: 0.5216(0.8974) Grad: 441185.1875  LR: 0.00000042  \n",
      "Epoch: [1][6000/7353] Elapsed 40m 45s (remain 9m 10s) Loss: 0.5149(0.8973) Grad: 473190.4375  LR: 0.00000041  \n",
      "Epoch: [1][6100/7353] Elapsed 41m 26s (remain 8m 30s) Loss: 1.0750(0.8967) Grad: 1210324.6250  LR: 0.00000041  \n",
      "Epoch: [1][6200/7353] Elapsed 42m 6s (remain 7m 49s) Loss: 0.8983(0.8954) Grad: 749264.3125  LR: 0.00000041  \n",
      "Epoch: [1][6300/7353] Elapsed 42m 47s (remain 7m 8s) Loss: 0.9332(0.8941) Grad: 672398.0000  LR: 0.00000041  \n",
      "Epoch: [1][6400/7353] Elapsed 43m 28s (remain 6m 27s) Loss: 0.5775(0.8937) Grad: 778102.2500  LR: 0.00000040  \n",
      "Epoch: [1][6500/7353] Elapsed 44m 8s (remain 5m 47s) Loss: 0.7913(0.8928) Grad: 569396.0000  LR: 0.00000040  \n",
      "Epoch: [1][6600/7353] Elapsed 44m 49s (remain 5m 6s) Loss: 0.3929(0.8924) Grad: 463577.4375  LR: 0.00000040  \n",
      "Epoch: [1][6700/7353] Elapsed 45m 30s (remain 4m 25s) Loss: 0.6879(0.8917) Grad: 543623.8750  LR: 0.00000039  \n",
      "Epoch: [1][6800/7353] Elapsed 46m 11s (remain 3m 44s) Loss: 1.4156(0.8914) Grad: 1349669.6250  LR: 0.00000039  \n",
      "Epoch: [1][6900/7353] Elapsed 46m 51s (remain 3m 4s) Loss: 0.5371(0.8897) Grad: 552301.3125  LR: 0.00000039  \n",
      "Epoch: [1][7000/7353] Elapsed 47m 32s (remain 2m 23s) Loss: 0.2505(0.8875) Grad: 259118.2031  LR: 0.00000039  \n",
      "Epoch: [1][7100/7353] Elapsed 48m 13s (remain 1m 42s) Loss: 0.5899(0.8869) Grad: 663332.0000  LR: 0.00000038  \n",
      "Epoch: [1][7200/7353] Elapsed 48m 54s (remain 1m 1s) Loss: 0.2560(0.8855) Grad: 297749.3438  LR: 0.00000038  \n",
      "Epoch: [1][7300/7353] Elapsed 49m 34s (remain 0m 21s) Loss: 1.2090(0.8845) Grad: 665783.9375  LR: 0.00000038  \n",
      "Epoch: [1][7352/7353] Elapsed 49m 56s (remain 0m 0s) Loss: 0.4993(0.8842) Grad: 638153.6250  LR: 0.00000038  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 48s) Loss: 0.2206(0.2206) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 54s) Loss: 1.7665(0.8233) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 48s) Loss: 2.8800(0.8645) \n",
      "EVAL: [300/7353] Elapsed 0m 17s (remain 6m 41s) Loss: 1.0809(0.8503) \n",
      "EVAL: [400/7353] Elapsed 0m 22s (remain 6m 35s) Loss: 3.2335(0.8880) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 30s) Loss: 0.1314(0.9297) \n",
      "EVAL: [600/7353] Elapsed 0m 34s (remain 6m 25s) Loss: 0.3491(0.9332) \n",
      "EVAL: [700/7353] Elapsed 0m 39s (remain 6m 19s) Loss: 0.3751(0.9069) \n",
      "EVAL: [800/7353] Elapsed 0m 45s (remain 6m 13s) Loss: 0.1832(0.9082) \n",
      "EVAL: [900/7353] Elapsed 0m 51s (remain 6m 8s) Loss: 0.5021(0.8886) \n",
      "EVAL: [1000/7353] Elapsed 0m 57s (remain 6m 3s) Loss: 0.3157(0.8755) \n",
      "EVAL: [1100/7353] Elapsed 1m 3s (remain 5m 57s) Loss: 0.2180(0.8777) \n",
      "EVAL: [1200/7353] Elapsed 1m 8s (remain 5m 52s) Loss: 0.2720(0.8766) \n",
      "EVAL: [1300/7353] Elapsed 1m 14s (remain 5m 46s) Loss: 0.2141(0.8856) \n",
      "EVAL: [1400/7353] Elapsed 1m 20s (remain 5m 40s) Loss: 0.2802(0.8752) \n",
      "EVAL: [1500/7353] Elapsed 1m 25s (remain 5m 35s) Loss: 0.1987(0.8987) \n",
      "EVAL: [1600/7353] Elapsed 1m 31s (remain 5m 29s) Loss: 1.8530(0.8892) \n",
      "EVAL: [1700/7353] Elapsed 1m 37s (remain 5m 23s) Loss: 0.4952(0.8937) \n",
      "EVAL: [1800/7353] Elapsed 1m 43s (remain 5m 18s) Loss: 0.4823(0.8920) \n",
      "EVAL: [1900/7353] Elapsed 1m 48s (remain 5m 12s) Loss: 0.2801(0.8921) \n",
      "EVAL: [2000/7353] Elapsed 1m 54s (remain 5m 6s) Loss: 0.2720(0.8851) \n",
      "EVAL: [2100/7353] Elapsed 2m 0s (remain 5m 1s) Loss: 1.2261(0.8898) \n",
      "EVAL: [2200/7353] Elapsed 2m 6s (remain 4m 55s) Loss: 1.8724(0.8848) \n",
      "EVAL: [2300/7353] Elapsed 2m 11s (remain 4m 49s) Loss: 0.4393(0.8854) \n",
      "EVAL: [2400/7353] Elapsed 2m 17s (remain 4m 43s) Loss: 1.0001(0.8845) \n",
      "EVAL: [2500/7353] Elapsed 2m 23s (remain 4m 37s) Loss: 0.0396(0.8775) \n",
      "EVAL: [2600/7353] Elapsed 2m 29s (remain 4m 32s) Loss: 0.2590(0.8811) \n",
      "EVAL: [2700/7353] Elapsed 2m 34s (remain 4m 26s) Loss: 0.2647(0.8748) \n",
      "EVAL: [2800/7353] Elapsed 2m 40s (remain 4m 20s) Loss: 1.4671(0.8752) \n",
      "EVAL: [2900/7353] Elapsed 2m 46s (remain 4m 15s) Loss: 0.3911(0.8723) \n",
      "EVAL: [3000/7353] Elapsed 2m 51s (remain 4m 9s) Loss: 0.1732(0.8768) \n",
      "EVAL: [3100/7353] Elapsed 2m 57s (remain 4m 3s) Loss: 2.1158(0.8781) \n",
      "EVAL: [3200/7353] Elapsed 3m 3s (remain 3m 57s) Loss: 0.1303(0.8767) \n",
      "EVAL: [3300/7353] Elapsed 3m 9s (remain 3m 52s) Loss: 0.3496(0.8740) \n",
      "EVAL: [3400/7353] Elapsed 3m 14s (remain 3m 46s) Loss: 0.2325(0.8762) \n",
      "EVAL: [3500/7353] Elapsed 3m 20s (remain 3m 40s) Loss: 0.7785(0.8736) \n",
      "EVAL: [3600/7353] Elapsed 3m 26s (remain 3m 34s) Loss: 0.1039(0.8718) \n",
      "EVAL: [3700/7353] Elapsed 3m 31s (remain 3m 29s) Loss: 0.2890(0.8749) \n",
      "EVAL: [3800/7353] Elapsed 3m 37s (remain 3m 23s) Loss: 1.9369(0.8748) \n",
      "EVAL: [3900/7353] Elapsed 3m 43s (remain 3m 17s) Loss: 0.4045(0.8745) \n",
      "EVAL: [4000/7353] Elapsed 3m 48s (remain 3m 11s) Loss: 2.1422(0.8750) \n",
      "EVAL: [4100/7353] Elapsed 3m 54s (remain 3m 6s) Loss: 0.3230(0.8731) \n",
      "EVAL: [4200/7353] Elapsed 4m 0s (remain 3m 0s) Loss: 1.8216(0.8729) \n",
      "EVAL: [4300/7353] Elapsed 4m 6s (remain 2m 54s) Loss: 0.1836(0.8718) \n",
      "EVAL: [4400/7353] Elapsed 4m 11s (remain 2m 48s) Loss: 1.8110(0.8712) \n",
      "EVAL: [4500/7353] Elapsed 4m 17s (remain 2m 43s) Loss: 1.3067(0.8726) \n",
      "EVAL: [4600/7353] Elapsed 4m 23s (remain 2m 37s) Loss: 1.2112(0.8780) \n",
      "EVAL: [4700/7353] Elapsed 4m 28s (remain 2m 31s) Loss: 0.2115(0.8841) \n",
      "EVAL: [4800/7353] Elapsed 4m 34s (remain 2m 25s) Loss: 0.4911(0.8853) \n",
      "EVAL: [4900/7353] Elapsed 4m 40s (remain 2m 20s) Loss: 2.7111(0.8849) \n",
      "EVAL: [5000/7353] Elapsed 4m 46s (remain 2m 14s) Loss: 0.2832(0.8840) \n",
      "EVAL: [5100/7353] Elapsed 4m 51s (remain 2m 8s) Loss: 0.1624(0.8851) \n",
      "EVAL: [5200/7353] Elapsed 4m 57s (remain 2m 3s) Loss: 0.9936(0.8874) \n",
      "EVAL: [5300/7353] Elapsed 5m 3s (remain 1m 57s) Loss: 0.4084(0.8845) \n",
      "EVAL: [5400/7353] Elapsed 5m 8s (remain 1m 51s) Loss: 0.6330(0.8862) \n",
      "EVAL: [5500/7353] Elapsed 5m 14s (remain 1m 45s) Loss: 0.1706(0.8863) \n",
      "EVAL: [5600/7353] Elapsed 5m 20s (remain 1m 40s) Loss: 0.9713(0.8873) \n",
      "EVAL: [5700/7353] Elapsed 5m 25s (remain 1m 34s) Loss: 0.2040(0.8880) \n",
      "EVAL: [5800/7353] Elapsed 5m 31s (remain 1m 28s) Loss: 0.6390(0.8891) \n",
      "EVAL: [5900/7353] Elapsed 5m 36s (remain 1m 22s) Loss: 0.1619(0.8859) \n",
      "EVAL: [6000/7353] Elapsed 5m 42s (remain 1m 17s) Loss: 0.4594(0.8870) \n",
      "EVAL: [6100/7353] Elapsed 5m 48s (remain 1m 11s) Loss: 0.2419(0.8889) \n",
      "EVAL: [6200/7353] Elapsed 5m 53s (remain 1m 5s) Loss: 2.0124(0.8877) \n",
      "EVAL: [6300/7353] Elapsed 5m 59s (remain 1m 0s) Loss: 0.3201(0.8887) \n",
      "EVAL: [6400/7353] Elapsed 6m 5s (remain 0m 54s) Loss: 0.5621(0.8858) \n",
      "EVAL: [6500/7353] Elapsed 6m 10s (remain 0m 48s) Loss: 0.0384(0.8865) \n",
      "EVAL: [6600/7353] Elapsed 6m 16s (remain 0m 42s) Loss: 0.1972(0.8879) \n",
      "EVAL: [6700/7353] Elapsed 6m 22s (remain 0m 37s) Loss: 1.6947(0.8867) \n",
      "EVAL: [6800/7353] Elapsed 6m 27s (remain 0m 31s) Loss: 1.2030(0.8872) \n",
      "EVAL: [6900/7353] Elapsed 6m 33s (remain 0m 25s) Loss: 0.2544(0.8884) \n",
      "EVAL: [7000/7353] Elapsed 6m 39s (remain 0m 20s) Loss: 2.7681(0.8874) \n",
      "EVAL: [7100/7353] Elapsed 6m 44s (remain 0m 14s) Loss: 0.0703(0.8870) \n",
      "EVAL: [7200/7353] Elapsed 6m 50s (remain 0m 8s) Loss: 2.3868(0.8855) \n",
      "EVAL: [7300/7353] Elapsed 6m 56s (remain 0m 2s) Loss: 0.9357(0.8860) \n",
      "EVAL: [7352/7353] Elapsed 6m 59s (remain 0m 0s) Loss: 0.1374(0.8859) \n",
      "Epoch 1 - Save Best Score: 0.8859 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/7353] Elapsed 0m 0s (remain 100m 52s) Loss: 1.6050(1.6050) Grad: inf  LR: 0.00000037  \n",
      "Epoch: [2][100/7353] Elapsed 1m 23s (remain 99m 51s) Loss: 0.4471(1.0665) Grad: 367252.8125  LR: 0.00000037  \n",
      "Epoch: [2][200/7353] Elapsed 2m 46s (remain 98m 32s) Loss: 0.5869(0.9970) Grad: 219467.4844  LR: 0.00000037  \n",
      "Epoch: [2][300/7353] Elapsed 4m 8s (remain 97m 8s) Loss: 0.8706(0.9633) Grad: 426527.4688  LR: 0.00000037  \n",
      "Epoch: [2][400/7353] Elapsed 5m 31s (remain 95m 47s) Loss: 1.1405(0.9384) Grad: 556436.5625  LR: 0.00000036  \n",
      "Epoch: [2][500/7353] Elapsed 6m 54s (remain 94m 25s) Loss: 0.6469(0.9185) Grad: 190175.0938  LR: 0.00000036  \n",
      "Epoch: [2][600/7353] Elapsed 8m 16s (remain 93m 0s) Loss: 0.5782(0.9025) Grad: 190287.7031  LR: 0.00000036  \n",
      "Epoch: [2][700/7353] Elapsed 9m 39s (remain 91m 38s) Loss: 1.5349(0.8903) Grad: 558875.6875  LR: 0.00000035  \n",
      "Epoch: [2][800/7353] Elapsed 11m 2s (remain 90m 16s) Loss: 0.8328(0.8927) Grad: 350791.3125  LR: 0.00000035  \n",
      "Epoch: [2][900/7353] Elapsed 12m 24s (remain 88m 53s) Loss: 0.7175(0.8851) Grad: 235888.3906  LR: 0.00000035  \n",
      "Epoch: [2][1000/7353] Elapsed 13m 47s (remain 87m 30s) Loss: 0.6947(0.8759) Grad: 266901.6250  LR: 0.00000034  \n",
      "Epoch: [2][1100/7353] Elapsed 15m 10s (remain 86m 7s) Loss: 0.5373(0.8685) Grad: 288230.6562  LR: 0.00000034  \n",
      "Epoch: [2][1200/7353] Elapsed 16m 32s (remain 84m 45s) Loss: 0.8810(0.8617) Grad: 391824.0000  LR: 0.00000034  \n",
      "Epoch: [2][1300/7353] Elapsed 17m 55s (remain 83m 22s) Loss: 0.7297(0.8624) Grad: 262478.3438  LR: 0.00000033  \n",
      "Epoch: [2][1400/7353] Elapsed 19m 18s (remain 81m 59s) Loss: 0.4349(0.8602) Grad: 318365.9375  LR: 0.00000033  \n",
      "Epoch: [2][1500/7353] Elapsed 20m 40s (remain 80m 37s) Loss: 0.6008(0.8625) Grad: 294868.6875  LR: 0.00000033  \n",
      "Epoch: [2][1600/7353] Elapsed 22m 3s (remain 79m 14s) Loss: 0.7295(0.8604) Grad: 205303.3594  LR: 0.00000032  \n",
      "Epoch: [2][1700/7353] Elapsed 23m 26s (remain 77m 52s) Loss: 0.7881(0.8614) Grad: 351831.3438  LR: 0.00000032  \n",
      "Epoch: [2][1800/7353] Elapsed 24m 48s (remain 76m 29s) Loss: 1.0685(0.8600) Grad: 349083.4688  LR: 0.00000032  \n",
      "Epoch: [2][1900/7353] Elapsed 26m 11s (remain 75m 7s) Loss: 0.8421(0.8603) Grad: 356483.3125  LR: 0.00000031  \n",
      "Epoch: [2][2000/7353] Elapsed 27m 34s (remain 73m 44s) Loss: 0.9395(0.8593) Grad: 388757.7188  LR: 0.00000031  \n",
      "Epoch: [2][2100/7353] Elapsed 28m 56s (remain 72m 21s) Loss: 0.9347(0.8572) Grad: 437356.5312  LR: 0.00000031  \n",
      "Epoch: [2][2200/7353] Elapsed 30m 19s (remain 70m 58s) Loss: 0.9075(0.8560) Grad: 293266.1875  LR: 0.00000030  \n",
      "Epoch: [2][2300/7353] Elapsed 31m 41s (remain 69m 35s) Loss: 0.6653(0.8547) Grad: 165157.0156  LR: 0.00000030  \n",
      "Epoch: [2][2400/7353] Elapsed 33m 4s (remain 68m 13s) Loss: 0.8440(0.8528) Grad: 223694.0781  LR: 0.00000030  \n",
      "Epoch: [2][2500/7353] Elapsed 34m 27s (remain 66m 51s) Loss: 0.8016(0.8519) Grad: 262470.8438  LR: 0.00000029  \n",
      "Epoch: [2][2600/7353] Elapsed 35m 50s (remain 65m 28s) Loss: 1.3236(0.8523) Grad: 464768.7812  LR: 0.00000029  \n",
      "Epoch: [2][2700/7353] Elapsed 37m 13s (remain 64m 6s) Loss: 0.4781(0.8514) Grad: 335293.9375  LR: 0.00000028  \n",
      "Epoch: [2][2800/7353] Elapsed 38m 35s (remain 62m 43s) Loss: 0.8092(0.8503) Grad: 388653.8750  LR: 0.00000028  \n",
      "Epoch: [2][2900/7353] Elapsed 39m 58s (remain 61m 20s) Loss: 0.9294(0.8508) Grad: 311516.2500  LR: 0.00000028  \n",
      "Epoch: [2][3000/7353] Elapsed 41m 21s (remain 59m 58s) Loss: 0.4677(0.8492) Grad: 324785.0000  LR: 0.00000027  \n",
      "Epoch: [2][3100/7353] Elapsed 42m 43s (remain 58m 35s) Loss: 0.5267(0.8475) Grad: 390594.4375  LR: 0.00000027  \n",
      "Epoch: [2][3200/7353] Elapsed 44m 6s (remain 57m 12s) Loss: 0.9152(0.8460) Grad: 263699.1562  LR: 0.00000027  \n",
      "Epoch: [2][3300/7353] Elapsed 45m 29s (remain 55m 49s) Loss: 0.7188(0.8454) Grad: 326283.9062  LR: 0.00000026  \n",
      "Epoch: [2][3400/7353] Elapsed 46m 51s (remain 54m 27s) Loss: 0.7954(0.8442) Grad: 223107.1094  LR: 0.00000026  \n",
      "Epoch: [2][3500/7353] Elapsed 48m 14s (remain 53m 4s) Loss: 0.7839(0.8416) Grad: 353237.5938  LR: 0.00000026  \n",
      "Epoch: [2][3600/7353] Elapsed 49m 37s (remain 51m 41s) Loss: 0.6049(0.8402) Grad: 337594.7812  LR: 0.00000025  \n",
      "Epoch: [2][3700/7353] Elapsed 50m 59s (remain 50m 19s) Loss: 1.3008(0.8399) Grad: 352774.6875  LR: 0.00000025  \n",
      "Epoch: [2][3800/7353] Elapsed 52m 22s (remain 48m 56s) Loss: 1.0385(0.8399) Grad: 325382.6562  LR: 0.00000025  \n",
      "Epoch: [2][3900/7353] Elapsed 53m 45s (remain 47m 33s) Loss: 0.7814(0.8388) Grad: 244455.8750  LR: 0.00000024  \n",
      "Epoch: [2][4000/7353] Elapsed 55m 7s (remain 46m 11s) Loss: 0.7714(0.8383) Grad: 281888.5938  LR: 0.00000024  \n",
      "Epoch: [2][4100/7353] Elapsed 56m 30s (remain 44m 48s) Loss: 1.1157(0.8383) Grad: 452704.2812  LR: 0.00000023  \n",
      "Epoch: [2][4200/7353] Elapsed 57m 53s (remain 43m 25s) Loss: 0.6367(0.8376) Grad: 405102.9062  LR: 0.00000023  \n",
      "Epoch: [2][4300/7353] Elapsed 59m 15s (remain 42m 3s) Loss: 0.8827(0.8377) Grad: 271654.9062  LR: 0.00000023  \n",
      "Epoch: [2][4400/7353] Elapsed 60m 38s (remain 40m 40s) Loss: 0.6918(0.8370) Grad: 207094.7500  LR: 0.00000022  \n",
      "Epoch: [2][4500/7353] Elapsed 62m 1s (remain 39m 18s) Loss: 1.0793(0.8363) Grad: 459740.4062  LR: 0.00000022  \n",
      "Epoch: [2][4600/7353] Elapsed 63m 24s (remain 37m 55s) Loss: 0.4819(0.8356) Grad: 186203.7031  LR: 0.00000022  \n",
      "Epoch: [2][4700/7353] Elapsed 64m 46s (remain 36m 32s) Loss: 0.4465(0.8344) Grad: 332000.6875  LR: 0.00000021  \n",
      "Epoch: [2][4800/7353] Elapsed 66m 9s (remain 35m 9s) Loss: 0.8723(0.8341) Grad: 284063.8750  LR: 0.00000021  \n",
      "Epoch: [2][4900/7353] Elapsed 67m 32s (remain 33m 47s) Loss: 1.1677(0.8334) Grad: 328903.5938  LR: 0.00000021  \n",
      "Epoch: [2][5000/7353] Elapsed 68m 54s (remain 32m 24s) Loss: 0.6121(0.8328) Grad: 179191.6562  LR: 0.00000020  \n",
      "Epoch: [2][5100/7353] Elapsed 70m 17s (remain 31m 1s) Loss: 0.5757(0.8325) Grad: 163917.6562  LR: 0.00000020  \n",
      "Epoch: [2][5200/7353] Elapsed 71m 40s (remain 29m 39s) Loss: 1.0662(0.8320) Grad: 540033.5000  LR: 0.00000020  \n",
      "Epoch: [2][5300/7353] Elapsed 73m 3s (remain 28m 16s) Loss: 0.5936(0.8316) Grad: 221390.8125  LR: 0.00000019  \n",
      "Epoch: [2][5400/7353] Elapsed 74m 25s (remain 26m 53s) Loss: 0.4144(0.8312) Grad: 307490.8125  LR: 0.00000019  \n",
      "Epoch: [2][5500/7353] Elapsed 75m 48s (remain 25m 31s) Loss: 0.5783(0.8306) Grad: 278147.6875  LR: 0.00000019  \n",
      "Epoch: [2][5600/7353] Elapsed 77m 11s (remain 24m 8s) Loss: 0.6412(0.8302) Grad: 310172.5938  LR: 0.00000018  \n",
      "Epoch: [2][5700/7353] Elapsed 78m 33s (remain 22m 45s) Loss: 0.8150(0.8312) Grad: 279783.2812  LR: 0.00000018  \n",
      "Epoch: [2][5800/7353] Elapsed 79m 56s (remain 21m 23s) Loss: 0.6562(0.8307) Grad: 267872.9688  LR: 0.00000018  \n",
      "Epoch: [2][5900/7353] Elapsed 81m 19s (remain 20m 0s) Loss: 0.5552(0.8296) Grad: 226226.2344  LR: 0.00000017  \n",
      "Epoch: [2][6000/7353] Elapsed 82m 42s (remain 18m 37s) Loss: 0.7525(0.8290) Grad: 214507.2656  LR: 0.00000017  \n",
      "Epoch: [2][6100/7353] Elapsed 84m 4s (remain 17m 15s) Loss: 0.5734(0.8290) Grad: 204882.0156  LR: 0.00000017  \n",
      "Epoch: [2][6200/7353] Elapsed 85m 27s (remain 15m 52s) Loss: 0.9852(0.8289) Grad: 239090.3750  LR: 0.00000016  \n",
      "Epoch: [2][6300/7353] Elapsed 86m 49s (remain 14m 29s) Loss: 0.4433(0.8288) Grad: 322321.0312  LR: 0.00000016  \n",
      "Epoch: [2][6400/7353] Elapsed 88m 12s (remain 13m 7s) Loss: 1.0862(0.8271) Grad: 397555.6875  LR: 0.00000016  \n",
      "Epoch: [2][6500/7353] Elapsed 89m 35s (remain 11m 44s) Loss: 0.5483(0.8279) Grad: 398151.6875  LR: 0.00000015  \n",
      "Epoch: [2][6600/7353] Elapsed 90m 58s (remain 10m 21s) Loss: 0.9264(0.8281) Grad: 208351.9531  LR: 0.00000015  \n",
      "Epoch: [2][6700/7353] Elapsed 92m 20s (remain 8m 59s) Loss: 0.7008(0.8275) Grad: 175911.1562  LR: 0.00000015  \n",
      "Epoch: [2][6800/7353] Elapsed 93m 43s (remain 7m 36s) Loss: 0.6030(0.8266) Grad: 246063.1406  LR: 0.00000014  \n",
      "Epoch: [2][6900/7353] Elapsed 95m 6s (remain 6m 13s) Loss: 1.0453(0.8261) Grad: 335894.2812  LR: 0.00000014  \n",
      "Epoch: [2][7000/7353] Elapsed 96m 29s (remain 4m 51s) Loss: 0.9575(0.8249) Grad: 461246.4375  LR: 0.00000014  \n",
      "Epoch: [2][7100/7353] Elapsed 97m 51s (remain 3m 28s) Loss: 0.6190(0.8243) Grad: 169504.8750  LR: 0.00000013  \n",
      "Epoch: [2][7200/7353] Elapsed 99m 14s (remain 2m 5s) Loss: 0.6412(0.8231) Grad: 229596.2031  LR: 0.00000013  \n",
      "Epoch: [2][7300/7353] Elapsed 100m 37s (remain 0m 43s) Loss: 0.7013(0.8226) Grad: 300997.7500  LR: 0.00000013  \n",
      "Epoch: [2][7352/7353] Elapsed 101m 20s (remain 0m 0s) Loss: 0.8162(0.8223) Grad: 240903.0625  LR: 0.00000013  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 48s) Loss: 0.3516(0.3516) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 53s) Loss: 1.0546(0.7422) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 48s) Loss: 1.3731(0.7703) \n",
      "EVAL: [300/7353] Elapsed 0m 17s (remain 6m 42s) Loss: 0.8036(0.7566) \n",
      "EVAL: [400/7353] Elapsed 0m 22s (remain 6m 36s) Loss: 1.4529(0.7791) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 30s) Loss: 0.2938(0.8133) \n",
      "EVAL: [600/7353] Elapsed 0m 34s (remain 6m 24s) Loss: 0.3826(0.8115) \n",
      "EVAL: [700/7353] Elapsed 0m 39s (remain 6m 19s) Loss: 0.3985(0.8029) \n",
      "EVAL: [800/7353] Elapsed 0m 45s (remain 6m 13s) Loss: 0.3358(0.8019) \n",
      "EVAL: [900/7353] Elapsed 0m 51s (remain 6m 7s) Loss: 0.4244(0.7839) \n",
      "EVAL: [1000/7353] Elapsed 0m 56s (remain 6m 1s) Loss: 0.7508(0.7731) \n",
      "EVAL: [1100/7353] Elapsed 1m 2s (remain 5m 56s) Loss: 0.3492(0.7707) \n",
      "EVAL: [1200/7353] Elapsed 1m 8s (remain 5m 50s) Loss: 0.3812(0.7673) \n",
      "EVAL: [1300/7353] Elapsed 1m 14s (remain 5m 44s) Loss: 0.3587(0.7726) \n",
      "EVAL: [1400/7353] Elapsed 1m 19s (remain 5m 39s) Loss: 0.3963(0.7648) \n",
      "EVAL: [1500/7353] Elapsed 1m 25s (remain 5m 33s) Loss: 0.3308(0.7801) \n",
      "EVAL: [1600/7353] Elapsed 1m 31s (remain 5m 28s) Loss: 1.3651(0.7730) \n",
      "EVAL: [1700/7353] Elapsed 1m 37s (remain 5m 23s) Loss: 0.7766(0.7772) \n",
      "EVAL: [1800/7353] Elapsed 1m 42s (remain 5m 17s) Loss: 0.9894(0.7764) \n",
      "EVAL: [1900/7353] Elapsed 1m 48s (remain 5m 11s) Loss: 0.7216(0.7811) \n",
      "EVAL: [2000/7353] Elapsed 1m 54s (remain 5m 5s) Loss: 0.3619(0.7773) \n",
      "EVAL: [2100/7353] Elapsed 2m 0s (remain 5m 0s) Loss: 1.1707(0.7784) \n",
      "EVAL: [2200/7353] Elapsed 2m 5s (remain 4m 54s) Loss: 1.3730(0.7738) \n",
      "EVAL: [2300/7353] Elapsed 2m 11s (remain 4m 48s) Loss: 0.4172(0.7745) \n",
      "EVAL: [2400/7353] Elapsed 2m 17s (remain 4m 43s) Loss: 1.1233(0.7741) \n",
      "EVAL: [2500/7353] Elapsed 2m 22s (remain 4m 37s) Loss: 0.2845(0.7690) \n",
      "EVAL: [2600/7353] Elapsed 2m 28s (remain 4m 31s) Loss: 0.3665(0.7727) \n",
      "EVAL: [2700/7353] Elapsed 2m 34s (remain 4m 25s) Loss: 0.3434(0.7692) \n",
      "EVAL: [2800/7353] Elapsed 2m 40s (remain 4m 20s) Loss: 1.2227(0.7683) \n",
      "EVAL: [2900/7353] Elapsed 2m 45s (remain 4m 14s) Loss: 0.4319(0.7672) \n",
      "EVAL: [3000/7353] Elapsed 2m 51s (remain 4m 8s) Loss: 0.3100(0.7702) \n",
      "EVAL: [3100/7353] Elapsed 2m 57s (remain 4m 2s) Loss: 1.4716(0.7724) \n",
      "EVAL: [3200/7353] Elapsed 3m 2s (remain 3m 57s) Loss: 0.2972(0.7715) \n",
      "EVAL: [3300/7353] Elapsed 3m 8s (remain 3m 51s) Loss: 0.3999(0.7703) \n",
      "EVAL: [3400/7353] Elapsed 3m 14s (remain 3m 45s) Loss: 0.3395(0.7722) \n",
      "EVAL: [3500/7353] Elapsed 3m 19s (remain 3m 39s) Loss: 1.5316(0.7713) \n",
      "EVAL: [3600/7353] Elapsed 3m 25s (remain 3m 34s) Loss: 0.3788(0.7730) \n",
      "EVAL: [3700/7353] Elapsed 3m 31s (remain 3m 28s) Loss: 0.3976(0.7762) \n",
      "EVAL: [3800/7353] Elapsed 3m 37s (remain 3m 22s) Loss: 1.3856(0.7764) \n",
      "EVAL: [3900/7353] Elapsed 3m 42s (remain 3m 17s) Loss: 0.9500(0.7775) \n",
      "EVAL: [4000/7353] Elapsed 3m 48s (remain 3m 11s) Loss: 1.3931(0.7792) \n",
      "EVAL: [4100/7353] Elapsed 3m 54s (remain 3m 5s) Loss: 0.3978(0.7786) \n",
      "EVAL: [4200/7353] Elapsed 3m 59s (remain 2m 59s) Loss: 1.3941(0.7797) \n",
      "EVAL: [4300/7353] Elapsed 4m 5s (remain 2m 54s) Loss: 0.3265(0.7794) \n",
      "EVAL: [4400/7353] Elapsed 4m 11s (remain 2m 48s) Loss: 0.9253(0.7789) \n",
      "EVAL: [4500/7353] Elapsed 4m 16s (remain 2m 42s) Loss: 0.7959(0.7807) \n",
      "EVAL: [4600/7353] Elapsed 4m 22s (remain 2m 37s) Loss: 1.7433(0.7860) \n",
      "EVAL: [4700/7353] Elapsed 4m 28s (remain 2m 31s) Loss: 0.3562(0.7877) \n",
      "EVAL: [4800/7353] Elapsed 4m 33s (remain 2m 25s) Loss: 0.4405(0.7893) \n",
      "EVAL: [4900/7353] Elapsed 4m 39s (remain 2m 19s) Loss: 1.6452(0.7887) \n",
      "EVAL: [5000/7353] Elapsed 4m 45s (remain 2m 14s) Loss: 0.3738(0.7892) \n",
      "EVAL: [5100/7353] Elapsed 4m 51s (remain 2m 8s) Loss: 0.3178(0.7901) \n",
      "EVAL: [5200/7353] Elapsed 4m 56s (remain 2m 2s) Loss: 0.7134(0.7917) \n",
      "EVAL: [5300/7353] Elapsed 5m 2s (remain 1m 57s) Loss: 0.4804(0.7899) \n",
      "EVAL: [5400/7353] Elapsed 5m 8s (remain 1m 51s) Loss: 0.5758(0.7904) \n",
      "EVAL: [5500/7353] Elapsed 5m 13s (remain 1m 45s) Loss: 0.3189(0.7902) \n",
      "EVAL: [5600/7353] Elapsed 5m 19s (remain 1m 39s) Loss: 0.6601(0.7909) \n",
      "EVAL: [5700/7353] Elapsed 5m 25s (remain 1m 34s) Loss: 0.6183(0.7929) \n",
      "EVAL: [5800/7353] Elapsed 5m 30s (remain 1m 28s) Loss: 0.5280(0.7932) \n",
      "EVAL: [5900/7353] Elapsed 5m 36s (remain 1m 22s) Loss: 0.5313(0.7916) \n",
      "EVAL: [6000/7353] Elapsed 5m 42s (remain 1m 17s) Loss: 0.4814(0.7924) \n",
      "EVAL: [6100/7353] Elapsed 5m 47s (remain 1m 11s) Loss: 0.3776(0.7930) \n",
      "EVAL: [6200/7353] Elapsed 5m 53s (remain 1m 5s) Loss: 1.4499(0.7925) \n",
      "EVAL: [6300/7353] Elapsed 5m 59s (remain 0m 59s) Loss: 0.3930(0.7936) \n",
      "EVAL: [6400/7353] Elapsed 6m 5s (remain 0m 54s) Loss: 0.5075(0.7920) \n",
      "EVAL: [6500/7353] Elapsed 6m 10s (remain 0m 48s) Loss: 0.2775(0.7925) \n",
      "EVAL: [6600/7353] Elapsed 6m 16s (remain 0m 42s) Loss: 0.3155(0.7934) \n",
      "EVAL: [6700/7353] Elapsed 6m 22s (remain 0m 37s) Loss: 0.9130(0.7918) \n",
      "EVAL: [6800/7353] Elapsed 6m 27s (remain 0m 31s) Loss: 1.5449(0.7925) \n",
      "EVAL: [6900/7353] Elapsed 6m 33s (remain 0m 25s) Loss: 0.3246(0.7928) \n",
      "EVAL: [7000/7353] Elapsed 6m 39s (remain 0m 20s) Loss: 1.3078(0.7927) \n",
      "EVAL: [7100/7353] Elapsed 6m 45s (remain 0m 14s) Loss: 0.4148(0.7914) \n",
      "EVAL: [7200/7353] Elapsed 6m 51s (remain 0m 8s) Loss: 0.9955(0.7905) \n",
      "EVAL: [7300/7353] Elapsed 6m 56s (remain 0m 2s) Loss: 0.7227(0.7914) \n",
      "EVAL: [7352/7353] Elapsed 6m 59s (remain 0m 0s) Loss: 0.3053(0.7910) \n",
      "Epoch 2 - Save Best Score: 0.7910 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/7353] Elapsed 0m 0s (remain 88m 6s) Loss: 0.4741(0.4741) Grad: inf  LR: 0.00000012  \n",
      "Epoch: [3][100/7353] Elapsed 1m 23s (remain 99m 40s) Loss: 0.8476(0.7151) Grad: 465134.0625  LR: 0.00000012  \n",
      "Epoch: [3][200/7353] Elapsed 2m 46s (remain 98m 33s) Loss: 0.5623(0.7572) Grad: 416200.8438  LR: 0.00000012  \n",
      "Epoch: [3][300/7353] Elapsed 4m 8s (remain 97m 12s) Loss: 0.9869(0.7702) Grad: 285476.7812  LR: 0.00000012  \n",
      "Epoch: [3][400/7353] Elapsed 5m 31s (remain 95m 50s) Loss: 0.5526(0.7793) Grad: 285068.7500  LR: 0.00000011  \n",
      "Epoch: [3][500/7353] Elapsed 6m 54s (remain 94m 27s) Loss: 0.9599(0.7809) Grad: 411572.3750  LR: 0.00000011  \n",
      "Epoch: [3][600/7353] Elapsed 8m 17s (remain 93m 3s) Loss: 1.0723(0.7803) Grad: 178829.7500  LR: 0.00000011  \n",
      "Epoch: [3][700/7353] Elapsed 9m 39s (remain 91m 41s) Loss: 1.1949(0.7841) Grad: 220651.3281  LR: 0.00000010  \n",
      "Epoch: [3][800/7353] Elapsed 11m 2s (remain 90m 19s) Loss: 0.7791(0.7880) Grad: 125251.5547  LR: 0.00000010  \n",
      "Epoch: [3][900/7353] Elapsed 12m 25s (remain 88m 57s) Loss: 1.0601(0.7919) Grad: 172962.4219  LR: 0.00000010  \n",
      "Epoch: [3][1000/7353] Elapsed 13m 47s (remain 87m 33s) Loss: 0.9257(0.7898) Grad: 99951.0391  LR: 0.00000010  \n",
      "Epoch: [3][1100/7353] Elapsed 15m 10s (remain 86m 11s) Loss: 0.7584(0.7919) Grad: 100719.3125  LR: 0.00000009  \n",
      "Epoch: [3][1200/7353] Elapsed 16m 33s (remain 84m 48s) Loss: 0.7781(0.7942) Grad: 201288.0000  LR: 0.00000009  \n",
      "Epoch: [3][1300/7353] Elapsed 17m 56s (remain 83m 26s) Loss: 0.8022(0.7945) Grad: 94999.2500  LR: 0.00000009  \n",
      "Epoch: [3][1400/7353] Elapsed 19m 18s (remain 82m 3s) Loss: 0.4303(0.7925) Grad: 153876.3438  LR: 0.00000008  \n",
      "Epoch: [3][1500/7353] Elapsed 20m 41s (remain 80m 40s) Loss: 0.5818(0.7953) Grad: 182743.8281  LR: 0.00000008  \n",
      "Epoch: [3][1600/7353] Elapsed 22m 4s (remain 79m 17s) Loss: 0.9672(0.7941) Grad: 167259.5938  LR: 0.00000008  \n",
      "Epoch: [3][1700/7353] Elapsed 23m 27s (remain 77m 55s) Loss: 1.0154(0.7966) Grad: 194287.2656  LR: 0.00000008  \n",
      "Epoch: [3][1800/7353] Elapsed 24m 49s (remain 76m 32s) Loss: 0.6944(0.7946) Grad: 128476.3125  LR: 0.00000007  \n",
      "Epoch: [3][1900/7353] Elapsed 26m 12s (remain 75m 9s) Loss: 1.1678(0.7960) Grad: 123834.2422  LR: 0.00000007  \n",
      "Epoch: [3][2000/7353] Elapsed 27m 35s (remain 73m 46s) Loss: 0.7127(0.7953) Grad: 114793.8281  LR: 0.00000007  \n",
      "Epoch: [3][2100/7353] Elapsed 28m 57s (remain 72m 24s) Loss: 1.0955(0.7946) Grad: 156468.2344  LR: 0.00000007  \n",
      "Epoch: [3][2200/7353] Elapsed 30m 20s (remain 71m 1s) Loss: 0.4213(0.7950) Grad: 76439.5938  LR: 0.00000006  \n",
      "Epoch: [3][2300/7353] Elapsed 31m 43s (remain 69m 38s) Loss: 0.7279(0.7938) Grad: 120319.2031  LR: 0.00000006  \n",
      "Epoch: [3][2400/7353] Elapsed 33m 6s (remain 68m 16s) Loss: 0.6259(0.7956) Grad: 88157.1016  LR: 0.00000006  \n",
      "Epoch: [3][2500/7353] Elapsed 34m 28s (remain 66m 53s) Loss: 0.7708(0.7962) Grad: 169771.0156  LR: 0.00000006  \n",
      "Epoch: [3][2600/7353] Elapsed 35m 51s (remain 65m 31s) Loss: 1.3611(0.7957) Grad: 691876.3750  LR: 0.00000006  \n",
      "Epoch: [3][2700/7353] Elapsed 37m 14s (remain 64m 8s) Loss: 1.3838(0.7953) Grad: 306569.6875  LR: 0.00000005  \n",
      "Epoch: [3][2800/7353] Elapsed 38m 37s (remain 62m 45s) Loss: 0.9227(0.7948) Grad: 210382.4219  LR: 0.00000005  \n",
      "Epoch: [3][2900/7353] Elapsed 39m 59s (remain 61m 22s) Loss: 0.6644(0.7948) Grad: 297102.7812  LR: 0.00000005  \n",
      "Epoch: [3][3000/7353] Elapsed 41m 22s (remain 60m 0s) Loss: 0.7802(0.7950) Grad: 479670.4062  LR: 0.00000005  \n",
      "Epoch: [3][3100/7353] Elapsed 42m 45s (remain 58m 37s) Loss: 0.4114(0.7957) Grad: 303060.9062  LR: 0.00000004  \n",
      "Epoch: [3][3200/7353] Elapsed 44m 8s (remain 57m 14s) Loss: 0.5506(0.7948) Grad: 162067.1719  LR: 0.00000004  \n",
      "Epoch: [3][3300/7353] Elapsed 45m 30s (remain 55m 52s) Loss: 1.3336(0.7957) Grad: 430569.3750  LR: 0.00000004  \n",
      "Epoch: [3][3400/7353] Elapsed 46m 53s (remain 54m 29s) Loss: 0.5981(0.7975) Grad: 169611.8125  LR: 0.00000004  \n",
      "Epoch: [3][3500/7353] Elapsed 48m 16s (remain 53m 6s) Loss: 0.7198(0.7980) Grad: 286961.3438  LR: 0.00000004  \n",
      "Epoch: [3][3600/7353] Elapsed 49m 39s (remain 51m 43s) Loss: 0.8611(0.7979) Grad: 472203.3750  LR: 0.00000003  \n",
      "Epoch: [3][3700/7353] Elapsed 51m 1s (remain 50m 21s) Loss: 0.4945(0.7974) Grad: 191276.8281  LR: 0.00000003  \n",
      "Epoch: [3][3800/7353] Elapsed 52m 24s (remain 48m 58s) Loss: 0.5673(0.7978) Grad: 400690.7500  LR: 0.00000003  \n",
      "Epoch: [3][3900/7353] Elapsed 53m 47s (remain 47m 35s) Loss: 0.6991(0.7973) Grad: 193231.4688  LR: 0.00000003  \n",
      "Epoch: [3][4000/7353] Elapsed 55m 10s (remain 46m 13s) Loss: 0.8301(0.7976) Grad: 196360.3750  LR: 0.00000003  \n",
      "Epoch: [3][4100/7353] Elapsed 56m 32s (remain 44m 50s) Loss: 0.9894(0.7976) Grad: 279140.1562  LR: 0.00000003  \n",
      "Epoch: [3][4200/7353] Elapsed 57m 55s (remain 43m 27s) Loss: 1.2188(0.7963) Grad: 424702.9062  LR: 0.00000002  \n",
      "Epoch: [3][4300/7353] Elapsed 59m 18s (remain 42m 4s) Loss: 0.9646(0.7960) Grad: 321294.7188  LR: 0.00000002  \n",
      "Epoch: [3][4400/7353] Elapsed 60m 40s (remain 40m 42s) Loss: 0.8583(0.7965) Grad: 282909.1562  LR: 0.00000002  \n",
      "Epoch: [3][4500/7353] Elapsed 62m 3s (remain 39m 19s) Loss: 0.9972(0.7976) Grad: 419997.9688  LR: 0.00000002  \n",
      "Epoch: [3][4600/7353] Elapsed 63m 26s (remain 37m 56s) Loss: 1.0216(0.7962) Grad: 368021.3750  LR: 0.00000002  \n",
      "Epoch: [3][4700/7353] Elapsed 64m 49s (remain 36m 34s) Loss: 1.0558(0.7962) Grad: 403914.4375  LR: 0.00000002  \n",
      "Epoch: [3][4800/7353] Elapsed 66m 12s (remain 35m 11s) Loss: 1.2564(0.7961) Grad: 889656.3750  LR: 0.00000002  \n",
      "Epoch: [3][4900/7353] Elapsed 67m 34s (remain 33m 48s) Loss: 0.7572(0.7976) Grad: 255896.1250  LR: 0.00000002  \n",
      "Epoch: [3][5000/7353] Elapsed 68m 57s (remain 32m 25s) Loss: 0.7236(0.7974) Grad: 249736.9062  LR: 0.00000001  \n",
      "Epoch: [3][5100/7353] Elapsed 70m 20s (remain 31m 3s) Loss: 0.9384(0.7975) Grad: 230597.1250  LR: 0.00000001  \n",
      "Epoch: [3][5200/7353] Elapsed 71m 43s (remain 29m 40s) Loss: 0.7330(0.7971) Grad: 236566.4375  LR: 0.00000001  \n",
      "Epoch: [3][5300/7353] Elapsed 73m 5s (remain 28m 17s) Loss: 0.7828(0.7968) Grad: 260580.5156  LR: 0.00000001  \n",
      "Epoch: [3][5400/7353] Elapsed 74m 28s (remain 26m 55s) Loss: 0.5160(0.7973) Grad: 223989.8750  LR: 0.00000001  \n",
      "Epoch: [3][5500/7353] Elapsed 75m 51s (remain 25m 32s) Loss: 0.8264(0.7967) Grad: 435100.5938  LR: 0.00000001  \n",
      "Epoch: [3][5600/7353] Elapsed 77m 14s (remain 24m 9s) Loss: 0.5764(0.7962) Grad: 310303.6250  LR: 0.00000001  \n",
      "Epoch: [3][5700/7353] Elapsed 78m 36s (remain 22m 46s) Loss: 0.5344(0.7963) Grad: 215561.4219  LR: 0.00000001  \n",
      "Epoch: [3][5800/7353] Elapsed 79m 59s (remain 21m 24s) Loss: 0.7475(0.7963) Grad: 388830.0938  LR: 0.00000001  \n",
      "Epoch: [3][5900/7353] Elapsed 81m 22s (remain 20m 1s) Loss: 0.6641(0.7962) Grad: 326093.6250  LR: 0.00000001  \n",
      "Epoch: [3][6000/7353] Elapsed 82m 45s (remain 18m 38s) Loss: 0.4695(0.7954) Grad: 329155.5000  LR: 0.00000000  \n",
      "Epoch: [3][6100/7353] Elapsed 84m 8s (remain 17m 15s) Loss: 0.4303(0.7950) Grad: 229854.5469  LR: 0.00000000  \n",
      "Epoch: [3][6200/7353] Elapsed 85m 30s (remain 15m 53s) Loss: 0.4781(0.7939) Grad: 363050.9688  LR: 0.00000000  \n",
      "Epoch: [3][6300/7353] Elapsed 86m 53s (remain 14m 30s) Loss: 1.1364(0.7941) Grad: 285700.1875  LR: 0.00000000  \n",
      "Epoch: [3][6400/7353] Elapsed 88m 16s (remain 13m 7s) Loss: 1.2197(0.7947) Grad: 334504.6875  LR: 0.00000000  \n",
      "Epoch: [3][6500/7353] Elapsed 89m 38s (remain 11m 44s) Loss: 0.7693(0.7955) Grad: 454004.3438  LR: 0.00000000  \n",
      "Epoch: [3][6600/7353] Elapsed 91m 1s (remain 10m 22s) Loss: 1.0650(0.7950) Grad: 717078.5000  LR: 0.00000000  \n",
      "Epoch: [3][6700/7353] Elapsed 92m 24s (remain 8m 59s) Loss: 0.8933(0.7949) Grad: 212623.5469  LR: 0.00000000  \n",
      "Epoch: [3][6800/7353] Elapsed 93m 47s (remain 7m 36s) Loss: 0.7839(0.7956) Grad: 324532.4062  LR: 0.00000000  \n",
      "Epoch: [3][6900/7353] Elapsed 95m 9s (remain 6m 13s) Loss: 0.6780(0.7960) Grad: 284175.8125  LR: 0.00000000  \n",
      "Epoch: [3][7000/7353] Elapsed 96m 32s (remain 4m 51s) Loss: 0.9259(0.7962) Grad: 413279.5938  LR: 0.00000000  \n",
      "Epoch: [3][7100/7353] Elapsed 97m 55s (remain 3m 28s) Loss: 0.7164(0.7966) Grad: 446232.9062  LR: 0.00000000  \n",
      "Epoch: [3][7200/7353] Elapsed 99m 18s (remain 2m 5s) Loss: 1.0554(0.7959) Grad: 236997.5781  LR: 0.00000000  \n",
      "Epoch: [3][7300/7353] Elapsed 100m 40s (remain 0m 43s) Loss: 0.9680(0.7957) Grad: 401579.3125  LR: 0.00000000  \n",
      "Epoch: [3][7352/7353] Elapsed 101m 23s (remain 0m 0s) Loss: 0.9717(0.7955) Grad: 243814.6406  LR: 0.00000000  \n",
      "EVAL: [0/7353] Elapsed 0m 0s (remain 6m 48s) Loss: 0.3665(0.3665) \n",
      "EVAL: [100/7353] Elapsed 0m 5s (remain 6m 57s) Loss: 1.0007(0.7403) \n",
      "EVAL: [200/7353] Elapsed 0m 11s (remain 6m 55s) Loss: 1.2928(0.7639) \n",
      "EVAL: [300/7353] Elapsed 0m 17s (remain 6m 46s) Loss: 0.7943(0.7583) \n",
      "EVAL: [400/7353] Elapsed 0m 23s (remain 6m 40s) Loss: 1.3517(0.7809) \n",
      "EVAL: [500/7353] Elapsed 0m 28s (remain 6m 33s) Loss: 0.3032(0.8113) \n",
      "EVAL: [600/7353] Elapsed 0m 34s (remain 6m 28s) Loss: 0.4062(0.8097) \n",
      "EVAL: [700/7353] Elapsed 0m 40s (remain 6m 22s) Loss: 0.4196(0.8014) \n",
      "EVAL: [800/7353] Elapsed 0m 46s (remain 6m 17s) Loss: 0.3494(0.8005) \n",
      "EVAL: [900/7353] Elapsed 0m 51s (remain 6m 11s) Loss: 0.4550(0.7846) \n",
      "EVAL: [1000/7353] Elapsed 0m 57s (remain 6m 5s) Loss: 0.7230(0.7743) \n",
      "EVAL: [1100/7353] Elapsed 1m 3s (remain 5m 59s) Loss: 0.3629(0.7721) \n",
      "EVAL: [1200/7353] Elapsed 1m 9s (remain 5m 53s) Loss: 0.4004(0.7690) \n",
      "EVAL: [1300/7353] Elapsed 1m 14s (remain 5m 47s) Loss: 0.3755(0.7742) \n",
      "EVAL: [1400/7353] Elapsed 1m 20s (remain 5m 42s) Loss: 0.4161(0.7674) \n",
      "EVAL: [1500/7353] Elapsed 1m 26s (remain 5m 36s) Loss: 0.3602(0.7813) \n",
      "EVAL: [1600/7353] Elapsed 1m 32s (remain 5m 31s) Loss: 1.3798(0.7753) \n",
      "EVAL: [1700/7353] Elapsed 1m 37s (remain 5m 25s) Loss: 0.7564(0.7791) \n",
      "EVAL: [1800/7353] Elapsed 1m 43s (remain 5m 19s) Loss: 0.8544(0.7785) \n",
      "EVAL: [1900/7353] Elapsed 1m 49s (remain 5m 14s) Loss: 0.6805(0.7821) \n",
      "EVAL: [2000/7353] Elapsed 1m 55s (remain 5m 8s) Loss: 0.3771(0.7779) \n",
      "EVAL: [2100/7353] Elapsed 2m 0s (remain 5m 2s) Loss: 1.1799(0.7794) \n",
      "EVAL: [2200/7353] Elapsed 2m 6s (remain 4m 56s) Loss: 1.4042(0.7751) \n",
      "EVAL: [2300/7353] Elapsed 2m 12s (remain 4m 50s) Loss: 0.4445(0.7759) \n",
      "EVAL: [2400/7353] Elapsed 2m 18s (remain 4m 44s) Loss: 1.0915(0.7754) \n",
      "EVAL: [2500/7353] Elapsed 2m 24s (remain 4m 39s) Loss: 0.3181(0.7707) \n",
      "EVAL: [2600/7353] Elapsed 2m 30s (remain 4m 34s) Loss: 0.3862(0.7741) \n",
      "EVAL: [2700/7353] Elapsed 2m 35s (remain 4m 28s) Loss: 0.3593(0.7709) \n",
      "EVAL: [2800/7353] Elapsed 2m 41s (remain 4m 22s) Loss: 1.2362(0.7703) \n",
      "EVAL: [2900/7353] Elapsed 2m 47s (remain 4m 16s) Loss: 0.4573(0.7696) \n",
      "EVAL: [3000/7353] Elapsed 2m 53s (remain 4m 11s) Loss: 0.3209(0.7723) \n",
      "EVAL: [3100/7353] Elapsed 2m 59s (remain 4m 5s) Loss: 1.5587(0.7741) \n",
      "EVAL: [3200/7353] Elapsed 3m 4s (remain 3m 59s) Loss: 0.3041(0.7735) \n",
      "EVAL: [3300/7353] Elapsed 3m 10s (remain 3m 54s) Loss: 0.4197(0.7723) \n",
      "EVAL: [3400/7353] Elapsed 3m 16s (remain 3m 48s) Loss: 0.3595(0.7742) \n",
      "EVAL: [3500/7353] Elapsed 3m 22s (remain 3m 42s) Loss: 1.3303(0.7733) \n",
      "EVAL: [3600/7353] Elapsed 3m 28s (remain 3m 37s) Loss: 0.4085(0.7743) \n",
      "EVAL: [3700/7353] Elapsed 3m 35s (remain 3m 32s) Loss: 0.4223(0.7768) \n",
      "EVAL: [3800/7353] Elapsed 3m 41s (remain 3m 26s) Loss: 1.4541(0.7769) \n",
      "EVAL: [3900/7353] Elapsed 3m 47s (remain 3m 21s) Loss: 0.9091(0.7777) \n",
      "EVAL: [4000/7353] Elapsed 3m 53s (remain 3m 15s) Loss: 1.4250(0.7792) \n",
      "EVAL: [4100/7353] Elapsed 3m 59s (remain 3m 9s) Loss: 0.4180(0.7786) \n",
      "EVAL: [4200/7353] Elapsed 4m 5s (remain 3m 4s) Loss: 1.4831(0.7791) \n",
      "EVAL: [4300/7353] Elapsed 4m 11s (remain 2m 58s) Loss: 0.3360(0.7790) \n",
      "EVAL: [4400/7353] Elapsed 4m 17s (remain 2m 52s) Loss: 0.9154(0.7784) \n",
      "EVAL: [4500/7353] Elapsed 4m 23s (remain 2m 46s) Loss: 0.8077(0.7799) \n",
      "EVAL: [4600/7353] Elapsed 4m 29s (remain 2m 40s) Loss: 1.4704(0.7847) \n",
      "EVAL: [4700/7353] Elapsed 4m 34s (remain 2m 35s) Loss: 0.3717(0.7866) \n",
      "EVAL: [4800/7353] Elapsed 4m 40s (remain 2m 29s) Loss: 0.4836(0.7881) \n",
      "EVAL: [4900/7353] Elapsed 4m 46s (remain 2m 23s) Loss: 1.7225(0.7873) \n",
      "EVAL: [5000/7353] Elapsed 4m 52s (remain 2m 17s) Loss: 0.3923(0.7876) \n",
      "EVAL: [5100/7353] Elapsed 4m 57s (remain 2m 11s) Loss: 0.3291(0.7881) \n",
      "EVAL: [5200/7353] Elapsed 5m 3s (remain 2m 5s) Loss: 0.6726(0.7895) \n",
      "EVAL: [5300/7353] Elapsed 5m 9s (remain 1m 59s) Loss: 0.4986(0.7875) \n",
      "EVAL: [5400/7353] Elapsed 5m 15s (remain 1m 54s) Loss: 0.6230(0.7883) \n",
      "EVAL: [5500/7353] Elapsed 5m 21s (remain 1m 48s) Loss: 0.3332(0.7882) \n",
      "EVAL: [5600/7353] Elapsed 5m 27s (remain 1m 42s) Loss: 0.6969(0.7893) \n",
      "EVAL: [5700/7353] Elapsed 5m 33s (remain 1m 36s) Loss: 0.6026(0.7909) \n",
      "EVAL: [5800/7353] Elapsed 5m 39s (remain 1m 30s) Loss: 0.5286(0.7913) \n",
      "EVAL: [5900/7353] Elapsed 5m 45s (remain 1m 24s) Loss: 0.5322(0.7897) \n",
      "EVAL: [6000/7353] Elapsed 5m 50s (remain 1m 19s) Loss: 0.4951(0.7903) \n",
      "EVAL: [6100/7353] Elapsed 5m 56s (remain 1m 13s) Loss: 0.3975(0.7908) \n",
      "EVAL: [6200/7353] Elapsed 6m 2s (remain 1m 7s) Loss: 1.5563(0.7905) \n",
      "EVAL: [6300/7353] Elapsed 6m 8s (remain 1m 1s) Loss: 0.4104(0.7915) \n",
      "EVAL: [6400/7353] Elapsed 6m 14s (remain 0m 55s) Loss: 0.5500(0.7901) \n",
      "EVAL: [6500/7353] Elapsed 6m 20s (remain 0m 49s) Loss: 0.3100(0.7905) \n",
      "EVAL: [6600/7353] Elapsed 6m 26s (remain 0m 44s) Loss: 0.3259(0.7913) \n",
      "EVAL: [6700/7353] Elapsed 6m 32s (remain 0m 38s) Loss: 0.8565(0.7899) \n",
      "EVAL: [6800/7353] Elapsed 6m 38s (remain 0m 32s) Loss: 1.2970(0.7903) \n",
      "EVAL: [6900/7353] Elapsed 6m 44s (remain 0m 26s) Loss: 0.3444(0.7904) \n",
      "EVAL: [7000/7353] Elapsed 6m 51s (remain 0m 20s) Loss: 1.2288(0.7900) \n",
      "EVAL: [7100/7353] Elapsed 6m 57s (remain 0m 14s) Loss: 0.4345(0.7886) \n",
      "EVAL: [7200/7353] Elapsed 7m 3s (remain 0m 8s) Loss: 0.9772(0.7876) \n",
      "EVAL: [7300/7353] Elapsed 7m 9s (remain 0m 3s) Loss: 0.6840(0.7881) \n",
      "EVAL: [7352/7353] Elapsed 7m 12s (remain 0m 0s) Loss: 0.3158(0.7876) \n",
      "Epoch 3 - Save Best Score: 0.7876 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-41d9cb46780b>:55: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7353] Elapsed 0m 0s (remain 45m 33s) Loss: 1.2886(1.2886) Grad: inf  LR: 0.00000050  \n",
      "Epoch: [1][100/7353] Elapsed 0m 42s (remain 51m 5s) Loss: 0.7612(1.0517) Grad: 492514.3125  LR: 0.00000050  \n",
      "Epoch: [1][200/7353] Elapsed 1m 24s (remain 49m 57s) Loss: 0.8907(1.0130) Grad: 458244.1250  LR: 0.00000050  \n",
      "Epoch: [1][300/7353] Elapsed 2m 5s (remain 49m 11s) Loss: 1.3458(1.0090) Grad: 782875.0625  LR: 0.00000050  \n",
      "Epoch: [1][400/7353] Elapsed 2m 47s (remain 48m 26s) Loss: 0.8739(1.0001) Grad: 481011.8438  LR: 0.00000050  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-105c76ad4fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0m_oof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[1;31m#LOGGER.info(f\"========== fold: {fold} result ==========\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-2a8e3328031d>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(folds, fold)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mawp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-41d9cb46780b>\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, awp)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-3333403a8286>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtransformer_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# LSTM/GRU header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         )\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1054\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    500\u001b[0m                 )\n\u001b[0;32m    501\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m                 output_states = layer_module(\n\u001b[0m\u001b[0;32m    503\u001b[0m                     \u001b[0mnext_kv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     ):\n\u001b[1;32m--> 346\u001b[1;33m         attention_output = self.attention(\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_states\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[0mquery_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train_df, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                #LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                #get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        #LOGGER.info(f\"========== CV ==========\")\n",
    "        #get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>essay_text</th>\n",
       "      <th>label</th>\n",
       "      <th>inputs</th>\n",
       "      <th>fold</th>\n",
       "      <th>oof_ineffective</th>\n",
       "      <th>oof_adequate</th>\n",
       "      <th>oof_effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>1</td>\n",
       "      <td>lead hi, i'm isaac, i'm going to be writing ab...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312385</td>\n",
       "      <td>1.141884</td>\n",
       "      <td>-2.849747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6efd9102298b</td>\n",
       "      <td>00944C693682</td>\n",
       "      <td>the environment suffers greatly from the many ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Limiting the usage of cars has personal and pr...</td>\n",
       "      <td>2</td>\n",
       "      <td>claim the environment suffers greatly from the...</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.063072</td>\n",
       "      <td>0.061954</td>\n",
       "      <td>0.578710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8df2da9994bf</td>\n",
       "      <td>00944C693682</td>\n",
       "      <td>It is also worth noting that cities that have ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Limiting the usage of cars has personal and pr...</td>\n",
       "      <td>2</td>\n",
       "      <td>claim it is also worth noting that cities that...</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.059776</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.562960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af494e4e0b4f</td>\n",
       "      <td>00BD97EA4041</td>\n",
       "      <td>To conclusion computers in school shouldn't de...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Should computers read the emotional expression...</td>\n",
       "      <td>1</td>\n",
       "      <td>concluding statement to conclusion computers i...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.197987</td>\n",
       "      <td>0.835998</td>\n",
       "      <td>-2.118340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>095847b91399</td>\n",
       "      <td>00C6E82FE5BA</td>\n",
       "      <td>People use face reconition alot and it can be ...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>I think that it wouldn't be valueable to have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>evidence people use face reconition alot and i...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517432</td>\n",
       "      <td>0.617753</td>\n",
       "      <td>-1.679376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>4c649f487587</td>\n",
       "      <td>FE3CA06DDCA1</td>\n",
       "      <td>they are give you as much detail as they have ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Why is it when someone asks you for advice the...</td>\n",
       "      <td>1</td>\n",
       "      <td>claim they are give you as much detail as they...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.083238</td>\n",
       "      <td>1.245852</td>\n",
       "      <td>-0.878734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>2a26c07f620e</td>\n",
       "      <td>FEF42864AE28</td>\n",
       "      <td>due to the distractions for students at home</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>During a long day at school, have you ever tho...</td>\n",
       "      <td>2</td>\n",
       "      <td>claim due to the distractions for students at ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.978429</td>\n",
       "      <td>0.612321</td>\n",
       "      <td>1.213933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>625808403363</td>\n",
       "      <td>FF9E0379CD98</td>\n",
       "      <td>if you need highlights or pensil for write som...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Some school offer distence learning as a optio...</td>\n",
       "      <td>1</td>\n",
       "      <td>evidence if you need highlights or pensil for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.364349</td>\n",
       "      <td>1.507439</td>\n",
       "      <td>-1.426399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>74c58fcc7ef8</td>\n",
       "      <td>FF9E0379CD98</td>\n",
       "      <td>you cant work or cant study after school with ...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Some school offer distence learning as a optio...</td>\n",
       "      <td>1</td>\n",
       "      <td>evidence you cant work or cant study after sch...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.312942</td>\n",
       "      <td>1.485517</td>\n",
       "      <td>-1.370835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7352</th>\n",
       "      <td>f7bb19f23dcd</td>\n",
       "      <td>FF9E0379CD98</td>\n",
       "      <td>in the end you cant take the class because is ...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Some school offer distence learning as a optio...</td>\n",
       "      <td>1</td>\n",
       "      <td>counterclaim in the end you cant take the clas...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.260187</td>\n",
       "      <td>1.444135</td>\n",
       "      <td>-1.284776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22059 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      discourse_id      essay_id  \\\n",
       "0     0013cc385424  007ACE74B050   \n",
       "1     6efd9102298b  00944C693682   \n",
       "2     8df2da9994bf  00944C693682   \n",
       "3     af494e4e0b4f  00BD97EA4041   \n",
       "4     095847b91399  00C6E82FE5BA   \n",
       "...            ...           ...   \n",
       "7348  4c649f487587  FE3CA06DDCA1   \n",
       "7349  2a26c07f620e  FEF42864AE28   \n",
       "7350  625808403363  FF9E0379CD98   \n",
       "7351  74c58fcc7ef8  FF9E0379CD98   \n",
       "7352  f7bb19f23dcd  FF9E0379CD98   \n",
       "\n",
       "                                         discourse_text        discourse_type  \\\n",
       "0     Hi, i'm Isaac, i'm going to be writing about h...                  Lead   \n",
       "1     the environment suffers greatly from the many ...                 Claim   \n",
       "2     It is also worth noting that cities that have ...                 Claim   \n",
       "3     To conclusion computers in school shouldn't de...  Concluding Statement   \n",
       "4     People use face reconition alot and it can be ...              Evidence   \n",
       "...                                                 ...                   ...   \n",
       "7348  they are give you as much detail as they have ...                 Claim   \n",
       "7349      due to the distractions for students at home                  Claim   \n",
       "7350  if you need highlights or pensil for write som...              Evidence   \n",
       "7351  you cant work or cant study after school with ...              Evidence   \n",
       "7352  in the end you cant take the class because is ...          Counterclaim   \n",
       "\n",
       "     discourse_effectiveness  \\\n",
       "0                   Adequate   \n",
       "1                  Effective   \n",
       "2                  Effective   \n",
       "3                   Adequate   \n",
       "4                   Adequate   \n",
       "...                      ...   \n",
       "7348                Adequate   \n",
       "7349               Effective   \n",
       "7350                Adequate   \n",
       "7351                Adequate   \n",
       "7352                Adequate   \n",
       "\n",
       "                                             essay_text  label  \\\n",
       "0     Hi, i'm Isaac, i'm going to be writing about h...      1   \n",
       "1     Limiting the usage of cars has personal and pr...      2   \n",
       "2     Limiting the usage of cars has personal and pr...      2   \n",
       "3     Should computers read the emotional expression...      1   \n",
       "4     I think that it wouldn't be valueable to have ...      1   \n",
       "...                                                 ...    ...   \n",
       "7348  Why is it when someone asks you for advice the...      1   \n",
       "7349  During a long day at school, have you ever tho...      2   \n",
       "7350  Some school offer distence learning as a optio...      1   \n",
       "7351  Some school offer distence learning as a optio...      1   \n",
       "7352  Some school offer distence learning as a optio...      1   \n",
       "\n",
       "                                                 inputs  fold  \\\n",
       "0     lead hi, i'm isaac, i'm going to be writing ab...     0   \n",
       "1     claim the environment suffers greatly from the...     0   \n",
       "2     claim it is also worth noting that cities that...     0   \n",
       "3     concluding statement to conclusion computers i...     0   \n",
       "4     evidence people use face reconition alot and i...     0   \n",
       "...                                                 ...   ...   \n",
       "7348  claim they are give you as much detail as they...     2   \n",
       "7349  claim due to the distractions for students at ...     2   \n",
       "7350  evidence if you need highlights or pensil for ...     2   \n",
       "7351  evidence you cant work or cant study after sch...     2   \n",
       "7352  counterclaim in the end you cant take the clas...     2   \n",
       "\n",
       "      oof_ineffective  oof_adequate  oof_effective  \n",
       "0            0.312385      1.141884      -2.849747  \n",
       "1           -2.063072      0.061954       0.578710  \n",
       "2           -2.059776      0.069159       0.562960  \n",
       "3           -0.197987      0.835998      -2.118340  \n",
       "4           -0.517432      0.617753      -1.679376  \n",
       "...               ...           ...            ...  \n",
       "7348        -0.083238      1.245852      -0.878734  \n",
       "7349        -1.978429      0.612321       1.213933  \n",
       "7350         0.364349      1.507439      -1.426399  \n",
       "7351         0.312942      1.485517      -1.370835  \n",
       "7352         0.260187      1.444135      -1.284776  \n",
       "\n",
       "[22059 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = log_loss(\n",
    "            oof_df['label'],\n",
    "            oof_df[['oof_ineffective'\t,'oof_adequate',\t'oof_effective']],\n",
    "            labels=[0, 1, 2],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.487588889049747"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
